{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim #손실함수 최적화 시 사용함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.]])\n",
      "tensor([[ 2.],\n",
      "        [ 4.],\n",
      "        [ 6.],\n",
      "        [ 8.],\n",
      "        [10.]])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "x_train = torch.FloatTensor([[1], [2], [3], [4], [5]]) #2차원 TENSOR\n",
    "y_train = torch.FloatTensor([[2], [4], [6], [8], [10]]) #2차원 TENSOR\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "print(x_train.size())\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad=True)\n",
    "print(W)\n",
    "b = torch.zeros(1, requires_grad=True) #requires_grad=True : 학습을 통해 나중에 변경을 요구하는 함수수\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h = x_train*W+b\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((h-y_train)**2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#어떤 식으로 내려오는지를 확인함 \n",
    "#'SGD'는경사하강법의종류,  lr은학습률(learning rate)를의미\n",
    "# 학습대상인 W와 b가 SGD의 입력됨.\n",
    "optimizer = optim.SGD([W, b], lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch  100/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch  200/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch  300/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch  400/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch  500/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch  600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch  700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch  800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch  900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 1900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 2900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 3900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 4900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 5900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 6900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 7900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 8900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 9900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 10900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 11900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 12900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 13900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 14900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 15900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 16900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 17900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 18900/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19000/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19100/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19200/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19300/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19400/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19500/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19600/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19700/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19800/20000 W:2.000 b:0.000 cost:0.000000\n",
      "Epoch 19900/20000 W:2.000 b:0.000 cost:0.000000\n"
     ]
    }
   ],
   "source": [
    "epochs = 20000\n",
    "for epoch in range(epochs):\n",
    "    h=x_train*W+b\n",
    "    \n",
    "    cost = torch.mean((h-y_train)**2) #cost가 최적화 되도록 개선해야 함\n",
    "    optimizer.zero_grad() #초기화 시킴\n",
    "    cost.backward() # 비용함수를 미분하여 gradient 계산\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0: #100번마다 한 번씩 출력\n",
    "        print('Epoch {:4d}/{} W:{:.3f} b:{:.3f} cost:{:.6f}'.format(\n",
    "            epoch, epochs, W.item(), b.item(), cost.item()\n",
    "        )) #{}안에 값이 들어감\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13.9999],\n",
      "        [15.9999],\n",
      "        [17.9999]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h = torch.FloatTensor([[7], [8], [9]])*W+b\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "print(w)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    z =  2*W\n",
    "    z.backward()\n",
    "    print('수식을 w로 미분한 값:{}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 3일 때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 3일 때')\n",
    "for i in range(1, 3):\n",
    "    print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 5일 때\n",
      "tensor([0.8303])\n",
      "tensor([0.1261])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "print('랜덤 시드가 5일 때')\n",
    "for i in range(1, 3):\n",
    "    print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 5일 때')\n",
    "for i in range(1, 3):\n",
    "    print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [2 3]\n",
      " [3 4]\n",
      " [4 5]]\n",
      "[[ 3]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_data1 = np.array([[1,2],[2,3],[3,4],[4,5]])\n",
    "t_data1 = np.array([3, 5, 7,9,11]).reshape(5,1)\n",
    "print(x_data1)\n",
    "print(t_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [2., 3.],\n",
      "        [3., 4.],\n",
      "        [4., 5.]])\n",
      "tensor([[1., 2.],\n",
      "        [2., 3.],\n",
      "        [3., 4.],\n",
      "        [4., 5.]])\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.from_numpy(x_data1).float()\n",
    "t_data = torch.from_numpy(x_data1).float()\n",
    "print(x_data)\n",
    "print(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((2,1), requires_grad=True)\n",
    "b=torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 y: tensor([[0.1176],\n",
      "        [0.1176],\n",
      "        [0.1176],\n",
      "        [0.1176]], grad_fn=<AddBackward0>) 9.808015823364258 w: tensor(2., requires_grad=True) b:  tensor([0.1176], requires_grad=True)\n",
      "epoch: 100 y: tensor([[0.1182],\n",
      "        [0.1182],\n",
      "        [0.1182],\n",
      "        [0.1182]], grad_fn=<AddBackward0>) 9.804691314697266 w: tensor(2., requires_grad=True) b:  tensor([0.1182], requires_grad=True)\n",
      "epoch: 200 y: tensor([[0.1188],\n",
      "        [0.1188],\n",
      "        [0.1188],\n",
      "        [0.1188]], grad_fn=<AddBackward0>) 9.801370620727539 w: tensor(2., requires_grad=True) b:  tensor([0.1188], requires_grad=True)\n",
      "epoch: 300 y: tensor([[0.1194],\n",
      "        [0.1194],\n",
      "        [0.1194],\n",
      "        [0.1194]], grad_fn=<AddBackward0>) 9.798051834106445 w: tensor(2., requires_grad=True) b:  tensor([0.1194], requires_grad=True)\n",
      "epoch: 400 y: tensor([[0.1199],\n",
      "        [0.1199],\n",
      "        [0.1199],\n",
      "        [0.1199]], grad_fn=<AddBackward0>) 9.794734954833984 w: tensor(2., requires_grad=True) b:  tensor([0.1199], requires_grad=True)\n",
      "epoch: 500 y: tensor([[0.1205],\n",
      "        [0.1205],\n",
      "        [0.1205],\n",
      "        [0.1205]], grad_fn=<AddBackward0>) 9.791417121887207 w: tensor(2., requires_grad=True) b:  tensor([0.1205], requires_grad=True)\n",
      "epoch: 600 y: tensor([[0.1211],\n",
      "        [0.1211],\n",
      "        [0.1211],\n",
      "        [0.1211]], grad_fn=<AddBackward0>) 9.788101196289062 w: tensor(2., requires_grad=True) b:  tensor([0.1211], requires_grad=True)\n",
      "epoch: 700 y: tensor([[0.1217],\n",
      "        [0.1217],\n",
      "        [0.1217],\n",
      "        [0.1217]], grad_fn=<AddBackward0>) 9.784785270690918 w: tensor(2., requires_grad=True) b:  tensor([0.1217], requires_grad=True)\n",
      "epoch: 800 y: tensor([[0.1222],\n",
      "        [0.1222],\n",
      "        [0.1222],\n",
      "        [0.1222]], grad_fn=<AddBackward0>) 9.781469345092773 w: tensor(2., requires_grad=True) b:  tensor([0.1223], requires_grad=True)\n",
      "epoch: 900 y: tensor([[0.1228],\n",
      "        [0.1228],\n",
      "        [0.1228],\n",
      "        [0.1228]], grad_fn=<AddBackward0>) 9.778160095214844 w: tensor(2., requires_grad=True) b:  tensor([0.1228], requires_grad=True)\n",
      "epoch: 1000 y: tensor([[0.1234],\n",
      "        [0.1234],\n",
      "        [0.1234],\n",
      "        [0.1234]], grad_fn=<AddBackward0>) 9.774849891662598 w: tensor(2., requires_grad=True) b:  tensor([0.1234], requires_grad=True)\n",
      "epoch: 1100 y: tensor([[0.1240],\n",
      "        [0.1240],\n",
      "        [0.1240],\n",
      "        [0.1240]], grad_fn=<AddBackward0>) 9.771541595458984 w: tensor(2., requires_grad=True) b:  tensor([0.1240], requires_grad=True)\n",
      "epoch: 1200 y: tensor([[0.1245],\n",
      "        [0.1245],\n",
      "        [0.1245],\n",
      "        [0.1245]], grad_fn=<AddBackward0>) 9.768232345581055 w: tensor(2., requires_grad=True) b:  tensor([0.1246], requires_grad=True)\n",
      "epoch: 1300 y: tensor([[0.1251],\n",
      "        [0.1251],\n",
      "        [0.1251],\n",
      "        [0.1251]], grad_fn=<AddBackward0>) 9.764925956726074 w: tensor(2., requires_grad=True) b:  tensor([0.1251], requires_grad=True)\n",
      "epoch: 1400 y: tensor([[0.1257],\n",
      "        [0.1257],\n",
      "        [0.1257],\n",
      "        [0.1257]], grad_fn=<AddBackward0>) 9.761618614196777 w: tensor(2., requires_grad=True) b:  tensor([0.1257], requires_grad=True)\n",
      "epoch: 1500 y: tensor([[0.1263],\n",
      "        [0.1263],\n",
      "        [0.1263],\n",
      "        [0.1263]], grad_fn=<AddBackward0>) 9.758312225341797 w: tensor(2., requires_grad=True) b:  tensor([0.1263], requires_grad=True)\n",
      "epoch: 1600 y: tensor([[0.1268],\n",
      "        [0.1268],\n",
      "        [0.1268],\n",
      "        [0.1268]], grad_fn=<AddBackward0>) 9.755006790161133 w: tensor(2., requires_grad=True) b:  tensor([0.1269], requires_grad=True)\n",
      "epoch: 1700 y: tensor([[0.1274],\n",
      "        [0.1274],\n",
      "        [0.1274],\n",
      "        [0.1274]], grad_fn=<AddBackward0>) 9.751702308654785 w: tensor(2., requires_grad=True) b:  tensor([0.1274], requires_grad=True)\n",
      "epoch: 1800 y: tensor([[0.1280],\n",
      "        [0.1280],\n",
      "        [0.1280],\n",
      "        [0.1280]], grad_fn=<AddBackward0>) 9.748401641845703 w: tensor(2., requires_grad=True) b:  tensor([0.1280], requires_grad=True)\n",
      "epoch: 1900 y: tensor([[0.1286],\n",
      "        [0.1286],\n",
      "        [0.1286],\n",
      "        [0.1286]], grad_fn=<AddBackward0>) 9.74510669708252 w: tensor(2., requires_grad=True) b:  tensor([0.1286], requires_grad=True)\n",
      "epoch: 2000 y: tensor([[0.1291],\n",
      "        [0.1291],\n",
      "        [0.1291],\n",
      "        [0.1291]], grad_fn=<AddBackward0>) 9.741811752319336 w: tensor(2., requires_grad=True) b:  tensor([0.1292], requires_grad=True)\n",
      "epoch: 2100 y: tensor([[0.1297],\n",
      "        [0.1297],\n",
      "        [0.1297],\n",
      "        [0.1297]], grad_fn=<AddBackward0>) 9.738517761230469 w: tensor(2., requires_grad=True) b:  tensor([0.1297], requires_grad=True)\n",
      "epoch: 2200 y: tensor([[0.1303],\n",
      "        [0.1303],\n",
      "        [0.1303],\n",
      "        [0.1303]], grad_fn=<AddBackward0>) 9.735225677490234 w: tensor(2., requires_grad=True) b:  tensor([0.1303], requires_grad=True)\n",
      "epoch: 2300 y: tensor([[0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309],\n",
      "        [0.1309]], grad_fn=<AddBackward0>) 9.731931686401367 w: tensor(2., requires_grad=True) b:  tensor([0.1309], requires_grad=True)\n",
      "epoch: 2400 y: tensor([[0.1314],\n",
      "        [0.1314],\n",
      "        [0.1314],\n",
      "        [0.1314]], grad_fn=<AddBackward0>) 9.728641510009766 w: tensor(2., requires_grad=True) b:  tensor([0.1314], requires_grad=True)\n",
      "epoch: 2500 y: tensor([[0.1320],\n",
      "        [0.1320],\n",
      "        [0.1320],\n",
      "        [0.1320]], grad_fn=<AddBackward0>) 9.725350379943848 w: tensor(2., requires_grad=True) b:  tensor([0.1320], requires_grad=True)\n",
      "epoch: 2600 y: tensor([[0.1326],\n",
      "        [0.1326],\n",
      "        [0.1326],\n",
      "        [0.1326]], grad_fn=<AddBackward0>) 9.722060203552246 w: tensor(2., requires_grad=True) b:  tensor([0.1326], requires_grad=True)\n",
      "epoch: 2700 y: tensor([[0.1332],\n",
      "        [0.1332],\n",
      "        [0.1332],\n",
      "        [0.1332]], grad_fn=<AddBackward0>) 9.718770980834961 w: tensor(2., requires_grad=True) b:  tensor([0.1332], requires_grad=True)\n",
      "epoch: 2800 y: tensor([[0.1337],\n",
      "        [0.1337],\n",
      "        [0.1337],\n",
      "        [0.1337]], grad_fn=<AddBackward0>) 9.715481758117676 w: tensor(2., requires_grad=True) b:  tensor([0.1337], requires_grad=True)\n",
      "epoch: 2900 y: tensor([[0.1343],\n",
      "        [0.1343],\n",
      "        [0.1343],\n",
      "        [0.1343]], grad_fn=<AddBackward0>) 9.71219253540039 w: tensor(2., requires_grad=True) b:  tensor([0.1343], requires_grad=True)\n",
      "epoch: 3000 y: tensor([[0.1349],\n",
      "        [0.1349],\n",
      "        [0.1349],\n",
      "        [0.1349]], grad_fn=<AddBackward0>) 9.708905220031738 w: tensor(2., requires_grad=True) b:  tensor([0.1349], requires_grad=True)\n",
      "epoch: 3100 y: tensor([[0.1355],\n",
      "        [0.1355],\n",
      "        [0.1355],\n",
      "        [0.1355]], grad_fn=<AddBackward0>) 9.705619812011719 w: tensor(2., requires_grad=True) b:  tensor([0.1355], requires_grad=True)\n",
      "epoch: 3200 y: tensor([[0.1360],\n",
      "        [0.1360],\n",
      "        [0.1360],\n",
      "        [0.1360]], grad_fn=<AddBackward0>) 9.70234203338623 w: tensor(2., requires_grad=True) b:  tensor([0.1360], requires_grad=True)\n",
      "epoch: 3300 y: tensor([[0.1366],\n",
      "        [0.1366],\n",
      "        [0.1366],\n",
      "        [0.1366]], grad_fn=<AddBackward0>) 9.699064254760742 w: tensor(2., requires_grad=True) b:  tensor([0.1366], requires_grad=True)\n",
      "epoch: 3400 y: tensor([[0.1372],\n",
      "        [0.1372],\n",
      "        [0.1372],\n",
      "        [0.1372]], grad_fn=<AddBackward0>) 9.695788383483887 w: tensor(2., requires_grad=True) b:  tensor([0.1372], requires_grad=True)\n",
      "epoch: 3500 y: tensor([[0.1377],\n",
      "        [0.1377],\n",
      "        [0.1377],\n",
      "        [0.1377]], grad_fn=<AddBackward0>) 9.692512512207031 w: tensor(2., requires_grad=True) b:  tensor([0.1377], requires_grad=True)\n",
      "epoch: 3600 y: tensor([[0.1383],\n",
      "        [0.1383],\n",
      "        [0.1383],\n",
      "        [0.1383]], grad_fn=<AddBackward0>) 9.689237594604492 w: tensor(2., requires_grad=True) b:  tensor([0.1383], requires_grad=True)\n",
      "epoch: 3700 y: tensor([[0.1389],\n",
      "        [0.1389],\n",
      "        [0.1389],\n",
      "        [0.1389]], grad_fn=<AddBackward0>) 9.685962677001953 w: tensor(2., requires_grad=True) b:  tensor([0.1389], requires_grad=True)\n",
      "epoch: 3800 y: tensor([[0.1395],\n",
      "        [0.1395],\n",
      "        [0.1395],\n",
      "        [0.1395]], grad_fn=<AddBackward0>) 9.68268871307373 w: tensor(2., requires_grad=True) b:  tensor([0.1395], requires_grad=True)\n",
      "epoch: 3900 y: tensor([[0.1400],\n",
      "        [0.1400],\n",
      "        [0.1400],\n",
      "        [0.1400]], grad_fn=<AddBackward0>) 9.679415702819824 w: tensor(2., requires_grad=True) b:  tensor([0.1400], requires_grad=True)\n",
      "epoch: 4000 y: tensor([[0.1406],\n",
      "        [0.1406],\n",
      "        [0.1406],\n",
      "        [0.1406]], grad_fn=<AddBackward0>) 9.676142692565918 w: tensor(2., requires_grad=True) b:  tensor([0.1406], requires_grad=True)\n",
      "epoch: 4100 y: tensor([[0.1412],\n",
      "        [0.1412],\n",
      "        [0.1412],\n",
      "        [0.1412]], grad_fn=<AddBackward0>) 9.672870635986328 w: tensor(2., requires_grad=True) b:  tensor([0.1412], requires_grad=True)\n",
      "epoch: 4200 y: tensor([[0.1417],\n",
      "        [0.1417],\n",
      "        [0.1417],\n",
      "        [0.1417]], grad_fn=<AddBackward0>) 9.669599533081055 w: tensor(2., requires_grad=True) b:  tensor([0.1418], requires_grad=True)\n",
      "epoch: 4300 y: tensor([[0.1423],\n",
      "        [0.1423],\n",
      "        [0.1423],\n",
      "        [0.1423]], grad_fn=<AddBackward0>) 9.666328430175781 w: tensor(2., requires_grad=True) b:  tensor([0.1423], requires_grad=True)\n",
      "epoch: 4400 y: tensor([[0.1429],\n",
      "        [0.1429],\n",
      "        [0.1429],\n",
      "        [0.1429]], grad_fn=<AddBackward0>) 9.66306209564209 w: tensor(2., requires_grad=True) b:  tensor([0.1429], requires_grad=True)\n",
      "epoch: 4500 y: tensor([[0.1435],\n",
      "        [0.1435],\n",
      "        [0.1435],\n",
      "        [0.1435]], grad_fn=<AddBackward0>) 9.659801483154297 w: tensor(2., requires_grad=True) b:  tensor([0.1435], requires_grad=True)\n",
      "epoch: 4600 y: tensor([[0.1440],\n",
      "        [0.1440],\n",
      "        [0.1440],\n",
      "        [0.1440]], grad_fn=<AddBackward0>) 9.65654182434082 w: tensor(2., requires_grad=True) b:  tensor([0.1440], requires_grad=True)\n",
      "epoch: 4700 y: tensor([[0.1446],\n",
      "        [0.1446],\n",
      "        [0.1446],\n",
      "        [0.1446]], grad_fn=<AddBackward0>) 9.653281211853027 w: tensor(2., requires_grad=True) b:  tensor([0.1446], requires_grad=True)\n",
      "epoch: 4800 y: tensor([[0.1452],\n",
      "        [0.1452],\n",
      "        [0.1452],\n",
      "        [0.1452]], grad_fn=<AddBackward0>) 9.650022506713867 w: tensor(2., requires_grad=True) b:  tensor([0.1452], requires_grad=True)\n",
      "epoch: 4900 y: tensor([[0.1457],\n",
      "        [0.1457],\n",
      "        [0.1457],\n",
      "        [0.1457]], grad_fn=<AddBackward0>) 9.646764755249023 w: tensor(2., requires_grad=True) b:  tensor([0.1458], requires_grad=True)\n",
      "epoch: 5000 y: tensor([[0.1463],\n",
      "        [0.1463],\n",
      "        [0.1463],\n",
      "        [0.1463]], grad_fn=<AddBackward0>) 9.64350700378418 w: tensor(2., requires_grad=True) b:  tensor([0.1463], requires_grad=True)\n",
      "epoch: 5100 y: tensor([[0.1469],\n",
      "        [0.1469],\n",
      "        [0.1469],\n",
      "        [0.1469]], grad_fn=<AddBackward0>) 9.640249252319336 w: tensor(2., requires_grad=True) b:  tensor([0.1469], requires_grad=True)\n",
      "epoch: 5200 y: tensor([[0.1475],\n",
      "        [0.1475],\n",
      "        [0.1475],\n",
      "        [0.1475]], grad_fn=<AddBackward0>) 9.636993408203125 w: tensor(2., requires_grad=True) b:  tensor([0.1475], requires_grad=True)\n",
      "epoch: 5300 y: tensor([[0.1480],\n",
      "        [0.1480],\n",
      "        [0.1480],\n",
      "        [0.1480]], grad_fn=<AddBackward0>) 9.63373851776123 w: tensor(2., requires_grad=True) b:  tensor([0.1480], requires_grad=True)\n",
      "epoch: 5400 y: tensor([[0.1486],\n",
      "        [0.1486],\n",
      "        [0.1486],\n",
      "        [0.1486]], grad_fn=<AddBackward0>) 9.63048267364502 w: tensor(2., requires_grad=True) b:  tensor([0.1486], requires_grad=True)\n",
      "epoch: 5500 y: tensor([[0.1492],\n",
      "        [0.1492],\n",
      "        [0.1492],\n",
      "        [0.1492]], grad_fn=<AddBackward0>) 9.627228736877441 w: tensor(2., requires_grad=True) b:  tensor([0.1492], requires_grad=True)\n",
      "epoch: 5600 y: tensor([[0.1497],\n",
      "        [0.1497],\n",
      "        [0.1497],\n",
      "        [0.1497]], grad_fn=<AddBackward0>) 9.62397575378418 w: tensor(2., requires_grad=True) b:  tensor([0.1497], requires_grad=True)\n",
      "epoch: 5700 y: tensor([[0.1503],\n",
      "        [0.1503],\n",
      "        [0.1503],\n",
      "        [0.1503]], grad_fn=<AddBackward0>) 9.62072467803955 w: tensor(2., requires_grad=True) b:  tensor([0.1503], requires_grad=True)\n",
      "epoch: 5800 y: tensor([[0.1509],\n",
      "        [0.1509],\n",
      "        [0.1509],\n",
      "        [0.1509]], grad_fn=<AddBackward0>) 9.617480278015137 w: tensor(2., requires_grad=True) b:  tensor([0.1509], requires_grad=True)\n",
      "epoch: 5900 y: tensor([[0.1515],\n",
      "        [0.1515],\n",
      "        [0.1515],\n",
      "        [0.1515]], grad_fn=<AddBackward0>) 9.614236831665039 w: tensor(2., requires_grad=True) b:  tensor([0.1515], requires_grad=True)\n",
      "epoch: 6000 y: tensor([[0.1520],\n",
      "        [0.1520],\n",
      "        [0.1520],\n",
      "        [0.1520]], grad_fn=<AddBackward0>) 9.610993385314941 w: tensor(2., requires_grad=True) b:  tensor([0.1520], requires_grad=True)\n",
      "epoch: 6100 y: tensor([[0.1526],\n",
      "        [0.1526],\n",
      "        [0.1526],\n",
      "        [0.1526]], grad_fn=<AddBackward0>) 9.607752799987793 w: tensor(2., requires_grad=True) b:  tensor([0.1526], requires_grad=True)\n",
      "epoch: 6200 y: tensor([[0.1532],\n",
      "        [0.1532],\n",
      "        [0.1532],\n",
      "        [0.1532]], grad_fn=<AddBackward0>) 9.604511260986328 w: tensor(2., requires_grad=True) b:  tensor([0.1532], requires_grad=True)\n",
      "epoch: 6300 y: tensor([[0.1537],\n",
      "        [0.1537],\n",
      "        [0.1537],\n",
      "        [0.1537]], grad_fn=<AddBackward0>) 9.601269721984863 w: tensor(2., requires_grad=True) b:  tensor([0.1537], requires_grad=True)\n",
      "epoch: 6400 y: tensor([[0.1543],\n",
      "        [0.1543],\n",
      "        [0.1543],\n",
      "        [0.1543]], grad_fn=<AddBackward0>) 9.598031044006348 w: tensor(2., requires_grad=True) b:  tensor([0.1543], requires_grad=True)\n",
      "epoch: 6500 y: tensor([[0.1549],\n",
      "        [0.1549],\n",
      "        [0.1549],\n",
      "        [0.1549]], grad_fn=<AddBackward0>) 9.594791412353516 w: tensor(2., requires_grad=True) b:  tensor([0.1549], requires_grad=True)\n",
      "epoch: 6600 y: tensor([[0.1554],\n",
      "        [0.1554],\n",
      "        [0.1554],\n",
      "        [0.1554]], grad_fn=<AddBackward0>) 9.591552734375 w: tensor(2., requires_grad=True) b:  tensor([0.1554], requires_grad=True)\n",
      "epoch: 6700 y: tensor([[0.1560],\n",
      "        [0.1560],\n",
      "        [0.1560],\n",
      "        [0.1560]], grad_fn=<AddBackward0>) 9.588314056396484 w: tensor(2., requires_grad=True) b:  tensor([0.1560], requires_grad=True)\n",
      "epoch: 6800 y: tensor([[0.1566],\n",
      "        [0.1566],\n",
      "        [0.1566],\n",
      "        [0.1566]], grad_fn=<AddBackward0>) 9.585076332092285 w: tensor(2., requires_grad=True) b:  tensor([0.1566], requires_grad=True)\n",
      "epoch: 6900 y: tensor([[0.1571],\n",
      "        [0.1571],\n",
      "        [0.1571],\n",
      "        [0.1571]], grad_fn=<AddBackward0>) 9.581840515136719 w: tensor(2., requires_grad=True) b:  tensor([0.1571], requires_grad=True)\n",
      "epoch: 7000 y: tensor([[0.1577],\n",
      "        [0.1577],\n",
      "        [0.1577],\n",
      "        [0.1577]], grad_fn=<AddBackward0>) 9.578605651855469 w: tensor(2., requires_grad=True) b:  tensor([0.1577], requires_grad=True)\n",
      "epoch: 7100 y: tensor([[0.1583],\n",
      "        [0.1583],\n",
      "        [0.1583],\n",
      "        [0.1583]], grad_fn=<AddBackward0>) 9.57537841796875 w: tensor(2., requires_grad=True) b:  tensor([0.1583], requires_grad=True)\n",
      "epoch: 7200 y: tensor([[0.1588],\n",
      "        [0.1588],\n",
      "        [0.1588],\n",
      "        [0.1588]], grad_fn=<AddBackward0>) 9.572152137756348 w: tensor(2., requires_grad=True) b:  tensor([0.1589], requires_grad=True)\n",
      "epoch: 7300 y: tensor([[0.1594],\n",
      "        [0.1594],\n",
      "        [0.1594],\n",
      "        [0.1594]], grad_fn=<AddBackward0>) 9.568925857543945 w: tensor(2., requires_grad=True) b:  tensor([0.1594], requires_grad=True)\n",
      "epoch: 7400 y: tensor([[0.1600],\n",
      "        [0.1600],\n",
      "        [0.1600],\n",
      "        [0.1600]], grad_fn=<AddBackward0>) 9.565701484680176 w: tensor(2., requires_grad=True) b:  tensor([0.1600], requires_grad=True)\n",
      "epoch: 7500 y: tensor([[0.1605],\n",
      "        [0.1605],\n",
      "        [0.1605],\n",
      "        [0.1605]], grad_fn=<AddBackward0>) 9.56247615814209 w: tensor(2., requires_grad=True) b:  tensor([0.1606], requires_grad=True)\n",
      "epoch: 7600 y: tensor([[0.1611],\n",
      "        [0.1611],\n",
      "        [0.1611],\n",
      "        [0.1611]], grad_fn=<AddBackward0>) 9.559253692626953 w: tensor(2., requires_grad=True) b:  tensor([0.1611], requires_grad=True)\n",
      "epoch: 7700 y: tensor([[0.1617],\n",
      "        [0.1617],\n",
      "        [0.1617],\n",
      "        [0.1617]], grad_fn=<AddBackward0>) 9.556029319763184 w: tensor(2., requires_grad=True) b:  tensor([0.1617], requires_grad=True)\n",
      "epoch: 7800 y: tensor([[0.1623],\n",
      "        [0.1623],\n",
      "        [0.1623],\n",
      "        [0.1623]], grad_fn=<AddBackward0>) 9.552807807922363 w: tensor(2., requires_grad=True) b:  tensor([0.1623], requires_grad=True)\n",
      "epoch: 7900 y: tensor([[0.1628],\n",
      "        [0.1628],\n",
      "        [0.1628],\n",
      "        [0.1628]], grad_fn=<AddBackward0>) 9.549585342407227 w: tensor(2., requires_grad=True) b:  tensor([0.1628], requires_grad=True)\n",
      "epoch: 8000 y: tensor([[0.1634],\n",
      "        [0.1634],\n",
      "        [0.1634],\n",
      "        [0.1634]], grad_fn=<AddBackward0>) 9.546364784240723 w: tensor(2., requires_grad=True) b:  tensor([0.1634], requires_grad=True)\n",
      "epoch: 8100 y: tensor([[0.1640],\n",
      "        [0.1640],\n",
      "        [0.1640],\n",
      "        [0.1640]], grad_fn=<AddBackward0>) 9.543144226074219 w: tensor(2., requires_grad=True) b:  tensor([0.1640], requires_grad=True)\n",
      "epoch: 8200 y: tensor([[0.1645],\n",
      "        [0.1645],\n",
      "        [0.1645],\n",
      "        [0.1645]], grad_fn=<AddBackward0>) 9.539924621582031 w: tensor(2., requires_grad=True) b:  tensor([0.1645], requires_grad=True)\n",
      "epoch: 8300 y: tensor([[0.1651],\n",
      "        [0.1651],\n",
      "        [0.1651],\n",
      "        [0.1651]], grad_fn=<AddBackward0>) 9.536705017089844 w: tensor(2., requires_grad=True) b:  tensor([0.1651], requires_grad=True)\n",
      "epoch: 8400 y: tensor([[0.1657],\n",
      "        [0.1657],\n",
      "        [0.1657],\n",
      "        [0.1657]], grad_fn=<AddBackward0>) 9.53349494934082 w: tensor(2., requires_grad=True) b:  tensor([0.1657], requires_grad=True)\n",
      "epoch: 8500 y: tensor([[0.1662],\n",
      "        [0.1662],\n",
      "        [0.1662],\n",
      "        [0.1662]], grad_fn=<AddBackward0>) 9.530285835266113 w: tensor(2., requires_grad=True) b:  tensor([0.1662], requires_grad=True)\n",
      "epoch: 8600 y: tensor([[0.1668],\n",
      "        [0.1668],\n",
      "        [0.1668],\n",
      "        [0.1668]], grad_fn=<AddBackward0>) 9.527076721191406 w: tensor(2., requires_grad=True) b:  tensor([0.1668], requires_grad=True)\n",
      "epoch: 8700 y: tensor([[0.1674],\n",
      "        [0.1674],\n",
      "        [0.1674],\n",
      "        [0.1674]], grad_fn=<AddBackward0>) 9.523868560791016 w: tensor(2., requires_grad=True) b:  tensor([0.1674], requires_grad=True)\n",
      "epoch: 8800 y: tensor([[0.1679],\n",
      "        [0.1679],\n",
      "        [0.1679],\n",
      "        [0.1679]], grad_fn=<AddBackward0>) 9.520660400390625 w: tensor(2., requires_grad=True) b:  tensor([0.1679], requires_grad=True)\n",
      "epoch: 8900 y: tensor([[0.1685],\n",
      "        [0.1685],\n",
      "        [0.1685],\n",
      "        [0.1685]], grad_fn=<AddBackward0>) 9.51745319366455 w: tensor(2., requires_grad=True) b:  tensor([0.1685], requires_grad=True)\n",
      "epoch: 9000 y: tensor([[0.1691],\n",
      "        [0.1691],\n",
      "        [0.1691],\n",
      "        [0.1691]], grad_fn=<AddBackward0>) 9.514246940612793 w: tensor(2., requires_grad=True) b:  tensor([0.1691], requires_grad=True)\n",
      "epoch: 9100 y: tensor([[0.1696],\n",
      "        [0.1696],\n",
      "        [0.1696],\n",
      "        [0.1696]], grad_fn=<AddBackward0>) 9.511041641235352 w: tensor(2., requires_grad=True) b:  tensor([0.1696], requires_grad=True)\n",
      "epoch: 9200 y: tensor([[0.1702],\n",
      "        [0.1702],\n",
      "        [0.1702],\n",
      "        [0.1702]], grad_fn=<AddBackward0>) 9.50783634185791 w: tensor(2., requires_grad=True) b:  tensor([0.1702], requires_grad=True)\n",
      "epoch: 9300 y: tensor([[0.1708],\n",
      "        [0.1708],\n",
      "        [0.1708],\n",
      "        [0.1708]], grad_fn=<AddBackward0>) 9.504631996154785 w: tensor(2., requires_grad=True) b:  tensor([0.1708], requires_grad=True)\n",
      "epoch: 9400 y: tensor([[0.1713],\n",
      "        [0.1713],\n",
      "        [0.1713],\n",
      "        [0.1713]], grad_fn=<AddBackward0>) 9.501428604125977 w: tensor(2., requires_grad=True) b:  tensor([0.1713], requires_grad=True)\n",
      "epoch: 9500 y: tensor([[0.1719],\n",
      "        [0.1719],\n",
      "        [0.1719],\n",
      "        [0.1719]], grad_fn=<AddBackward0>) 9.498224258422852 w: tensor(2., requires_grad=True) b:  tensor([0.1719], requires_grad=True)\n",
      "epoch: 9600 y: tensor([[0.1725],\n",
      "        [0.1725],\n",
      "        [0.1725],\n",
      "        [0.1725]], grad_fn=<AddBackward0>) 9.49502182006836 w: tensor(2., requires_grad=True) b:  tensor([0.1725], requires_grad=True)\n",
      "epoch: 9700 y: tensor([[0.1730],\n",
      "        [0.1730],\n",
      "        [0.1730],\n",
      "        [0.1730]], grad_fn=<AddBackward0>) 9.491828918457031 w: tensor(2., requires_grad=True) b:  tensor([0.1730], requires_grad=True)\n",
      "epoch: 9800 y: tensor([[0.1736],\n",
      "        [0.1736],\n",
      "        [0.1736],\n",
      "        [0.1736]], grad_fn=<AddBackward0>) 9.488636016845703 w: tensor(2., requires_grad=True) b:  tensor([0.1736], requires_grad=True)\n",
      "epoch: 9900 y: tensor([[0.1741],\n",
      "        [0.1741],\n",
      "        [0.1741],\n",
      "        [0.1741]], grad_fn=<AddBackward0>) 9.485443115234375 w: tensor(2., requires_grad=True) b:  tensor([0.1742], requires_grad=True)\n",
      "epoch: 10000 y: tensor([[0.1747],\n",
      "        [0.1747],\n",
      "        [0.1747],\n",
      "        [0.1747]], grad_fn=<AddBackward0>) 9.48225212097168 w: tensor(2., requires_grad=True) b:  tensor([0.1747], requires_grad=True)\n",
      "epoch: 10100 y: tensor([[0.1753],\n",
      "        [0.1753],\n",
      "        [0.1753],\n",
      "        [0.1753]], grad_fn=<AddBackward0>) 9.479061126708984 w: tensor(2., requires_grad=True) b:  tensor([0.1753], requires_grad=True)\n",
      "epoch: 10200 y: tensor([[0.1758],\n",
      "        [0.1758],\n",
      "        [0.1758],\n",
      "        [0.1758]], grad_fn=<AddBackward0>) 9.475870132446289 w: tensor(2., requires_grad=True) b:  tensor([0.1758], requires_grad=True)\n",
      "epoch: 10300 y: tensor([[0.1764],\n",
      "        [0.1764],\n",
      "        [0.1764],\n",
      "        [0.1764]], grad_fn=<AddBackward0>) 9.472681045532227 w: tensor(2., requires_grad=True) b:  tensor([0.1764], requires_grad=True)\n",
      "epoch: 10400 y: tensor([[0.1770],\n",
      "        [0.1770],\n",
      "        [0.1770],\n",
      "        [0.1770]], grad_fn=<AddBackward0>) 9.46949291229248 w: tensor(2., requires_grad=True) b:  tensor([0.1770], requires_grad=True)\n",
      "epoch: 10500 y: tensor([[0.1775],\n",
      "        [0.1775],\n",
      "        [0.1775],\n",
      "        [0.1775]], grad_fn=<AddBackward0>) 9.466303825378418 w: tensor(2., requires_grad=True) b:  tensor([0.1775], requires_grad=True)\n",
      "epoch: 10600 y: tensor([[0.1781],\n",
      "        [0.1781],\n",
      "        [0.1781],\n",
      "        [0.1781]], grad_fn=<AddBackward0>) 9.463115692138672 w: tensor(2., requires_grad=True) b:  tensor([0.1781], requires_grad=True)\n",
      "epoch: 10700 y: tensor([[0.1787],\n",
      "        [0.1787],\n",
      "        [0.1787],\n",
      "        [0.1787]], grad_fn=<AddBackward0>) 9.459929466247559 w: tensor(2., requires_grad=True) b:  tensor([0.1787], requires_grad=True)\n",
      "epoch: 10800 y: tensor([[0.1792],\n",
      "        [0.1792],\n",
      "        [0.1792],\n",
      "        [0.1792]], grad_fn=<AddBackward0>) 9.456743240356445 w: tensor(2., requires_grad=True) b:  tensor([0.1792], requires_grad=True)\n",
      "epoch: 10900 y: tensor([[0.1798],\n",
      "        [0.1798],\n",
      "        [0.1798],\n",
      "        [0.1798]], grad_fn=<AddBackward0>) 9.453557968139648 w: tensor(2., requires_grad=True) b:  tensor([0.1798], requires_grad=True)\n",
      "epoch: 11000 y: tensor([[0.1804],\n",
      "        [0.1804],\n",
      "        [0.1804],\n",
      "        [0.1804]], grad_fn=<AddBackward0>) 9.45037841796875 w: tensor(2., requires_grad=True) b:  tensor([0.1804], requires_grad=True)\n",
      "epoch: 11100 y: tensor([[0.1809],\n",
      "        [0.1809],\n",
      "        [0.1809],\n",
      "        [0.1809]], grad_fn=<AddBackward0>) 9.4472017288208 w: tensor(2., requires_grad=True) b:  tensor([0.1809], requires_grad=True)\n",
      "epoch: 11200 y: tensor([[0.1815],\n",
      "        [0.1815],\n",
      "        [0.1815],\n",
      "        [0.1815]], grad_fn=<AddBackward0>) 9.444026947021484 w: tensor(2., requires_grad=True) b:  tensor([0.1815], requires_grad=True)\n",
      "epoch: 11300 y: tensor([[0.1820],\n",
      "        [0.1820],\n",
      "        [0.1820],\n",
      "        [0.1820]], grad_fn=<AddBackward0>) 9.440851211547852 w: tensor(2., requires_grad=True) b:  tensor([0.1821], requires_grad=True)\n",
      "epoch: 11400 y: tensor([[0.1826],\n",
      "        [0.1826],\n",
      "        [0.1826],\n",
      "        [0.1826]], grad_fn=<AddBackward0>) 9.437678337097168 w: tensor(2., requires_grad=True) b:  tensor([0.1826], requires_grad=True)\n",
      "epoch: 11500 y: tensor([[0.1832],\n",
      "        [0.1832],\n",
      "        [0.1832],\n",
      "        [0.1832]], grad_fn=<AddBackward0>) 9.434504508972168 w: tensor(2., requires_grad=True) b:  tensor([0.1832], requires_grad=True)\n",
      "epoch: 11600 y: tensor([[0.1837],\n",
      "        [0.1837],\n",
      "        [0.1837],\n",
      "        [0.1837]], grad_fn=<AddBackward0>) 9.431331634521484 w: tensor(2., requires_grad=True) b:  tensor([0.1837], requires_grad=True)\n",
      "epoch: 11700 y: tensor([[0.1843],\n",
      "        [0.1843],\n",
      "        [0.1843],\n",
      "        [0.1843]], grad_fn=<AddBackward0>) 9.428157806396484 w: tensor(2., requires_grad=True) b:  tensor([0.1843], requires_grad=True)\n",
      "epoch: 11800 y: tensor([[0.1849],\n",
      "        [0.1849],\n",
      "        [0.1849],\n",
      "        [0.1849]], grad_fn=<AddBackward0>) 9.42498779296875 w: tensor(2., requires_grad=True) b:  tensor([0.1849], requires_grad=True)\n",
      "epoch: 11900 y: tensor([[0.1854],\n",
      "        [0.1854],\n",
      "        [0.1854],\n",
      "        [0.1854]], grad_fn=<AddBackward0>) 9.421815872192383 w: tensor(2., requires_grad=True) b:  tensor([0.1854], requires_grad=True)\n",
      "epoch: 12000 y: tensor([[0.1860],\n",
      "        [0.1860],\n",
      "        [0.1860],\n",
      "        [0.1860]], grad_fn=<AddBackward0>) 9.418645858764648 w: tensor(2., requires_grad=True) b:  tensor([0.1860], requires_grad=True)\n",
      "epoch: 12100 y: tensor([[0.1866],\n",
      "        [0.1866],\n",
      "        [0.1866],\n",
      "        [0.1866]], grad_fn=<AddBackward0>) 9.415475845336914 w: tensor(2., requires_grad=True) b:  tensor([0.1866], requires_grad=True)\n",
      "epoch: 12200 y: tensor([[0.1871],\n",
      "        [0.1871],\n",
      "        [0.1871],\n",
      "        [0.1871]], grad_fn=<AddBackward0>) 9.412307739257812 w: tensor(2., requires_grad=True) b:  tensor([0.1871], requires_grad=True)\n",
      "epoch: 12300 y: tensor([[0.1877],\n",
      "        [0.1877],\n",
      "        [0.1877],\n",
      "        [0.1877]], grad_fn=<AddBackward0>) 9.40914249420166 w: tensor(2., requires_grad=True) b:  tensor([0.1877], requires_grad=True)\n",
      "epoch: 12400 y: tensor([[0.1882],\n",
      "        [0.1882],\n",
      "        [0.1882],\n",
      "        [0.1882]], grad_fn=<AddBackward0>) 9.405982971191406 w: tensor(2., requires_grad=True) b:  tensor([0.1882], requires_grad=True)\n",
      "epoch: 12500 y: tensor([[0.1888],\n",
      "        [0.1888],\n",
      "        [0.1888],\n",
      "        [0.1888]], grad_fn=<AddBackward0>) 9.402824401855469 w: tensor(2., requires_grad=True) b:  tensor([0.1888], requires_grad=True)\n",
      "epoch: 12600 y: tensor([[0.1894],\n",
      "        [0.1894],\n",
      "        [0.1894],\n",
      "        [0.1894]], grad_fn=<AddBackward0>) 9.399665832519531 w: tensor(2., requires_grad=True) b:  tensor([0.1894], requires_grad=True)\n",
      "epoch: 12700 y: tensor([[0.1899],\n",
      "        [0.1899],\n",
      "        [0.1899],\n",
      "        [0.1899]], grad_fn=<AddBackward0>) 9.396509170532227 w: tensor(2., requires_grad=True) b:  tensor([0.1899], requires_grad=True)\n",
      "epoch: 12800 y: tensor([[0.1905],\n",
      "        [0.1905],\n",
      "        [0.1905],\n",
      "        [0.1905]], grad_fn=<AddBackward0>) 9.393351554870605 w: tensor(2., requires_grad=True) b:  tensor([0.1905], requires_grad=True)\n",
      "epoch: 12900 y: tensor([[0.1911],\n",
      "        [0.1911],\n",
      "        [0.1911],\n",
      "        [0.1911]], grad_fn=<AddBackward0>) 9.390195846557617 w: tensor(2., requires_grad=True) b:  tensor([0.1911], requires_grad=True)\n",
      "epoch: 13000 y: tensor([[0.1916],\n",
      "        [0.1916],\n",
      "        [0.1916],\n",
      "        [0.1916]], grad_fn=<AddBackward0>) 9.387039184570312 w: tensor(2., requires_grad=True) b:  tensor([0.1916], requires_grad=True)\n",
      "epoch: 13100 y: tensor([[0.1922],\n",
      "        [0.1922],\n",
      "        [0.1922],\n",
      "        [0.1922]], grad_fn=<AddBackward0>) 9.38388442993164 w: tensor(2., requires_grad=True) b:  tensor([0.1922], requires_grad=True)\n",
      "epoch: 13200 y: tensor([[0.1927],\n",
      "        [0.1927],\n",
      "        [0.1927],\n",
      "        [0.1927]], grad_fn=<AddBackward0>) 9.380729675292969 w: tensor(2., requires_grad=True) b:  tensor([0.1927], requires_grad=True)\n",
      "epoch: 13300 y: tensor([[0.1933],\n",
      "        [0.1933],\n",
      "        [0.1933],\n",
      "        [0.1933]], grad_fn=<AddBackward0>) 9.37757682800293 w: tensor(2., requires_grad=True) b:  tensor([0.1933], requires_grad=True)\n",
      "epoch: 13400 y: tensor([[0.1939],\n",
      "        [0.1939],\n",
      "        [0.1939],\n",
      "        [0.1939]], grad_fn=<AddBackward0>) 9.374423027038574 w: tensor(2., requires_grad=True) b:  tensor([0.1939], requires_grad=True)\n",
      "epoch: 13500 y: tensor([[0.1944],\n",
      "        [0.1944],\n",
      "        [0.1944],\n",
      "        [0.1944]], grad_fn=<AddBackward0>) 9.371271133422852 w: tensor(2., requires_grad=True) b:  tensor([0.1944], requires_grad=True)\n",
      "epoch: 13600 y: tensor([[0.1950],\n",
      "        [0.1950],\n",
      "        [0.1950],\n",
      "        [0.1950]], grad_fn=<AddBackward0>) 9.368120193481445 w: tensor(2., requires_grad=True) b:  tensor([0.1950], requires_grad=True)\n",
      "epoch: 13700 y: tensor([[0.1955],\n",
      "        [0.1955],\n",
      "        [0.1955],\n",
      "        [0.1955]], grad_fn=<AddBackward0>) 9.364977836608887 w: tensor(2., requires_grad=True) b:  tensor([0.1955], requires_grad=True)\n",
      "epoch: 13800 y: tensor([[0.1961],\n",
      "        [0.1961],\n",
      "        [0.1961],\n",
      "        [0.1961]], grad_fn=<AddBackward0>) 9.361835479736328 w: tensor(2., requires_grad=True) b:  tensor([0.1961], requires_grad=True)\n",
      "epoch: 13900 y: tensor([[0.1967],\n",
      "        [0.1967],\n",
      "        [0.1967],\n",
      "        [0.1967]], grad_fn=<AddBackward0>) 9.358694076538086 w: tensor(2., requires_grad=True) b:  tensor([0.1967], requires_grad=True)\n",
      "epoch: 14000 y: tensor([[0.1972],\n",
      "        [0.1972],\n",
      "        [0.1972],\n",
      "        [0.1972]], grad_fn=<AddBackward0>) 9.355552673339844 w: tensor(2., requires_grad=True) b:  tensor([0.1972], requires_grad=True)\n",
      "epoch: 14100 y: tensor([[0.1978],\n",
      "        [0.1978],\n",
      "        [0.1978],\n",
      "        [0.1978]], grad_fn=<AddBackward0>) 9.352412223815918 w: tensor(2., requires_grad=True) b:  tensor([0.1978], requires_grad=True)\n",
      "epoch: 14200 y: tensor([[0.1983],\n",
      "        [0.1983],\n",
      "        [0.1983],\n",
      "        [0.1983]], grad_fn=<AddBackward0>) 9.349271774291992 w: tensor(2., requires_grad=True) b:  tensor([0.1984], requires_grad=True)\n",
      "epoch: 14300 y: tensor([[0.1989],\n",
      "        [0.1989],\n",
      "        [0.1989],\n",
      "        [0.1989]], grad_fn=<AddBackward0>) 9.3461332321167 w: tensor(2., requires_grad=True) b:  tensor([0.1989], requires_grad=True)\n",
      "epoch: 14400 y: tensor([[0.1995],\n",
      "        [0.1995],\n",
      "        [0.1995],\n",
      "        [0.1995]], grad_fn=<AddBackward0>) 9.342994689941406 w: tensor(2., requires_grad=True) b:  tensor([0.1995], requires_grad=True)\n",
      "epoch: 14500 y: tensor([[0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000]], grad_fn=<AddBackward0>) 9.33985710144043 w: tensor(2., requires_grad=True) b:  tensor([0.2000], requires_grad=True)\n",
      "epoch: 14600 y: tensor([[0.2006],\n",
      "        [0.2006],\n",
      "        [0.2006],\n",
      "        [0.2006]], grad_fn=<AddBackward0>) 9.336719512939453 w: tensor(2., requires_grad=True) b:  tensor([0.2006], requires_grad=True)\n",
      "epoch: 14700 y: tensor([[0.2011],\n",
      "        [0.2011],\n",
      "        [0.2011],\n",
      "        [0.2011]], grad_fn=<AddBackward0>) 9.333582878112793 w: tensor(2., requires_grad=True) b:  tensor([0.2012], requires_grad=True)\n",
      "epoch: 14800 y: tensor([[0.2017],\n",
      "        [0.2017],\n",
      "        [0.2017],\n",
      "        [0.2017]], grad_fn=<AddBackward0>) 9.33044719696045 w: tensor(2., requires_grad=True) b:  tensor([0.2017], requires_grad=True)\n",
      "epoch: 14900 y: tensor([[0.2023],\n",
      "        [0.2023],\n",
      "        [0.2023],\n",
      "        [0.2023]], grad_fn=<AddBackward0>) 9.327312469482422 w: tensor(2., requires_grad=True) b:  tensor([0.2023], requires_grad=True)\n",
      "epoch: 15000 y: tensor([[0.2028],\n",
      "        [0.2028],\n",
      "        [0.2028],\n",
      "        [0.2028]], grad_fn=<AddBackward0>) 9.32418441772461 w: tensor(2., requires_grad=True) b:  tensor([0.2028], requires_grad=True)\n",
      "epoch: 15100 y: tensor([[0.2034],\n",
      "        [0.2034],\n",
      "        [0.2034],\n",
      "        [0.2034]], grad_fn=<AddBackward0>) 9.321059226989746 w: tensor(2., requires_grad=True) b:  tensor([0.2034], requires_grad=True)\n",
      "epoch: 15200 y: tensor([[0.2039],\n",
      "        [0.2039],\n",
      "        [0.2039],\n",
      "        [0.2039]], grad_fn=<AddBackward0>) 9.317933082580566 w: tensor(2., requires_grad=True) b:  tensor([0.2039], requires_grad=True)\n",
      "epoch: 15300 y: tensor([[0.2045],\n",
      "        [0.2045],\n",
      "        [0.2045],\n",
      "        [0.2045]], grad_fn=<AddBackward0>) 9.314809799194336 w: tensor(2., requires_grad=True) b:  tensor([0.2045], requires_grad=True)\n",
      "epoch: 15400 y: tensor([[0.2051],\n",
      "        [0.2051],\n",
      "        [0.2051],\n",
      "        [0.2051]], grad_fn=<AddBackward0>) 9.311685562133789 w: tensor(2., requires_grad=True) b:  tensor([0.2051], requires_grad=True)\n",
      "epoch: 15500 y: tensor([[0.2056],\n",
      "        [0.2056],\n",
      "        [0.2056],\n",
      "        [0.2056]], grad_fn=<AddBackward0>) 9.308562278747559 w: tensor(2., requires_grad=True) b:  tensor([0.2056], requires_grad=True)\n",
      "epoch: 15600 y: tensor([[0.2062],\n",
      "        [0.2062],\n",
      "        [0.2062],\n",
      "        [0.2062]], grad_fn=<AddBackward0>) 9.305438995361328 w: tensor(2., requires_grad=True) b:  tensor([0.2062], requires_grad=True)\n",
      "epoch: 15700 y: tensor([[0.2067],\n",
      "        [0.2067],\n",
      "        [0.2067],\n",
      "        [0.2067]], grad_fn=<AddBackward0>) 9.30231761932373 w: tensor(2., requires_grad=True) b:  tensor([0.2067], requires_grad=True)\n",
      "epoch: 15800 y: tensor([[0.2073],\n",
      "        [0.2073],\n",
      "        [0.2073],\n",
      "        [0.2073]], grad_fn=<AddBackward0>) 9.299196243286133 w: tensor(2., requires_grad=True) b:  tensor([0.2073], requires_grad=True)\n",
      "epoch: 15900 y: tensor([[0.2079],\n",
      "        [0.2079],\n",
      "        [0.2079],\n",
      "        [0.2079]], grad_fn=<AddBackward0>) 9.296075820922852 w: tensor(2., requires_grad=True) b:  tensor([0.2079], requires_grad=True)\n",
      "epoch: 16000 y: tensor([[0.2084],\n",
      "        [0.2084],\n",
      "        [0.2084],\n",
      "        [0.2084]], grad_fn=<AddBackward0>) 9.29295539855957 w: tensor(2., requires_grad=True) b:  tensor([0.2084], requires_grad=True)\n",
      "epoch: 16100 y: tensor([[0.2090],\n",
      "        [0.2090],\n",
      "        [0.2090],\n",
      "        [0.2090]], grad_fn=<AddBackward0>) 9.289834976196289 w: tensor(2., requires_grad=True) b:  tensor([0.2090], requires_grad=True)\n",
      "epoch: 16200 y: tensor([[0.2095],\n",
      "        [0.2095],\n",
      "        [0.2095],\n",
      "        [0.2095]], grad_fn=<AddBackward0>) 9.28671646118164 w: tensor(2., requires_grad=True) b:  tensor([0.2095], requires_grad=True)\n",
      "epoch: 16300 y: tensor([[0.2101],\n",
      "        [0.2101],\n",
      "        [0.2101],\n",
      "        [0.2101]], grad_fn=<AddBackward0>) 9.28360366821289 w: tensor(2., requires_grad=True) b:  tensor([0.2101], requires_grad=True)\n",
      "epoch: 16400 y: tensor([[0.2106],\n",
      "        [0.2106],\n",
      "        [0.2106],\n",
      "        [0.2106]], grad_fn=<AddBackward0>) 9.28049373626709 w: tensor(2., requires_grad=True) b:  tensor([0.2107], requires_grad=True)\n",
      "epoch: 16500 y: tensor([[0.2112],\n",
      "        [0.2112],\n",
      "        [0.2112],\n",
      "        [0.2112]], grad_fn=<AddBackward0>) 9.277385711669922 w: tensor(2., requires_grad=True) b:  tensor([0.2112], requires_grad=True)\n",
      "epoch: 16600 y: tensor([[0.2118],\n",
      "        [0.2118],\n",
      "        [0.2118],\n",
      "        [0.2118]], grad_fn=<AddBackward0>) 9.274276733398438 w: tensor(2., requires_grad=True) b:  tensor([0.2118], requires_grad=True)\n",
      "epoch: 16700 y: tensor([[0.2123],\n",
      "        [0.2123],\n",
      "        [0.2123],\n",
      "        [0.2123]], grad_fn=<AddBackward0>) 9.271169662475586 w: tensor(2., requires_grad=True) b:  tensor([0.2123], requires_grad=True)\n",
      "epoch: 16800 y: tensor([[0.2129],\n",
      "        [0.2129],\n",
      "        [0.2129],\n",
      "        [0.2129]], grad_fn=<AddBackward0>) 9.268062591552734 w: tensor(2., requires_grad=True) b:  tensor([0.2129], requires_grad=True)\n",
      "epoch: 16900 y: tensor([[0.2134],\n",
      "        [0.2134],\n",
      "        [0.2134],\n",
      "        [0.2134]], grad_fn=<AddBackward0>) 9.2649564743042 w: tensor(2., requires_grad=True) b:  tensor([0.2134], requires_grad=True)\n",
      "epoch: 17000 y: tensor([[0.2140],\n",
      "        [0.2140],\n",
      "        [0.2140],\n",
      "        [0.2140]], grad_fn=<AddBackward0>) 9.261850357055664 w: tensor(2., requires_grad=True) b:  tensor([0.2140], requires_grad=True)\n",
      "epoch: 17100 y: tensor([[0.2145],\n",
      "        [0.2145],\n",
      "        [0.2145],\n",
      "        [0.2145]], grad_fn=<AddBackward0>) 9.258745193481445 w: tensor(2., requires_grad=True) b:  tensor([0.2146], requires_grad=True)\n",
      "epoch: 17200 y: tensor([[0.2151],\n",
      "        [0.2151],\n",
      "        [0.2151],\n",
      "        [0.2151]], grad_fn=<AddBackward0>) 9.25564193725586 w: tensor(2., requires_grad=True) b:  tensor([0.2151], requires_grad=True)\n",
      "epoch: 17300 y: tensor([[0.2157],\n",
      "        [0.2157],\n",
      "        [0.2157],\n",
      "        [0.2157]], grad_fn=<AddBackward0>) 9.252537727355957 w: tensor(2., requires_grad=True) b:  tensor([0.2157], requires_grad=True)\n",
      "epoch: 17400 y: tensor([[0.2162],\n",
      "        [0.2162],\n",
      "        [0.2162],\n",
      "        [0.2162]], grad_fn=<AddBackward0>) 9.249433517456055 w: tensor(2., requires_grad=True) b:  tensor([0.2162], requires_grad=True)\n",
      "epoch: 17500 y: tensor([[0.2168],\n",
      "        [0.2168],\n",
      "        [0.2168],\n",
      "        [0.2168]], grad_fn=<AddBackward0>) 9.246332168579102 w: tensor(2., requires_grad=True) b:  tensor([0.2168], requires_grad=True)\n",
      "epoch: 17600 y: tensor([[0.2173],\n",
      "        [0.2173],\n",
      "        [0.2173],\n",
      "        [0.2173]], grad_fn=<AddBackward0>) 9.243231773376465 w: tensor(2., requires_grad=True) b:  tensor([0.2173], requires_grad=True)\n",
      "epoch: 17700 y: tensor([[0.2179],\n",
      "        [0.2179],\n",
      "        [0.2179],\n",
      "        [0.2179]], grad_fn=<AddBackward0>) 9.240139961242676 w: tensor(2., requires_grad=True) b:  tensor([0.2179], requires_grad=True)\n",
      "epoch: 17800 y: tensor([[0.2184],\n",
      "        [0.2184],\n",
      "        [0.2184],\n",
      "        [0.2184]], grad_fn=<AddBackward0>) 9.23704719543457 w: tensor(2., requires_grad=True) b:  tensor([0.2185], requires_grad=True)\n",
      "epoch: 17900 y: tensor([[0.2190],\n",
      "        [0.2190],\n",
      "        [0.2190],\n",
      "        [0.2190]], grad_fn=<AddBackward0>) 9.233955383300781 w: tensor(2., requires_grad=True) b:  tensor([0.2190], requires_grad=True)\n",
      "epoch: 18000 y: tensor([[0.2196],\n",
      "        [0.2196],\n",
      "        [0.2196],\n",
      "        [0.2196]], grad_fn=<AddBackward0>) 9.230864524841309 w: tensor(2., requires_grad=True) b:  tensor([0.2196], requires_grad=True)\n",
      "epoch: 18100 y: tensor([[0.2201],\n",
      "        [0.2201],\n",
      "        [0.2201],\n",
      "        [0.2201]], grad_fn=<AddBackward0>) 9.22777271270752 w: tensor(2., requires_grad=True) b:  tensor([0.2201], requires_grad=True)\n",
      "epoch: 18200 y: tensor([[0.2207],\n",
      "        [0.2207],\n",
      "        [0.2207],\n",
      "        [0.2207]], grad_fn=<AddBackward0>) 9.22468376159668 w: tensor(2., requires_grad=True) b:  tensor([0.2207], requires_grad=True)\n",
      "epoch: 18300 y: tensor([[0.2212],\n",
      "        [0.2212],\n",
      "        [0.2212],\n",
      "        [0.2212]], grad_fn=<AddBackward0>) 9.221593856811523 w: tensor(2., requires_grad=True) b:  tensor([0.2212], requires_grad=True)\n",
      "epoch: 18400 y: tensor([[0.2218],\n",
      "        [0.2218],\n",
      "        [0.2218],\n",
      "        [0.2218]], grad_fn=<AddBackward0>) 9.218505859375 w: tensor(2., requires_grad=True) b:  tensor([0.2218], requires_grad=True)\n",
      "epoch: 18500 y: tensor([[0.2223],\n",
      "        [0.2223],\n",
      "        [0.2223],\n",
      "        [0.2223]], grad_fn=<AddBackward0>) 9.215417861938477 w: tensor(2., requires_grad=True) b:  tensor([0.2223], requires_grad=True)\n",
      "epoch: 18600 y: tensor([[0.2229],\n",
      "        [0.2229],\n",
      "        [0.2229],\n",
      "        [0.2229]], grad_fn=<AddBackward0>) 9.212329864501953 w: tensor(2., requires_grad=True) b:  tensor([0.2229], requires_grad=True)\n",
      "epoch: 18700 y: tensor([[0.2234],\n",
      "        [0.2234],\n",
      "        [0.2234],\n",
      "        [0.2234]], grad_fn=<AddBackward0>) 9.209242820739746 w: tensor(2., requires_grad=True) b:  tensor([0.2235], requires_grad=True)\n",
      "epoch: 18800 y: tensor([[0.2240],\n",
      "        [0.2240],\n",
      "        [0.2240],\n",
      "        [0.2240]], grad_fn=<AddBackward0>) 9.206157684326172 w: tensor(2., requires_grad=True) b:  tensor([0.2240], requires_grad=True)\n",
      "epoch: 18900 y: tensor([[0.2246],\n",
      "        [0.2246],\n",
      "        [0.2246],\n",
      "        [0.2246]], grad_fn=<AddBackward0>) 9.203070640563965 w: tensor(2., requires_grad=True) b:  tensor([0.2246], requires_grad=True)\n",
      "epoch: 19000 y: tensor([[0.2251],\n",
      "        [0.2251],\n",
      "        [0.2251],\n",
      "        [0.2251]], grad_fn=<AddBackward0>) 9.199993133544922 w: tensor(2., requires_grad=True) b:  tensor([0.2251], requires_grad=True)\n",
      "epoch: 19100 y: tensor([[0.2257],\n",
      "        [0.2257],\n",
      "        [0.2257],\n",
      "        [0.2257]], grad_fn=<AddBackward0>) 9.196916580200195 w: tensor(2., requires_grad=True) b:  tensor([0.2257], requires_grad=True)\n",
      "epoch: 19200 y: tensor([[0.2262],\n",
      "        [0.2262],\n",
      "        [0.2262],\n",
      "        [0.2262]], grad_fn=<AddBackward0>) 9.193841934204102 w: tensor(2., requires_grad=True) b:  tensor([0.2262], requires_grad=True)\n",
      "epoch: 19300 y: tensor([[0.2268],\n",
      "        [0.2268],\n",
      "        [0.2268],\n",
      "        [0.2268]], grad_fn=<AddBackward0>) 9.190767288208008 w: tensor(2., requires_grad=True) b:  tensor([0.2268], requires_grad=True)\n",
      "epoch: 19400 y: tensor([[0.2273],\n",
      "        [0.2273],\n",
      "        [0.2273],\n",
      "        [0.2273]], grad_fn=<AddBackward0>) 9.187692642211914 w: tensor(2., requires_grad=True) b:  tensor([0.2273], requires_grad=True)\n",
      "epoch: 19500 y: tensor([[0.2279],\n",
      "        [0.2279],\n",
      "        [0.2279],\n",
      "        [0.2279]], grad_fn=<AddBackward0>) 9.184619903564453 w: tensor(2., requires_grad=True) b:  tensor([0.2279], requires_grad=True)\n",
      "epoch: 19600 y: tensor([[0.2284],\n",
      "        [0.2284],\n",
      "        [0.2284],\n",
      "        [0.2284]], grad_fn=<AddBackward0>) 9.181546211242676 w: tensor(2., requires_grad=True) b:  tensor([0.2284], requires_grad=True)\n",
      "epoch: 19700 y: tensor([[0.2290],\n",
      "        [0.2290],\n",
      "        [0.2290],\n",
      "        [0.2290]], grad_fn=<AddBackward0>) 9.178474426269531 w: tensor(2., requires_grad=True) b:  tensor([0.2290], requires_grad=True)\n",
      "epoch: 19800 y: tensor([[0.2295],\n",
      "        [0.2295],\n",
      "        [0.2295],\n",
      "        [0.2295]], grad_fn=<AddBackward0>) 9.17540168762207 w: tensor(2., requires_grad=True) b:  tensor([0.2296], requires_grad=True)\n",
      "epoch: 19900 y: tensor([[0.2301],\n",
      "        [0.2301],\n",
      "        [0.2301],\n",
      "        [0.2301]], grad_fn=<AddBackward0>) 9.172330856323242 w: tensor(2., requires_grad=True) b:  tensor([0.2301], requires_grad=True)\n",
      "epoch: 20000 y: tensor([[0.2307],\n",
      "        [0.2307],\n",
      "        [0.2307],\n",
      "        [0.2307]], grad_fn=<AddBackward0>) 9.169260025024414 w: tensor(2., requires_grad=True) b:  tensor([0.2307], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD([w, b], lr=1e-6)\n",
    "epochs = 20001\n",
    "for epoch in range(epochs):\n",
    "    y=x_data.matmul(W) +b\n",
    "    cost = torch.mean((t_data-y)**2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch:' , epoch, 'y:', y, cost.item(), 'w:', w, 'b: ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    h = x.matmul(W) +b\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2307]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(torch.FloatTensor([[10, 11]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2307]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h = torch.FloatTensor([[11, 12]]).matmul(W) + b\n",
    "print(h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
