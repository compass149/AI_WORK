{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim #손실함수 최적화 시 사용함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.]])\n",
      "tensor([[ 2.],\n",
      "        [ 4.],\n",
      "        [ 6.],\n",
      "        [ 8.],\n",
      "        [10.]])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "x_train = torch.FloatTensor([[1], [2], [3], [4], [5]]) #2차원 TENSOR\n",
    "y_train = torch.FloatTensor([[2], [4], [6], [8], [10]]) #2차원 TENSOR\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "print(x_train.size())\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad=True)\n",
    "print(W)\n",
    "b = torch.zeros(1, requires_grad=True) #requires_grad=True : 학습을 통해 나중에 변경을 요구하는 함수수\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h = x_train*W+b\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((h-y_train)**2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#어떤 식으로 내려오는지를 확인함 \n",
    "#'SGD'는경사하강법의종류,  lr은학습률(learning rate)를의미\n",
    "# 학습대상인 W와 b가 SGD의 입력됨.\n",
    "optimizer = optim.SGD([W, b], lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20000 W:0.087 b:0.024 cost:41.944607\n",
      "Epoch  100/20000 W:1.701 b:0.452 cost:0.393517\n",
      "Epoch  200/20000 W:1.852 b:0.476 cost:0.044956\n",
      "Epoch  300/20000 W:1.870 b:0.464 cost:0.039331\n",
      "Epoch  400/20000 W:1.875 b:0.449 cost:0.036737\n",
      "Epoch  500/20000 W:1.880 b:0.434 cost:0.034334\n",
      "Epoch  600/20000 W:1.884 b:0.420 cost:0.032089\n",
      "Epoch  700/20000 W:1.888 b:0.406 cost:0.029990\n",
      "Epoch  800/20000 W:1.891 b:0.392 cost:0.028029\n",
      "Epoch  900/20000 W:1.895 b:0.379 cost:0.026196\n",
      "Epoch 1000/20000 W:1.898 b:0.367 cost:0.024483\n",
      "Epoch 1100/20000 W:1.902 b:0.354 cost:0.022882\n",
      "Epoch 1200/20000 W:1.905 b:0.343 cost:0.021386\n",
      "Epoch 1300/20000 W:1.908 b:0.331 cost:0.019987\n",
      "Epoch 1400/20000 W:1.911 b:0.320 cost:0.018680\n",
      "Epoch 1500/20000 W:1.914 b:0.310 cost:0.017459\n",
      "Epoch 1600/20000 W:1.917 b:0.299 cost:0.016317\n",
      "Epoch 1700/20000 W:1.920 b:0.289 cost:0.015250\n",
      "Epoch 1800/20000 W:1.923 b:0.280 cost:0.014253\n",
      "Epoch 1900/20000 W:1.925 b:0.270 cost:0.013321\n",
      "Epoch 2000/20000 W:1.928 b:0.261 cost:0.012450\n",
      "Epoch 2100/20000 W:1.930 b:0.253 cost:0.011635\n",
      "Epoch 2200/20000 W:1.932 b:0.244 cost:0.010875\n",
      "Epoch 2300/20000 W:1.935 b:0.236 cost:0.010163\n",
      "Epoch 2400/20000 W:1.937 b:0.228 cost:0.009499\n",
      "Epoch 2500/20000 W:1.939 b:0.221 cost:0.008878\n",
      "Epoch 2600/20000 W:1.941 b:0.213 cost:0.008297\n",
      "Epoch 2700/20000 W:1.943 b:0.206 cost:0.007755\n",
      "Epoch 2800/20000 W:1.945 b:0.199 cost:0.007247\n",
      "Epoch 2900/20000 W:1.947 b:0.193 cost:0.006773\n",
      "Epoch 3000/20000 W:1.948 b:0.186 cost:0.006331\n",
      "Epoch 3100/20000 W:1.950 b:0.180 cost:0.005917\n",
      "Epoch 3200/20000 W:1.952 b:0.174 cost:0.005530\n",
      "Epoch 3300/20000 W:1.953 b:0.168 cost:0.005168\n",
      "Epoch 3400/20000 W:1.955 b:0.163 cost:0.004830\n",
      "Epoch 3500/20000 W:1.956 b:0.157 cost:0.004514\n",
      "Epoch 3600/20000 W:1.958 b:0.152 cost:0.004219\n",
      "Epoch 3700/20000 W:1.959 b:0.147 cost:0.003943\n",
      "Epoch 3800/20000 W:1.961 b:0.142 cost:0.003685\n",
      "Epoch 3900/20000 W:1.962 b:0.138 cost:0.003444\n",
      "Epoch 4000/20000 W:1.963 b:0.133 cost:0.003219\n",
      "Epoch 4100/20000 W:1.964 b:0.129 cost:0.003009\n",
      "Epoch 4200/20000 W:1.966 b:0.124 cost:0.002812\n",
      "Epoch 4300/20000 W:1.967 b:0.120 cost:0.002628\n",
      "Epoch 4400/20000 W:1.968 b:0.116 cost:0.002456\n",
      "Epoch 4500/20000 W:1.969 b:0.112 cost:0.002295\n",
      "Epoch 4600/20000 W:1.970 b:0.109 cost:0.002145\n",
      "Epoch 4700/20000 W:1.971 b:0.105 cost:0.002005\n",
      "Epoch 4800/20000 W:1.972 b:0.101 cost:0.001874\n",
      "Epoch 4900/20000 W:1.973 b:0.098 cost:0.001751\n",
      "Epoch 5000/20000 W:1.974 b:0.095 cost:0.001637\n",
      "Epoch 5100/20000 W:1.975 b:0.092 cost:0.001530\n",
      "Epoch 5200/20000 W:1.975 b:0.089 cost:0.001430\n",
      "Epoch 5300/20000 W:1.976 b:0.086 cost:0.001336\n",
      "Epoch 5400/20000 W:1.977 b:0.083 cost:0.001249\n",
      "Epoch 5500/20000 W:1.978 b:0.080 cost:0.001167\n",
      "Epoch 5600/20000 W:1.979 b:0.077 cost:0.001091\n",
      "Epoch 5700/20000 W:1.979 b:0.075 cost:0.001020\n",
      "Epoch 5800/20000 W:1.980 b:0.072 cost:0.000953\n",
      "Epoch 5900/20000 W:1.981 b:0.070 cost:0.000891\n",
      "Epoch 6000/20000 W:1.981 b:0.068 cost:0.000832\n",
      "Epoch 6100/20000 W:1.982 b:0.065 cost:0.000778\n",
      "Epoch 6200/20000 W:1.982 b:0.063 cost:0.000727\n",
      "Epoch 6300/20000 W:1.983 b:0.061 cost:0.000680\n",
      "Epoch 6400/20000 W:1.984 b:0.059 cost:0.000635\n",
      "Epoch 6500/20000 W:1.984 b:0.057 cost:0.000594\n",
      "Epoch 6600/20000 W:1.985 b:0.055 cost:0.000555\n",
      "Epoch 6700/20000 W:1.985 b:0.053 cost:0.000519\n",
      "Epoch 6800/20000 W:1.986 b:0.052 cost:0.000485\n",
      "Epoch 6900/20000 W:1.986 b:0.050 cost:0.000453\n",
      "Epoch 7000/20000 W:1.987 b:0.048 cost:0.000423\n",
      "Epoch 7100/20000 W:1.987 b:0.047 cost:0.000396\n",
      "Epoch 7200/20000 W:1.988 b:0.045 cost:0.000370\n",
      "Epoch 7300/20000 W:1.988 b:0.044 cost:0.000346\n",
      "Epoch 7400/20000 W:1.988 b:0.042 cost:0.000323\n",
      "Epoch 7500/20000 W:1.989 b:0.041 cost:0.000302\n",
      "Epoch 7600/20000 W:1.989 b:0.039 cost:0.000282\n",
      "Epoch 7700/20000 W:1.989 b:0.038 cost:0.000264\n",
      "Epoch 7800/20000 W:1.990 b:0.037 cost:0.000246\n",
      "Epoch 7900/20000 W:1.990 b:0.036 cost:0.000230\n",
      "Epoch 8000/20000 W:1.990 b:0.034 cost:0.000215\n",
      "Epoch 8100/20000 W:1.991 b:0.033 cost:0.000201\n",
      "Epoch 8200/20000 W:1.991 b:0.032 cost:0.000188\n",
      "Epoch 8300/20000 W:1.991 b:0.031 cost:0.000176\n",
      "Epoch 8400/20000 W:1.992 b:0.030 cost:0.000164\n",
      "Epoch 8500/20000 W:1.992 b:0.029 cost:0.000154\n",
      "Epoch 8600/20000 W:1.992 b:0.028 cost:0.000144\n",
      "Epoch 8700/20000 W:1.992 b:0.027 cost:0.000134\n",
      "Epoch 8800/20000 W:1.993 b:0.026 cost:0.000125\n",
      "Epoch 8900/20000 W:1.993 b:0.025 cost:0.000117\n",
      "Epoch 9000/20000 W:1.993 b:0.025 cost:0.000110\n",
      "Epoch 9100/20000 W:1.993 b:0.024 cost:0.000102\n",
      "Epoch 9200/20000 W:1.994 b:0.023 cost:0.000096\n",
      "Epoch 9300/20000 W:1.994 b:0.022 cost:0.000089\n",
      "Epoch 9400/20000 W:1.994 b:0.021 cost:0.000084\n",
      "Epoch 9500/20000 W:1.994 b:0.021 cost:0.000078\n",
      "Epoch 9600/20000 W:1.994 b:0.020 cost:0.000073\n",
      "Epoch 9700/20000 W:1.995 b:0.019 cost:0.000068\n",
      "Epoch 9800/20000 W:1.995 b:0.019 cost:0.000064\n",
      "Epoch 9900/20000 W:1.995 b:0.018 cost:0.000060\n",
      "Epoch 10000/20000 W:1.995 b:0.017 cost:0.000056\n",
      "Epoch 10100/20000 W:1.995 b:0.017 cost:0.000052\n",
      "Epoch 10200/20000 W:1.995 b:0.016 cost:0.000049\n",
      "Epoch 10300/20000 W:1.996 b:0.016 cost:0.000046\n",
      "Epoch 10400/20000 W:1.996 b:0.015 cost:0.000043\n",
      "Epoch 10500/20000 W:1.996 b:0.015 cost:0.000040\n",
      "Epoch 10600/20000 W:1.996 b:0.014 cost:0.000037\n",
      "Epoch 10700/20000 W:1.996 b:0.014 cost:0.000035\n",
      "Epoch 10800/20000 W:1.996 b:0.013 cost:0.000032\n",
      "Epoch 10900/20000 W:1.996 b:0.013 cost:0.000030\n",
      "Epoch 11000/20000 W:1.997 b:0.012 cost:0.000028\n",
      "Epoch 11100/20000 W:1.997 b:0.012 cost:0.000027\n",
      "Epoch 11200/20000 W:1.997 b:0.012 cost:0.000025\n",
      "Epoch 11300/20000 W:1.997 b:0.011 cost:0.000023\n",
      "Epoch 11400/20000 W:1.997 b:0.011 cost:0.000022\n",
      "Epoch 11500/20000 W:1.997 b:0.011 cost:0.000020\n",
      "Epoch 11600/20000 W:1.997 b:0.010 cost:0.000019\n",
      "Epoch 11700/20000 W:1.997 b:0.010 cost:0.000018\n",
      "Epoch 11800/20000 W:1.997 b:0.010 cost:0.000017\n",
      "Epoch 11900/20000 W:1.997 b:0.009 cost:0.000015\n",
      "Epoch 12000/20000 W:1.998 b:0.009 cost:0.000014\n",
      "Epoch 12100/20000 W:1.998 b:0.009 cost:0.000013\n",
      "Epoch 12200/20000 W:1.998 b:0.008 cost:0.000013\n",
      "Epoch 12300/20000 W:1.998 b:0.008 cost:0.000012\n",
      "Epoch 12400/20000 W:1.998 b:0.008 cost:0.000011\n",
      "Epoch 12500/20000 W:1.998 b:0.008 cost:0.000010\n",
      "Epoch 12600/20000 W:1.998 b:0.007 cost:0.000010\n",
      "Epoch 12700/20000 W:1.998 b:0.007 cost:0.000009\n",
      "Epoch 12800/20000 W:1.998 b:0.007 cost:0.000008\n",
      "Epoch 12900/20000 W:1.998 b:0.007 cost:0.000008\n",
      "Epoch 13000/20000 W:1.998 b:0.006 cost:0.000007\n",
      "Epoch 13100/20000 W:1.998 b:0.006 cost:0.000007\n",
      "Epoch 13200/20000 W:1.998 b:0.006 cost:0.000006\n",
      "Epoch 13300/20000 W:1.998 b:0.006 cost:0.000006\n",
      "Epoch 13400/20000 W:1.998 b:0.006 cost:0.000006\n",
      "Epoch 13500/20000 W:1.999 b:0.005 cost:0.000005\n",
      "Epoch 13600/20000 W:1.999 b:0.005 cost:0.000005\n",
      "Epoch 13700/20000 W:1.999 b:0.005 cost:0.000005\n",
      "Epoch 13800/20000 W:1.999 b:0.005 cost:0.000004\n",
      "Epoch 13900/20000 W:1.999 b:0.005 cost:0.000004\n",
      "Epoch 14000/20000 W:1.999 b:0.005 cost:0.000004\n",
      "Epoch 14100/20000 W:1.999 b:0.004 cost:0.000003\n",
      "Epoch 14200/20000 W:1.999 b:0.004 cost:0.000003\n",
      "Epoch 14300/20000 W:1.999 b:0.004 cost:0.000003\n",
      "Epoch 14400/20000 W:1.999 b:0.004 cost:0.000003\n",
      "Epoch 14500/20000 W:1.999 b:0.004 cost:0.000003\n",
      "Epoch 14600/20000 W:1.999 b:0.004 cost:0.000003\n",
      "Epoch 14700/20000 W:1.999 b:0.004 cost:0.000002\n",
      "Epoch 14800/20000 W:1.999 b:0.003 cost:0.000002\n",
      "Epoch 14900/20000 W:1.999 b:0.003 cost:0.000002\n",
      "Epoch 15000/20000 W:1.999 b:0.003 cost:0.000002\n",
      "Epoch 15100/20000 W:1.999 b:0.003 cost:0.000002\n",
      "Epoch 15200/20000 W:1.999 b:0.003 cost:0.000002\n",
      "Epoch 15300/20000 W:1.999 b:0.003 cost:0.000002\n",
      "Epoch 15400/20000 W:1.999 b:0.003 cost:0.000001\n",
      "Epoch 15500/20000 W:1.999 b:0.003 cost:0.000001\n",
      "Epoch 15600/20000 W:1.999 b:0.003 cost:0.000001\n",
      "Epoch 15700/20000 W:1.999 b:0.003 cost:0.000001\n",
      "Epoch 15800/20000 W:1.999 b:0.002 cost:0.000001\n",
      "Epoch 15900/20000 W:1.999 b:0.002 cost:0.000001\n",
      "Epoch 16000/20000 W:1.999 b:0.002 cost:0.000001\n",
      "Epoch 16100/20000 W:1.999 b:0.002 cost:0.000001\n",
      "Epoch 16200/20000 W:1.999 b:0.002 cost:0.000001\n",
      "Epoch 16300/20000 W:1.999 b:0.002 cost:0.000001\n",
      "Epoch 16400/20000 W:1.999 b:0.002 cost:0.000001\n",
      "Epoch 16500/20000 W:1.999 b:0.002 cost:0.000001\n",
      "Epoch 16600/20000 W:1.999 b:0.002 cost:0.000001\n",
      "Epoch 16700/20000 W:1.999 b:0.002 cost:0.000001\n",
      "Epoch 16800/20000 W:2.000 b:0.002 cost:0.000001\n",
      "Epoch 16900/20000 W:2.000 b:0.002 cost:0.000001\n",
      "Epoch 17000/20000 W:2.000 b:0.002 cost:0.000000\n",
      "Epoch 17100/20000 W:2.000 b:0.002 cost:0.000000\n",
      "Epoch 17200/20000 W:2.000 b:0.002 cost:0.000000\n",
      "Epoch 17300/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 17400/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 17500/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 17600/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 17700/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 17800/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 17900/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 18000/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 18100/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 18200/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 18300/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 18400/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 18500/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 18600/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 18700/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 18800/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 18900/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 19000/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 19100/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 19200/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 19300/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 19400/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 19500/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 19600/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 19700/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 19800/20000 W:2.000 b:0.001 cost:0.000000\n",
      "Epoch 19900/20000 W:2.000 b:0.001 cost:0.000000\n"
     ]
    }
   ],
   "source": [
    "epochs = 20000\n",
    "for epoch in range(epochs):\n",
    "    h=x_train*W+b\n",
    "    \n",
    "    cost = torch.mean((h-y_train)**2) #cost가 최적화 되도록 개선해야 함\n",
    "    optimizer.zero_grad() #초기화 시킴\n",
    "    cost.backward() # 비용함수를 미분하여 gradient 계산\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0: #100번마다 한 번씩 출력\n",
    "        print('Epoch {:4d}/{} W:{:.3f} b:{:.3f} cost:{:.6f}'.format(\n",
    "            epoch, epochs, W.item(), b.item(), cost.item()\n",
    "        )) #{}안에 값이 들어감\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13.9994],\n",
      "        [15.9993],\n",
      "        [17.9991]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h = torch.FloatTensor([[7], [8], [9]])*W+b\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n",
      "수식을 w로 미분한 값:None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "print(w)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    z =  2*W\n",
    "    z.backward()\n",
    "    print('수식을 w로 미분한 값:{}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 3일 때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 3일 때')\n",
    "for i in range(1, 3):\n",
    "    print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 5일 때\n",
      "tensor([0.8303])\n",
      "tensor([0.1261])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "print('랜덤 시드가 5일 때')\n",
    "for i in range(1, 3):\n",
    "    print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 5일 때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 5일 때')\n",
    "for i in range(1, 3):\n",
    "    print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [2 3]\n",
      " [3 4]\n",
      " [4 5]]\n",
      "[[ 3]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_data1 = np.array([[1,2],[2,3],[3,4],[4,5]])\n",
    "t_data1 = np.array([3, 5, 7,9,11]).reshape(5,1)\n",
    "print(x_data1)\n",
    "print(t_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [2., 3.],\n",
      "        [3., 4.],\n",
      "        [4., 5.]])\n",
      "tensor([[1., 2.],\n",
      "        [2., 3.],\n",
      "        [3., 4.],\n",
      "        [4., 5.]])\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.from_numpy(x_data1).float()\n",
    "t_data = torch.from_numpy(x_data1).float()\n",
    "print(x_data)\n",
    "print(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((2,1), requires_grad=True)\n",
    "b=torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 y: tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>) 10.5 w: tensor(2., requires_grad=True) b:  tensor([6.0000e-06], requires_grad=True)\n",
      "epoch: 100 y: tensor([[0.0006],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0006]], grad_fn=<AddBackward0>) 10.496400833129883 w: tensor(2., requires_grad=True) b:  tensor([0.0006], requires_grad=True)\n",
      "epoch: 200 y: tensor([[0.0012],\n",
      "        [0.0012],\n",
      "        [0.0012],\n",
      "        [0.0012]], grad_fn=<AddBackward0>) 10.492803573608398 w: tensor(2., requires_grad=True) b:  tensor([0.0012], requires_grad=True)\n",
      "epoch: 300 y: tensor([[0.0018],\n",
      "        [0.0018],\n",
      "        [0.0018],\n",
      "        [0.0018]], grad_fn=<AddBackward0>) 10.489206314086914 w: tensor(2., requires_grad=True) b:  tensor([0.0018], requires_grad=True)\n",
      "epoch: 400 y: tensor([[0.0024],\n",
      "        [0.0024],\n",
      "        [0.0024],\n",
      "        [0.0024]], grad_fn=<AddBackward0>) 10.485610961914062 w: tensor(2., requires_grad=True) b:  tensor([0.0024], requires_grad=True)\n",
      "epoch: 500 y: tensor([[0.0030],\n",
      "        [0.0030],\n",
      "        [0.0030],\n",
      "        [0.0030]], grad_fn=<AddBackward0>) 10.48201847076416 w: tensor(2., requires_grad=True) b:  tensor([0.0030], requires_grad=True)\n",
      "epoch: 600 y: tensor([[0.0036],\n",
      "        [0.0036],\n",
      "        [0.0036],\n",
      "        [0.0036]], grad_fn=<AddBackward0>) 10.478425979614258 w: tensor(2., requires_grad=True) b:  tensor([0.0036], requires_grad=True)\n",
      "epoch: 700 y: tensor([[0.0042],\n",
      "        [0.0042],\n",
      "        [0.0042],\n",
      "        [0.0042]], grad_fn=<AddBackward0>) 10.474834442138672 w: tensor(2., requires_grad=True) b:  tensor([0.0042], requires_grad=True)\n",
      "epoch: 800 y: tensor([[0.0048],\n",
      "        [0.0048],\n",
      "        [0.0048],\n",
      "        [0.0048]], grad_fn=<AddBackward0>) 10.471246719360352 w: tensor(2., requires_grad=True) b:  tensor([0.0048], requires_grad=True)\n",
      "epoch: 900 y: tensor([[0.0054],\n",
      "        [0.0054],\n",
      "        [0.0054],\n",
      "        [0.0054]], grad_fn=<AddBackward0>) 10.467658042907715 w: tensor(2., requires_grad=True) b:  tensor([0.0054], requires_grad=True)\n",
      "epoch: 1000 y: tensor([[0.0060],\n",
      "        [0.0060],\n",
      "        [0.0060],\n",
      "        [0.0060]], grad_fn=<AddBackward0>) 10.464072227478027 w: tensor(2., requires_grad=True) b:  tensor([0.0060], requires_grad=True)\n",
      "epoch: 1100 y: tensor([[0.0066],\n",
      "        [0.0066],\n",
      "        [0.0066],\n",
      "        [0.0066]], grad_fn=<AddBackward0>) 10.460487365722656 w: tensor(2., requires_grad=True) b:  tensor([0.0066], requires_grad=True)\n",
      "epoch: 1200 y: tensor([[0.0072],\n",
      "        [0.0072],\n",
      "        [0.0072],\n",
      "        [0.0072]], grad_fn=<AddBackward0>) 10.456903457641602 w: tensor(2., requires_grad=True) b:  tensor([0.0072], requires_grad=True)\n",
      "epoch: 1300 y: tensor([[0.0078],\n",
      "        [0.0078],\n",
      "        [0.0078],\n",
      "        [0.0078]], grad_fn=<AddBackward0>) 10.45332145690918 w: tensor(2., requires_grad=True) b:  tensor([0.0078], requires_grad=True)\n",
      "epoch: 1400 y: tensor([[0.0084],\n",
      "        [0.0084],\n",
      "        [0.0084],\n",
      "        [0.0084]], grad_fn=<AddBackward0>) 10.44974136352539 w: tensor(2., requires_grad=True) b:  tensor([0.0084], requires_grad=True)\n",
      "epoch: 1500 y: tensor([[0.0090],\n",
      "        [0.0090],\n",
      "        [0.0090],\n",
      "        [0.0090]], grad_fn=<AddBackward0>) 10.446162223815918 w: tensor(2., requires_grad=True) b:  tensor([0.0090], requires_grad=True)\n",
      "epoch: 1600 y: tensor([[0.0096],\n",
      "        [0.0096],\n",
      "        [0.0096],\n",
      "        [0.0096]], grad_fn=<AddBackward0>) 10.442584991455078 w: tensor(2., requires_grad=True) b:  tensor([0.0096], requires_grad=True)\n",
      "epoch: 1700 y: tensor([[0.0102],\n",
      "        [0.0102],\n",
      "        [0.0102],\n",
      "        [0.0102]], grad_fn=<AddBackward0>) 10.439007759094238 w: tensor(2., requires_grad=True) b:  tensor([0.0102], requires_grad=True)\n",
      "epoch: 1800 y: tensor([[0.0108],\n",
      "        [0.0108],\n",
      "        [0.0108],\n",
      "        [0.0108]], grad_fn=<AddBackward0>) 10.435432434082031 w: tensor(2., requires_grad=True) b:  tensor([0.0108], requires_grad=True)\n",
      "epoch: 1900 y: tensor([[0.0114],\n",
      "        [0.0114],\n",
      "        [0.0114],\n",
      "        [0.0114]], grad_fn=<AddBackward0>) 10.431859970092773 w: tensor(2., requires_grad=True) b:  tensor([0.0114], requires_grad=True)\n",
      "epoch: 2000 y: tensor([[0.0120],\n",
      "        [0.0120],\n",
      "        [0.0120],\n",
      "        [0.0120]], grad_fn=<AddBackward0>) 10.4282865524292 w: tensor(2., requires_grad=True) b:  tensor([0.0120], requires_grad=True)\n",
      "epoch: 2100 y: tensor([[0.0126],\n",
      "        [0.0126],\n",
      "        [0.0126],\n",
      "        [0.0126]], grad_fn=<AddBackward0>) 10.42471694946289 w: tensor(2., requires_grad=True) b:  tensor([0.0126], requires_grad=True)\n",
      "epoch: 2200 y: tensor([[0.0132],\n",
      "        [0.0132],\n",
      "        [0.0132],\n",
      "        [0.0132]], grad_fn=<AddBackward0>) 10.421147346496582 w: tensor(2., requires_grad=True) b:  tensor([0.0132], requires_grad=True)\n",
      "epoch: 2300 y: tensor([[0.0138],\n",
      "        [0.0138],\n",
      "        [0.0138],\n",
      "        [0.0138]], grad_fn=<AddBackward0>) 10.417579650878906 w: tensor(2., requires_grad=True) b:  tensor([0.0138], requires_grad=True)\n",
      "epoch: 2400 y: tensor([[0.0144],\n",
      "        [0.0144],\n",
      "        [0.0144],\n",
      "        [0.0144]], grad_fn=<AddBackward0>) 10.414013862609863 w: tensor(2., requires_grad=True) b:  tensor([0.0144], requires_grad=True)\n",
      "epoch: 2500 y: tensor([[0.0150],\n",
      "        [0.0150],\n",
      "        [0.0150],\n",
      "        [0.0150]], grad_fn=<AddBackward0>) 10.41044807434082 w: tensor(2., requires_grad=True) b:  tensor([0.0150], requires_grad=True)\n",
      "epoch: 2600 y: tensor([[0.0156],\n",
      "        [0.0156],\n",
      "        [0.0156],\n",
      "        [0.0156]], grad_fn=<AddBackward0>) 10.406885147094727 w: tensor(2., requires_grad=True) b:  tensor([0.0156], requires_grad=True)\n",
      "epoch: 2700 y: tensor([[0.0162],\n",
      "        [0.0162],\n",
      "        [0.0162],\n",
      "        [0.0162]], grad_fn=<AddBackward0>) 10.40332317352295 w: tensor(2., requires_grad=True) b:  tensor([0.0162], requires_grad=True)\n",
      "epoch: 2800 y: tensor([[0.0168],\n",
      "        [0.0168],\n",
      "        [0.0168],\n",
      "        [0.0168]], grad_fn=<AddBackward0>) 10.399762153625488 w: tensor(2., requires_grad=True) b:  tensor([0.0168], requires_grad=True)\n",
      "epoch: 2900 y: tensor([[0.0173],\n",
      "        [0.0173],\n",
      "        [0.0173],\n",
      "        [0.0173]], grad_fn=<AddBackward0>) 10.39620304107666 w: tensor(2., requires_grad=True) b:  tensor([0.0174], requires_grad=True)\n",
      "epoch: 3000 y: tensor([[0.0179],\n",
      "        [0.0179],\n",
      "        [0.0179],\n",
      "        [0.0179]], grad_fn=<AddBackward0>) 10.392645835876465 w: tensor(2., requires_grad=True) b:  tensor([0.0180], requires_grad=True)\n",
      "epoch: 3100 y: tensor([[0.0185],\n",
      "        [0.0185],\n",
      "        [0.0185],\n",
      "        [0.0185]], grad_fn=<AddBackward0>) 10.389089584350586 w: tensor(2., requires_grad=True) b:  tensor([0.0185], requires_grad=True)\n",
      "epoch: 3200 y: tensor([[0.0191],\n",
      "        [0.0191],\n",
      "        [0.0191],\n",
      "        [0.0191]], grad_fn=<AddBackward0>) 10.385534286499023 w: tensor(2., requires_grad=True) b:  tensor([0.0191], requires_grad=True)\n",
      "epoch: 3300 y: tensor([[0.0197],\n",
      "        [0.0197],\n",
      "        [0.0197],\n",
      "        [0.0197]], grad_fn=<AddBackward0>) 10.381980895996094 w: tensor(2., requires_grad=True) b:  tensor([0.0197], requires_grad=True)\n",
      "epoch: 3400 y: tensor([[0.0203],\n",
      "        [0.0203],\n",
      "        [0.0203],\n",
      "        [0.0203]], grad_fn=<AddBackward0>) 10.378427505493164 w: tensor(2., requires_grad=True) b:  tensor([0.0203], requires_grad=True)\n",
      "epoch: 3500 y: tensor([[0.0209],\n",
      "        [0.0209],\n",
      "        [0.0209],\n",
      "        [0.0209]], grad_fn=<AddBackward0>) 10.3748779296875 w: tensor(2., requires_grad=True) b:  tensor([0.0209], requires_grad=True)\n",
      "epoch: 3600 y: tensor([[0.0215],\n",
      "        [0.0215],\n",
      "        [0.0215],\n",
      "        [0.0215]], grad_fn=<AddBackward0>) 10.371328353881836 w: tensor(2., requires_grad=True) b:  tensor([0.0215], requires_grad=True)\n",
      "epoch: 3700 y: tensor([[0.0221],\n",
      "        [0.0221],\n",
      "        [0.0221],\n",
      "        [0.0221]], grad_fn=<AddBackward0>) 10.367780685424805 w: tensor(2., requires_grad=True) b:  tensor([0.0221], requires_grad=True)\n",
      "epoch: 3800 y: tensor([[0.0227],\n",
      "        [0.0227],\n",
      "        [0.0227],\n",
      "        [0.0227]], grad_fn=<AddBackward0>) 10.36423397064209 w: tensor(2., requires_grad=True) b:  tensor([0.0227], requires_grad=True)\n",
      "epoch: 3900 y: tensor([[0.0233],\n",
      "        [0.0233],\n",
      "        [0.0233],\n",
      "        [0.0233]], grad_fn=<AddBackward0>) 10.360690116882324 w: tensor(2., requires_grad=True) b:  tensor([0.0233], requires_grad=True)\n",
      "epoch: 4000 y: tensor([[0.0239],\n",
      "        [0.0239],\n",
      "        [0.0239],\n",
      "        [0.0239]], grad_fn=<AddBackward0>) 10.357145309448242 w: tensor(2., requires_grad=True) b:  tensor([0.0239], requires_grad=True)\n",
      "epoch: 4100 y: tensor([[0.0245],\n",
      "        [0.0245],\n",
      "        [0.0245],\n",
      "        [0.0245]], grad_fn=<AddBackward0>) 10.35360336303711 w: tensor(2., requires_grad=True) b:  tensor([0.0245], requires_grad=True)\n",
      "epoch: 4200 y: tensor([[0.0251],\n",
      "        [0.0251],\n",
      "        [0.0251],\n",
      "        [0.0251]], grad_fn=<AddBackward0>) 10.350062370300293 w: tensor(2., requires_grad=True) b:  tensor([0.0251], requires_grad=True)\n",
      "epoch: 4300 y: tensor([[0.0257],\n",
      "        [0.0257],\n",
      "        [0.0257],\n",
      "        [0.0257]], grad_fn=<AddBackward0>) 10.34652328491211 w: tensor(2., requires_grad=True) b:  tensor([0.0257], requires_grad=True)\n",
      "epoch: 4400 y: tensor([[0.0263],\n",
      "        [0.0263],\n",
      "        [0.0263],\n",
      "        [0.0263]], grad_fn=<AddBackward0>) 10.342985153198242 w: tensor(2., requires_grad=True) b:  tensor([0.0263], requires_grad=True)\n",
      "epoch: 4500 y: tensor([[0.0269],\n",
      "        [0.0269],\n",
      "        [0.0269],\n",
      "        [0.0269]], grad_fn=<AddBackward0>) 10.339449882507324 w: tensor(2., requires_grad=True) b:  tensor([0.0269], requires_grad=True)\n",
      "epoch: 4600 y: tensor([[0.0275],\n",
      "        [0.0275],\n",
      "        [0.0275],\n",
      "        [0.0275]], grad_fn=<AddBackward0>) 10.335914611816406 w: tensor(2., requires_grad=True) b:  tensor([0.0275], requires_grad=True)\n",
      "epoch: 4700 y: tensor([[0.0281],\n",
      "        [0.0281],\n",
      "        [0.0281],\n",
      "        [0.0281]], grad_fn=<AddBackward0>) 10.332380294799805 w: tensor(2., requires_grad=True) b:  tensor([0.0281], requires_grad=True)\n",
      "epoch: 4800 y: tensor([[0.0287],\n",
      "        [0.0287],\n",
      "        [0.0287],\n",
      "        [0.0287]], grad_fn=<AddBackward0>) 10.328847885131836 w: tensor(2., requires_grad=True) b:  tensor([0.0287], requires_grad=True)\n",
      "epoch: 4900 y: tensor([[0.0293],\n",
      "        [0.0293],\n",
      "        [0.0293],\n",
      "        [0.0293]], grad_fn=<AddBackward0>) 10.3253173828125 w: tensor(2., requires_grad=True) b:  tensor([0.0293], requires_grad=True)\n",
      "epoch: 5000 y: tensor([[0.0299],\n",
      "        [0.0299],\n",
      "        [0.0299],\n",
      "        [0.0299]], grad_fn=<AddBackward0>) 10.32178783416748 w: tensor(2., requires_grad=True) b:  tensor([0.0299], requires_grad=True)\n",
      "epoch: 5100 y: tensor([[0.0304],\n",
      "        [0.0304],\n",
      "        [0.0304],\n",
      "        [0.0304]], grad_fn=<AddBackward0>) 10.318260192871094 w: tensor(2., requires_grad=True) b:  tensor([0.0305], requires_grad=True)\n",
      "epoch: 5200 y: tensor([[0.0310],\n",
      "        [0.0310],\n",
      "        [0.0310],\n",
      "        [0.0310]], grad_fn=<AddBackward0>) 10.314733505249023 w: tensor(2., requires_grad=True) b:  tensor([0.0310], requires_grad=True)\n",
      "epoch: 5300 y: tensor([[0.0316],\n",
      "        [0.0316],\n",
      "        [0.0316],\n",
      "        [0.0316]], grad_fn=<AddBackward0>) 10.31120777130127 w: tensor(2., requires_grad=True) b:  tensor([0.0316], requires_grad=True)\n",
      "epoch: 5400 y: tensor([[0.0322],\n",
      "        [0.0322],\n",
      "        [0.0322],\n",
      "        [0.0322]], grad_fn=<AddBackward0>) 10.307684898376465 w: tensor(2., requires_grad=True) b:  tensor([0.0322], requires_grad=True)\n",
      "epoch: 5500 y: tensor([[0.0328],\n",
      "        [0.0328],\n",
      "        [0.0328],\n",
      "        [0.0328]], grad_fn=<AddBackward0>) 10.304161071777344 w: tensor(2., requires_grad=True) b:  tensor([0.0328], requires_grad=True)\n",
      "epoch: 5600 y: tensor([[0.0334],\n",
      "        [0.0334],\n",
      "        [0.0334],\n",
      "        [0.0334]], grad_fn=<AddBackward0>) 10.300640106201172 w: tensor(2., requires_grad=True) b:  tensor([0.0334], requires_grad=True)\n",
      "epoch: 5700 y: tensor([[0.0340],\n",
      "        [0.0340],\n",
      "        [0.0340],\n",
      "        [0.0340]], grad_fn=<AddBackward0>) 10.29712200164795 w: tensor(2., requires_grad=True) b:  tensor([0.0340], requires_grad=True)\n",
      "epoch: 5800 y: tensor([[0.0346],\n",
      "        [0.0346],\n",
      "        [0.0346],\n",
      "        [0.0346]], grad_fn=<AddBackward0>) 10.293603897094727 w: tensor(2., requires_grad=True) b:  tensor([0.0346], requires_grad=True)\n",
      "epoch: 5900 y: tensor([[0.0352],\n",
      "        [0.0352],\n",
      "        [0.0352],\n",
      "        [0.0352]], grad_fn=<AddBackward0>) 10.29008674621582 w: tensor(2., requires_grad=True) b:  tensor([0.0352], requires_grad=True)\n",
      "epoch: 6000 y: tensor([[0.0358],\n",
      "        [0.0358],\n",
      "        [0.0358],\n",
      "        [0.0358]], grad_fn=<AddBackward0>) 10.286571502685547 w: tensor(2., requires_grad=True) b:  tensor([0.0358], requires_grad=True)\n",
      "epoch: 6100 y: tensor([[0.0364],\n",
      "        [0.0364],\n",
      "        [0.0364],\n",
      "        [0.0364]], grad_fn=<AddBackward0>) 10.283058166503906 w: tensor(2., requires_grad=True) b:  tensor([0.0364], requires_grad=True)\n",
      "epoch: 6200 y: tensor([[0.0370],\n",
      "        [0.0370],\n",
      "        [0.0370],\n",
      "        [0.0370]], grad_fn=<AddBackward0>) 10.279544830322266 w: tensor(2., requires_grad=True) b:  tensor([0.0370], requires_grad=True)\n",
      "epoch: 6300 y: tensor([[0.0376],\n",
      "        [0.0376],\n",
      "        [0.0376],\n",
      "        [0.0376]], grad_fn=<AddBackward0>) 10.276033401489258 w: tensor(2., requires_grad=True) b:  tensor([0.0376], requires_grad=True)\n",
      "epoch: 6400 y: tensor([[0.0382],\n",
      "        [0.0382],\n",
      "        [0.0382],\n",
      "        [0.0382]], grad_fn=<AddBackward0>) 10.272523880004883 w: tensor(2., requires_grad=True) b:  tensor([0.0382], requires_grad=True)\n",
      "epoch: 6500 y: tensor([[0.0387],\n",
      "        [0.0387],\n",
      "        [0.0387],\n",
      "        [0.0387]], grad_fn=<AddBackward0>) 10.269015312194824 w: tensor(2., requires_grad=True) b:  tensor([0.0388], requires_grad=True)\n",
      "epoch: 6600 y: tensor([[0.0393],\n",
      "        [0.0393],\n",
      "        [0.0393],\n",
      "        [0.0393]], grad_fn=<AddBackward0>) 10.265507698059082 w: tensor(2., requires_grad=True) b:  tensor([0.0393], requires_grad=True)\n",
      "epoch: 6700 y: tensor([[0.0399],\n",
      "        [0.0399],\n",
      "        [0.0399],\n",
      "        [0.0399]], grad_fn=<AddBackward0>) 10.262003898620605 w: tensor(2., requires_grad=True) b:  tensor([0.0399], requires_grad=True)\n",
      "epoch: 6800 y: tensor([[0.0405],\n",
      "        [0.0405],\n",
      "        [0.0405],\n",
      "        [0.0405]], grad_fn=<AddBackward0>) 10.258499145507812 w: tensor(2., requires_grad=True) b:  tensor([0.0405], requires_grad=True)\n",
      "epoch: 6900 y: tensor([[0.0411],\n",
      "        [0.0411],\n",
      "        [0.0411],\n",
      "        [0.0411]], grad_fn=<AddBackward0>) 10.254996299743652 w: tensor(2., requires_grad=True) b:  tensor([0.0411], requires_grad=True)\n",
      "epoch: 7000 y: tensor([[0.0417],\n",
      "        [0.0417],\n",
      "        [0.0417],\n",
      "        [0.0417]], grad_fn=<AddBackward0>) 10.251494407653809 w: tensor(2., requires_grad=True) b:  tensor([0.0417], requires_grad=True)\n",
      "epoch: 7100 y: tensor([[0.0423],\n",
      "        [0.0423],\n",
      "        [0.0423],\n",
      "        [0.0423]], grad_fn=<AddBackward0>) 10.247995376586914 w: tensor(2., requires_grad=True) b:  tensor([0.0423], requires_grad=True)\n",
      "epoch: 7200 y: tensor([[0.0429],\n",
      "        [0.0429],\n",
      "        [0.0429],\n",
      "        [0.0429]], grad_fn=<AddBackward0>) 10.244495391845703 w: tensor(2., requires_grad=True) b:  tensor([0.0429], requires_grad=True)\n",
      "epoch: 7300 y: tensor([[0.0435],\n",
      "        [0.0435],\n",
      "        [0.0435],\n",
      "        [0.0435]], grad_fn=<AddBackward0>) 10.241000175476074 w: tensor(2., requires_grad=True) b:  tensor([0.0435], requires_grad=True)\n",
      "epoch: 7400 y: tensor([[0.0441],\n",
      "        [0.0441],\n",
      "        [0.0441],\n",
      "        [0.0441]], grad_fn=<AddBackward0>) 10.237503051757812 w: tensor(2., requires_grad=True) b:  tensor([0.0441], requires_grad=True)\n",
      "epoch: 7500 y: tensor([[0.0447],\n",
      "        [0.0447],\n",
      "        [0.0447],\n",
      "        [0.0447]], grad_fn=<AddBackward0>) 10.2340087890625 w: tensor(2., requires_grad=True) b:  tensor([0.0447], requires_grad=True)\n",
      "epoch: 7600 y: tensor([[0.0453],\n",
      "        [0.0453],\n",
      "        [0.0453],\n",
      "        [0.0453]], grad_fn=<AddBackward0>) 10.23051643371582 w: tensor(2., requires_grad=True) b:  tensor([0.0453], requires_grad=True)\n",
      "epoch: 7700 y: tensor([[0.0458],\n",
      "        [0.0458],\n",
      "        [0.0458],\n",
      "        [0.0458]], grad_fn=<AddBackward0>) 10.227025985717773 w: tensor(2., requires_grad=True) b:  tensor([0.0459], requires_grad=True)\n",
      "epoch: 7800 y: tensor([[0.0464],\n",
      "        [0.0464],\n",
      "        [0.0464],\n",
      "        [0.0464]], grad_fn=<AddBackward0>) 10.223535537719727 w: tensor(2., requires_grad=True) b:  tensor([0.0464], requires_grad=True)\n",
      "epoch: 7900 y: tensor([[0.0470],\n",
      "        [0.0470],\n",
      "        [0.0470],\n",
      "        [0.0470]], grad_fn=<AddBackward0>) 10.220046997070312 w: tensor(2., requires_grad=True) b:  tensor([0.0470], requires_grad=True)\n",
      "epoch: 8000 y: tensor([[0.0476],\n",
      "        [0.0476],\n",
      "        [0.0476],\n",
      "        [0.0476]], grad_fn=<AddBackward0>) 10.216558456420898 w: tensor(2., requires_grad=True) b:  tensor([0.0476], requires_grad=True)\n",
      "epoch: 8100 y: tensor([[0.0482],\n",
      "        [0.0482],\n",
      "        [0.0482],\n",
      "        [0.0482]], grad_fn=<AddBackward0>) 10.21307373046875 w: tensor(2., requires_grad=True) b:  tensor([0.0482], requires_grad=True)\n",
      "epoch: 8200 y: tensor([[0.0488],\n",
      "        [0.0488],\n",
      "        [0.0488],\n",
      "        [0.0488]], grad_fn=<AddBackward0>) 10.209588050842285 w: tensor(2., requires_grad=True) b:  tensor([0.0488], requires_grad=True)\n",
      "epoch: 8300 y: tensor([[0.0494],\n",
      "        [0.0494],\n",
      "        [0.0494],\n",
      "        [0.0494]], grad_fn=<AddBackward0>) 10.206106185913086 w: tensor(2., requires_grad=True) b:  tensor([0.0494], requires_grad=True)\n",
      "epoch: 8400 y: tensor([[0.0500],\n",
      "        [0.0500],\n",
      "        [0.0500],\n",
      "        [0.0500]], grad_fn=<AddBackward0>) 10.20262336730957 w: tensor(2., requires_grad=True) b:  tensor([0.0500], requires_grad=True)\n",
      "epoch: 8500 y: tensor([[0.0506],\n",
      "        [0.0506],\n",
      "        [0.0506],\n",
      "        [0.0506]], grad_fn=<AddBackward0>) 10.199142456054688 w: tensor(2., requires_grad=True) b:  tensor([0.0506], requires_grad=True)\n",
      "epoch: 8600 y: tensor([[0.0512],\n",
      "        [0.0512],\n",
      "        [0.0512],\n",
      "        [0.0512]], grad_fn=<AddBackward0>) 10.195663452148438 w: tensor(2., requires_grad=True) b:  tensor([0.0512], requires_grad=True)\n",
      "epoch: 8700 y: tensor([[0.0517],\n",
      "        [0.0517],\n",
      "        [0.0517],\n",
      "        [0.0517]], grad_fn=<AddBackward0>) 10.19218635559082 w: tensor(2., requires_grad=True) b:  tensor([0.0518], requires_grad=True)\n",
      "epoch: 8800 y: tensor([[0.0523],\n",
      "        [0.0523],\n",
      "        [0.0523],\n",
      "        [0.0523]], grad_fn=<AddBackward0>) 10.18871021270752 w: tensor(2., requires_grad=True) b:  tensor([0.0523], requires_grad=True)\n",
      "epoch: 8900 y: tensor([[0.0529],\n",
      "        [0.0529],\n",
      "        [0.0529],\n",
      "        [0.0529]], grad_fn=<AddBackward0>) 10.185235977172852 w: tensor(2., requires_grad=True) b:  tensor([0.0529], requires_grad=True)\n",
      "epoch: 9000 y: tensor([[0.0535],\n",
      "        [0.0535],\n",
      "        [0.0535],\n",
      "        [0.0535]], grad_fn=<AddBackward0>) 10.1817626953125 w: tensor(2., requires_grad=True) b:  tensor([0.0535], requires_grad=True)\n",
      "epoch: 9100 y: tensor([[0.0541],\n",
      "        [0.0541],\n",
      "        [0.0541],\n",
      "        [0.0541]], grad_fn=<AddBackward0>) 10.178289413452148 w: tensor(2., requires_grad=True) b:  tensor([0.0541], requires_grad=True)\n",
      "epoch: 9200 y: tensor([[0.0547],\n",
      "        [0.0547],\n",
      "        [0.0547],\n",
      "        [0.0547]], grad_fn=<AddBackward0>) 10.174819946289062 w: tensor(2., requires_grad=True) b:  tensor([0.0547], requires_grad=True)\n",
      "epoch: 9300 y: tensor([[0.0553],\n",
      "        [0.0553],\n",
      "        [0.0553],\n",
      "        [0.0553]], grad_fn=<AddBackward0>) 10.171350479125977 w: tensor(2., requires_grad=True) b:  tensor([0.0553], requires_grad=True)\n",
      "epoch: 9400 y: tensor([[0.0559],\n",
      "        [0.0559],\n",
      "        [0.0559],\n",
      "        [0.0559]], grad_fn=<AddBackward0>) 10.167881965637207 w: tensor(2., requires_grad=True) b:  tensor([0.0559], requires_grad=True)\n",
      "epoch: 9500 y: tensor([[0.0565],\n",
      "        [0.0565],\n",
      "        [0.0565],\n",
      "        [0.0565]], grad_fn=<AddBackward0>) 10.164416313171387 w: tensor(2., requires_grad=True) b:  tensor([0.0565], requires_grad=True)\n",
      "epoch: 9600 y: tensor([[0.0571],\n",
      "        [0.0571],\n",
      "        [0.0571],\n",
      "        [0.0571]], grad_fn=<AddBackward0>) 10.160951614379883 w: tensor(2., requires_grad=True) b:  tensor([0.0571], requires_grad=True)\n",
      "epoch: 9700 y: tensor([[0.0576],\n",
      "        [0.0576],\n",
      "        [0.0576],\n",
      "        [0.0576]], grad_fn=<AddBackward0>) 10.157487869262695 w: tensor(2., requires_grad=True) b:  tensor([0.0576], requires_grad=True)\n",
      "epoch: 9800 y: tensor([[0.0582],\n",
      "        [0.0582],\n",
      "        [0.0582],\n",
      "        [0.0582]], grad_fn=<AddBackward0>) 10.15402603149414 w: tensor(2., requires_grad=True) b:  tensor([0.0582], requires_grad=True)\n",
      "epoch: 9900 y: tensor([[0.0588],\n",
      "        [0.0588],\n",
      "        [0.0588],\n",
      "        [0.0588]], grad_fn=<AddBackward0>) 10.150564193725586 w: tensor(2., requires_grad=True) b:  tensor([0.0588], requires_grad=True)\n",
      "epoch: 10000 y: tensor([[0.0594],\n",
      "        [0.0594],\n",
      "        [0.0594],\n",
      "        [0.0594]], grad_fn=<AddBackward0>) 10.14710521697998 w: tensor(2., requires_grad=True) b:  tensor([0.0594], requires_grad=True)\n",
      "epoch: 10100 y: tensor([[0.0600],\n",
      "        [0.0600],\n",
      "        [0.0600],\n",
      "        [0.0600]], grad_fn=<AddBackward0>) 10.143646240234375 w: tensor(2., requires_grad=True) b:  tensor([0.0600], requires_grad=True)\n",
      "epoch: 10200 y: tensor([[0.0606],\n",
      "        [0.0606],\n",
      "        [0.0606],\n",
      "        [0.0606]], grad_fn=<AddBackward0>) 10.140190124511719 w: tensor(2., requires_grad=True) b:  tensor([0.0606], requires_grad=True)\n",
      "epoch: 10300 y: tensor([[0.0612],\n",
      "        [0.0612],\n",
      "        [0.0612],\n",
      "        [0.0612]], grad_fn=<AddBackward0>) 10.136734008789062 w: tensor(2., requires_grad=True) b:  tensor([0.0612], requires_grad=True)\n",
      "epoch: 10400 y: tensor([[0.0618],\n",
      "        [0.0618],\n",
      "        [0.0618],\n",
      "        [0.0618]], grad_fn=<AddBackward0>) 10.133279800415039 w: tensor(2., requires_grad=True) b:  tensor([0.0618], requires_grad=True)\n",
      "epoch: 10500 y: tensor([[0.0623],\n",
      "        [0.0623],\n",
      "        [0.0623],\n",
      "        [0.0623]], grad_fn=<AddBackward0>) 10.129827499389648 w: tensor(2., requires_grad=True) b:  tensor([0.0623], requires_grad=True)\n",
      "epoch: 10600 y: tensor([[0.0629],\n",
      "        [0.0629],\n",
      "        [0.0629],\n",
      "        [0.0629]], grad_fn=<AddBackward0>) 10.12637710571289 w: tensor(2., requires_grad=True) b:  tensor([0.0629], requires_grad=True)\n",
      "epoch: 10700 y: tensor([[0.0635],\n",
      "        [0.0635],\n",
      "        [0.0635],\n",
      "        [0.0635]], grad_fn=<AddBackward0>) 10.122928619384766 w: tensor(2., requires_grad=True) b:  tensor([0.0635], requires_grad=True)\n",
      "epoch: 10800 y: tensor([[0.0641],\n",
      "        [0.0641],\n",
      "        [0.0641],\n",
      "        [0.0641]], grad_fn=<AddBackward0>) 10.119481086730957 w: tensor(2., requires_grad=True) b:  tensor([0.0641], requires_grad=True)\n",
      "epoch: 10900 y: tensor([[0.0647],\n",
      "        [0.0647],\n",
      "        [0.0647],\n",
      "        [0.0647]], grad_fn=<AddBackward0>) 10.116033554077148 w: tensor(2., requires_grad=True) b:  tensor([0.0647], requires_grad=True)\n",
      "epoch: 11000 y: tensor([[0.0653],\n",
      "        [0.0653],\n",
      "        [0.0653],\n",
      "        [0.0653]], grad_fn=<AddBackward0>) 10.112586975097656 w: tensor(2., requires_grad=True) b:  tensor([0.0653], requires_grad=True)\n",
      "epoch: 11100 y: tensor([[0.0659],\n",
      "        [0.0659],\n",
      "        [0.0659],\n",
      "        [0.0659]], grad_fn=<AddBackward0>) 10.109142303466797 w: tensor(2., requires_grad=True) b:  tensor([0.0659], requires_grad=True)\n",
      "epoch: 11200 y: tensor([[0.0665],\n",
      "        [0.0665],\n",
      "        [0.0665],\n",
      "        [0.0665]], grad_fn=<AddBackward0>) 10.105698585510254 w: tensor(2., requires_grad=True) b:  tensor([0.0665], requires_grad=True)\n",
      "epoch: 11300 y: tensor([[0.0670],\n",
      "        [0.0670],\n",
      "        [0.0670],\n",
      "        [0.0670]], grad_fn=<AddBackward0>) 10.102258682250977 w: tensor(2., requires_grad=True) b:  tensor([0.0670], requires_grad=True)\n",
      "epoch: 11400 y: tensor([[0.0676],\n",
      "        [0.0676],\n",
      "        [0.0676],\n",
      "        [0.0676]], grad_fn=<AddBackward0>) 10.0988187789917 w: tensor(2., requires_grad=True) b:  tensor([0.0676], requires_grad=True)\n",
      "epoch: 11500 y: tensor([[0.0682],\n",
      "        [0.0682],\n",
      "        [0.0682],\n",
      "        [0.0682]], grad_fn=<AddBackward0>) 10.095380783081055 w: tensor(2., requires_grad=True) b:  tensor([0.0682], requires_grad=True)\n",
      "epoch: 11600 y: tensor([[0.0688],\n",
      "        [0.0688],\n",
      "        [0.0688],\n",
      "        [0.0688]], grad_fn=<AddBackward0>) 10.091941833496094 w: tensor(2., requires_grad=True) b:  tensor([0.0688], requires_grad=True)\n",
      "epoch: 11700 y: tensor([[0.0694],\n",
      "        [0.0694],\n",
      "        [0.0694],\n",
      "        [0.0694]], grad_fn=<AddBackward0>) 10.088505744934082 w: tensor(2., requires_grad=True) b:  tensor([0.0694], requires_grad=True)\n",
      "epoch: 11800 y: tensor([[0.0700],\n",
      "        [0.0700],\n",
      "        [0.0700],\n",
      "        [0.0700]], grad_fn=<AddBackward0>) 10.08506965637207 w: tensor(2., requires_grad=True) b:  tensor([0.0700], requires_grad=True)\n",
      "epoch: 11900 y: tensor([[0.0706],\n",
      "        [0.0706],\n",
      "        [0.0706],\n",
      "        [0.0706]], grad_fn=<AddBackward0>) 10.081636428833008 w: tensor(2., requires_grad=True) b:  tensor([0.0706], requires_grad=True)\n",
      "epoch: 12000 y: tensor([[0.0711],\n",
      "        [0.0711],\n",
      "        [0.0711],\n",
      "        [0.0711]], grad_fn=<AddBackward0>) 10.078206062316895 w: tensor(2., requires_grad=True) b:  tensor([0.0711], requires_grad=True)\n",
      "epoch: 12100 y: tensor([[0.0717],\n",
      "        [0.0717],\n",
      "        [0.0717],\n",
      "        [0.0717]], grad_fn=<AddBackward0>) 10.074776649475098 w: tensor(2., requires_grad=True) b:  tensor([0.0717], requires_grad=True)\n",
      "epoch: 12200 y: tensor([[0.0723],\n",
      "        [0.0723],\n",
      "        [0.0723],\n",
      "        [0.0723]], grad_fn=<AddBackward0>) 10.071346282958984 w: tensor(2., requires_grad=True) b:  tensor([0.0723], requires_grad=True)\n",
      "epoch: 12300 y: tensor([[0.0729],\n",
      "        [0.0729],\n",
      "        [0.0729],\n",
      "        [0.0729]], grad_fn=<AddBackward0>) 10.067917823791504 w: tensor(2., requires_grad=True) b:  tensor([0.0729], requires_grad=True)\n",
      "epoch: 12400 y: tensor([[0.0735],\n",
      "        [0.0735],\n",
      "        [0.0735],\n",
      "        [0.0735]], grad_fn=<AddBackward0>) 10.06449031829834 w: tensor(2., requires_grad=True) b:  tensor([0.0735], requires_grad=True)\n",
      "epoch: 12500 y: tensor([[0.0741],\n",
      "        [0.0741],\n",
      "        [0.0741],\n",
      "        [0.0741]], grad_fn=<AddBackward0>) 10.061065673828125 w: tensor(2., requires_grad=True) b:  tensor([0.0741], requires_grad=True)\n",
      "epoch: 12600 y: tensor([[0.0747],\n",
      "        [0.0747],\n",
      "        [0.0747],\n",
      "        [0.0747]], grad_fn=<AddBackward0>) 10.057642936706543 w: tensor(2., requires_grad=True) b:  tensor([0.0747], requires_grad=True)\n",
      "epoch: 12700 y: tensor([[0.0752],\n",
      "        [0.0752],\n",
      "        [0.0752],\n",
      "        [0.0752]], grad_fn=<AddBackward0>) 10.054221153259277 w: tensor(2., requires_grad=True) b:  tensor([0.0752], requires_grad=True)\n",
      "epoch: 12800 y: tensor([[0.0758],\n",
      "        [0.0758],\n",
      "        [0.0758],\n",
      "        [0.0758]], grad_fn=<AddBackward0>) 10.050800323486328 w: tensor(2., requires_grad=True) b:  tensor([0.0758], requires_grad=True)\n",
      "epoch: 12900 y: tensor([[0.0764],\n",
      "        [0.0764],\n",
      "        [0.0764],\n",
      "        [0.0764]], grad_fn=<AddBackward0>) 10.047380447387695 w: tensor(2., requires_grad=True) b:  tensor([0.0764], requires_grad=True)\n",
      "epoch: 13000 y: tensor([[0.0770],\n",
      "        [0.0770],\n",
      "        [0.0770],\n",
      "        [0.0770]], grad_fn=<AddBackward0>) 10.043960571289062 w: tensor(2., requires_grad=True) b:  tensor([0.0770], requires_grad=True)\n",
      "epoch: 13100 y: tensor([[0.0776],\n",
      "        [0.0776],\n",
      "        [0.0776],\n",
      "        [0.0776]], grad_fn=<AddBackward0>) 10.040542602539062 w: tensor(2., requires_grad=True) b:  tensor([0.0776], requires_grad=True)\n",
      "epoch: 13200 y: tensor([[0.0782],\n",
      "        [0.0782],\n",
      "        [0.0782],\n",
      "        [0.0782]], grad_fn=<AddBackward0>) 10.037128448486328 w: tensor(2., requires_grad=True) b:  tensor([0.0782], requires_grad=True)\n",
      "epoch: 13300 y: tensor([[0.0787],\n",
      "        [0.0787],\n",
      "        [0.0787],\n",
      "        [0.0787]], grad_fn=<AddBackward0>) 10.033716201782227 w: tensor(2., requires_grad=True) b:  tensor([0.0788], requires_grad=True)\n",
      "epoch: 13400 y: tensor([[0.0793],\n",
      "        [0.0793],\n",
      "        [0.0793],\n",
      "        [0.0793]], grad_fn=<AddBackward0>) 10.030303001403809 w: tensor(2., requires_grad=True) b:  tensor([0.0793], requires_grad=True)\n",
      "epoch: 13500 y: tensor([[0.0799],\n",
      "        [0.0799],\n",
      "        [0.0799],\n",
      "        [0.0799]], grad_fn=<AddBackward0>) 10.026891708374023 w: tensor(2., requires_grad=True) b:  tensor([0.0799], requires_grad=True)\n",
      "epoch: 13600 y: tensor([[0.0805],\n",
      "        [0.0805],\n",
      "        [0.0805],\n",
      "        [0.0805]], grad_fn=<AddBackward0>) 10.023480415344238 w: tensor(2., requires_grad=True) b:  tensor([0.0805], requires_grad=True)\n",
      "epoch: 13700 y: tensor([[0.0811],\n",
      "        [0.0811],\n",
      "        [0.0811],\n",
      "        [0.0811]], grad_fn=<AddBackward0>) 10.02007007598877 w: tensor(2., requires_grad=True) b:  tensor([0.0811], requires_grad=True)\n",
      "epoch: 13800 y: tensor([[0.0817],\n",
      "        [0.0817],\n",
      "        [0.0817],\n",
      "        [0.0817]], grad_fn=<AddBackward0>) 10.016663551330566 w: tensor(2., requires_grad=True) b:  tensor([0.0817], requires_grad=True)\n",
      "epoch: 13900 y: tensor([[0.0823],\n",
      "        [0.0823],\n",
      "        [0.0823],\n",
      "        [0.0823]], grad_fn=<AddBackward0>) 10.01325798034668 w: tensor(2., requires_grad=True) b:  tensor([0.0823], requires_grad=True)\n",
      "epoch: 14000 y: tensor([[0.0828],\n",
      "        [0.0828],\n",
      "        [0.0828],\n",
      "        [0.0828]], grad_fn=<AddBackward0>) 10.009854316711426 w: tensor(2., requires_grad=True) b:  tensor([0.0828], requires_grad=True)\n",
      "epoch: 14100 y: tensor([[0.0834],\n",
      "        [0.0834],\n",
      "        [0.0834],\n",
      "        [0.0834]], grad_fn=<AddBackward0>) 10.006450653076172 w: tensor(2., requires_grad=True) b:  tensor([0.0834], requires_grad=True)\n",
      "epoch: 14200 y: tensor([[0.0840],\n",
      "        [0.0840],\n",
      "        [0.0840],\n",
      "        [0.0840]], grad_fn=<AddBackward0>) 10.003047943115234 w: tensor(2., requires_grad=True) b:  tensor([0.0840], requires_grad=True)\n",
      "epoch: 14300 y: tensor([[0.0846],\n",
      "        [0.0846],\n",
      "        [0.0846],\n",
      "        [0.0846]], grad_fn=<AddBackward0>) 9.99964714050293 w: tensor(2., requires_grad=True) b:  tensor([0.0846], requires_grad=True)\n",
      "epoch: 14400 y: tensor([[0.0852],\n",
      "        [0.0852],\n",
      "        [0.0852],\n",
      "        [0.0852]], grad_fn=<AddBackward0>) 9.996247291564941 w: tensor(2., requires_grad=True) b:  tensor([0.0852], requires_grad=True)\n",
      "epoch: 14500 y: tensor([[0.0858],\n",
      "        [0.0858],\n",
      "        [0.0858],\n",
      "        [0.0858]], grad_fn=<AddBackward0>) 9.992851257324219 w: tensor(2., requires_grad=True) b:  tensor([0.0858], requires_grad=True)\n",
      "epoch: 14600 y: tensor([[0.0863],\n",
      "        [0.0863],\n",
      "        [0.0863],\n",
      "        [0.0863]], grad_fn=<AddBackward0>) 9.989455223083496 w: tensor(2., requires_grad=True) b:  tensor([0.0863], requires_grad=True)\n",
      "epoch: 14700 y: tensor([[0.0869],\n",
      "        [0.0869],\n",
      "        [0.0869],\n",
      "        [0.0869]], grad_fn=<AddBackward0>) 9.986061096191406 w: tensor(2., requires_grad=True) b:  tensor([0.0869], requires_grad=True)\n",
      "epoch: 14800 y: tensor([[0.0875],\n",
      "        [0.0875],\n",
      "        [0.0875],\n",
      "        [0.0875]], grad_fn=<AddBackward0>) 9.982666969299316 w: tensor(2., requires_grad=True) b:  tensor([0.0875], requires_grad=True)\n",
      "epoch: 14900 y: tensor([[0.0881],\n",
      "        [0.0881],\n",
      "        [0.0881],\n",
      "        [0.0881]], grad_fn=<AddBackward0>) 9.979272842407227 w: tensor(2., requires_grad=True) b:  tensor([0.0881], requires_grad=True)\n",
      "epoch: 15000 y: tensor([[0.0887],\n",
      "        [0.0887],\n",
      "        [0.0887],\n",
      "        [0.0887]], grad_fn=<AddBackward0>) 9.975879669189453 w: tensor(2., requires_grad=True) b:  tensor([0.0887], requires_grad=True)\n",
      "epoch: 15100 y: tensor([[0.0892],\n",
      "        [0.0892],\n",
      "        [0.0892],\n",
      "        [0.0892]], grad_fn=<AddBackward0>) 9.972493171691895 w: tensor(2., requires_grad=True) b:  tensor([0.0893], requires_grad=True)\n",
      "epoch: 15200 y: tensor([[0.0898],\n",
      "        [0.0898],\n",
      "        [0.0898],\n",
      "        [0.0898]], grad_fn=<AddBackward0>) 9.969104766845703 w: tensor(2., requires_grad=True) b:  tensor([0.0898], requires_grad=True)\n",
      "epoch: 15300 y: tensor([[0.0904],\n",
      "        [0.0904],\n",
      "        [0.0904],\n",
      "        [0.0904]], grad_fn=<AddBackward0>) 9.965718269348145 w: tensor(2., requires_grad=True) b:  tensor([0.0904], requires_grad=True)\n",
      "epoch: 15400 y: tensor([[0.0910],\n",
      "        [0.0910],\n",
      "        [0.0910],\n",
      "        [0.0910]], grad_fn=<AddBackward0>) 9.962333679199219 w: tensor(2., requires_grad=True) b:  tensor([0.0910], requires_grad=True)\n",
      "epoch: 15500 y: tensor([[0.0916],\n",
      "        [0.0916],\n",
      "        [0.0916],\n",
      "        [0.0916]], grad_fn=<AddBackward0>) 9.958948135375977 w: tensor(2., requires_grad=True) b:  tensor([0.0916], requires_grad=True)\n",
      "epoch: 15600 y: tensor([[0.0922],\n",
      "        [0.0922],\n",
      "        [0.0922],\n",
      "        [0.0922]], grad_fn=<AddBackward0>) 9.955562591552734 w: tensor(2., requires_grad=True) b:  tensor([0.0922], requires_grad=True)\n",
      "epoch: 15700 y: tensor([[0.0927],\n",
      "        [0.0927],\n",
      "        [0.0927],\n",
      "        [0.0927]], grad_fn=<AddBackward0>) 9.95218276977539 w: tensor(2., requires_grad=True) b:  tensor([0.0927], requires_grad=True)\n",
      "epoch: 15800 y: tensor([[0.0933],\n",
      "        [0.0933],\n",
      "        [0.0933],\n",
      "        [0.0933]], grad_fn=<AddBackward0>) 9.948802947998047 w: tensor(2., requires_grad=True) b:  tensor([0.0933], requires_grad=True)\n",
      "epoch: 15900 y: tensor([[0.0939],\n",
      "        [0.0939],\n",
      "        [0.0939],\n",
      "        [0.0939]], grad_fn=<AddBackward0>) 9.94542407989502 w: tensor(2., requires_grad=True) b:  tensor([0.0939], requires_grad=True)\n",
      "epoch: 16000 y: tensor([[0.0945],\n",
      "        [0.0945],\n",
      "        [0.0945],\n",
      "        [0.0945]], grad_fn=<AddBackward0>) 9.942048072814941 w: tensor(2., requires_grad=True) b:  tensor([0.0945], requires_grad=True)\n",
      "epoch: 16100 y: tensor([[0.0951],\n",
      "        [0.0951],\n",
      "        [0.0951],\n",
      "        [0.0951]], grad_fn=<AddBackward0>) 9.938671112060547 w: tensor(2., requires_grad=True) b:  tensor([0.0951], requires_grad=True)\n",
      "epoch: 16200 y: tensor([[0.0956],\n",
      "        [0.0956],\n",
      "        [0.0956],\n",
      "        [0.0956]], grad_fn=<AddBackward0>) 9.935295104980469 w: tensor(2., requires_grad=True) b:  tensor([0.0956], requires_grad=True)\n",
      "epoch: 16300 y: tensor([[0.0962],\n",
      "        [0.0962],\n",
      "        [0.0962],\n",
      "        [0.0962]], grad_fn=<AddBackward0>) 9.931920051574707 w: tensor(2., requires_grad=True) b:  tensor([0.0962], requires_grad=True)\n",
      "epoch: 16400 y: tensor([[0.0968],\n",
      "        [0.0968],\n",
      "        [0.0968],\n",
      "        [0.0968]], grad_fn=<AddBackward0>) 9.928549766540527 w: tensor(2., requires_grad=True) b:  tensor([0.0968], requires_grad=True)\n",
      "epoch: 16500 y: tensor([[0.0974],\n",
      "        [0.0974],\n",
      "        [0.0974],\n",
      "        [0.0974]], grad_fn=<AddBackward0>) 9.925179481506348 w: tensor(2., requires_grad=True) b:  tensor([0.0974], requires_grad=True)\n",
      "epoch: 16600 y: tensor([[0.0980],\n",
      "        [0.0980],\n",
      "        [0.0980],\n",
      "        [0.0980]], grad_fn=<AddBackward0>) 9.9218111038208 w: tensor(2., requires_grad=True) b:  tensor([0.0980], requires_grad=True)\n",
      "epoch: 16700 y: tensor([[0.0985],\n",
      "        [0.0985],\n",
      "        [0.0985],\n",
      "        [0.0985]], grad_fn=<AddBackward0>) 9.918441772460938 w: tensor(2., requires_grad=True) b:  tensor([0.0986], requires_grad=True)\n",
      "epoch: 16800 y: tensor([[0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.0991]], grad_fn=<AddBackward0>) 9.915075302124023 w: tensor(2., requires_grad=True) b:  tensor([0.0991], requires_grad=True)\n",
      "epoch: 16900 y: tensor([[0.0997],\n",
      "        [0.0997],\n",
      "        [0.0997],\n",
      "        [0.0997]], grad_fn=<AddBackward0>) 9.911707878112793 w: tensor(2., requires_grad=True) b:  tensor([0.0997], requires_grad=True)\n",
      "epoch: 17000 y: tensor([[0.1003],\n",
      "        [0.1003],\n",
      "        [0.1003],\n",
      "        [0.1003]], grad_fn=<AddBackward0>) 9.908344268798828 w: tensor(2., requires_grad=True) b:  tensor([0.1003], requires_grad=True)\n",
      "epoch: 17100 y: tensor([[0.1009],\n",
      "        [0.1009],\n",
      "        [0.1009],\n",
      "        [0.1009]], grad_fn=<AddBackward0>) 9.904983520507812 w: tensor(2., requires_grad=True) b:  tensor([0.1009], requires_grad=True)\n",
      "epoch: 17200 y: tensor([[0.1014],\n",
      "        [0.1014],\n",
      "        [0.1014],\n",
      "        [0.1014]], grad_fn=<AddBackward0>) 9.901622772216797 w: tensor(2., requires_grad=True) b:  tensor([0.1015], requires_grad=True)\n",
      "epoch: 17300 y: tensor([[0.1020],\n",
      "        [0.1020],\n",
      "        [0.1020],\n",
      "        [0.1020]], grad_fn=<AddBackward0>) 9.898262977600098 w: tensor(2., requires_grad=True) b:  tensor([0.1020], requires_grad=True)\n",
      "epoch: 17400 y: tensor([[0.1026],\n",
      "        [0.1026],\n",
      "        [0.1026],\n",
      "        [0.1026]], grad_fn=<AddBackward0>) 9.894904136657715 w: tensor(2., requires_grad=True) b:  tensor([0.1026], requires_grad=True)\n",
      "epoch: 17500 y: tensor([[0.1032],\n",
      "        [0.1032],\n",
      "        [0.1032],\n",
      "        [0.1032]], grad_fn=<AddBackward0>) 9.891544342041016 w: tensor(2., requires_grad=True) b:  tensor([0.1032], requires_grad=True)\n",
      "epoch: 17600 y: tensor([[0.1038],\n",
      "        [0.1038],\n",
      "        [0.1038],\n",
      "        [0.1038]], grad_fn=<AddBackward0>) 9.888188362121582 w: tensor(2., requires_grad=True) b:  tensor([0.1038], requires_grad=True)\n",
      "epoch: 17700 y: tensor([[0.1043],\n",
      "        [0.1043],\n",
      "        [0.1043],\n",
      "        [0.1043]], grad_fn=<AddBackward0>) 9.884835243225098 w: tensor(2., requires_grad=True) b:  tensor([0.1043], requires_grad=True)\n",
      "epoch: 17800 y: tensor([[0.1049],\n",
      "        [0.1049],\n",
      "        [0.1049],\n",
      "        [0.1049]], grad_fn=<AddBackward0>) 9.88148307800293 w: tensor(2., requires_grad=True) b:  tensor([0.1049], requires_grad=True)\n",
      "epoch: 17900 y: tensor([[0.1055],\n",
      "        [0.1055],\n",
      "        [0.1055],\n",
      "        [0.1055]], grad_fn=<AddBackward0>) 9.878131866455078 w: tensor(2., requires_grad=True) b:  tensor([0.1055], requires_grad=True)\n",
      "epoch: 18000 y: tensor([[0.1061],\n",
      "        [0.1061],\n",
      "        [0.1061],\n",
      "        [0.1061]], grad_fn=<AddBackward0>) 9.874780654907227 w: tensor(2., requires_grad=True) b:  tensor([0.1061], requires_grad=True)\n",
      "epoch: 18100 y: tensor([[0.1067],\n",
      "        [0.1067],\n",
      "        [0.1067],\n",
      "        [0.1067]], grad_fn=<AddBackward0>) 9.871429443359375 w: tensor(2., requires_grad=True) b:  tensor([0.1067], requires_grad=True)\n",
      "epoch: 18200 y: tensor([[0.1072],\n",
      "        [0.1072],\n",
      "        [0.1072],\n",
      "        [0.1072]], grad_fn=<AddBackward0>) 9.868080139160156 w: tensor(2., requires_grad=True) b:  tensor([0.1072], requires_grad=True)\n",
      "epoch: 18300 y: tensor([[0.1078],\n",
      "        [0.1078],\n",
      "        [0.1078],\n",
      "        [0.1078]], grad_fn=<AddBackward0>) 9.864734649658203 w: tensor(2., requires_grad=True) b:  tensor([0.1078], requires_grad=True)\n",
      "epoch: 18400 y: tensor([[0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084],\n",
      "        [0.1084]], grad_fn=<AddBackward0>) 9.861391067504883 w: tensor(2., requires_grad=True) b:  tensor([0.1084], requires_grad=True)\n",
      "epoch: 18500 y: tensor([[0.1090],\n",
      "        [0.1090],\n",
      "        [0.1090],\n",
      "        [0.1090]], grad_fn=<AddBackward0>) 9.858048439025879 w: tensor(2., requires_grad=True) b:  tensor([0.1090], requires_grad=True)\n",
      "epoch: 18600 y: tensor([[0.1095],\n",
      "        [0.1095],\n",
      "        [0.1095],\n",
      "        [0.1095]], grad_fn=<AddBackward0>) 9.854704856872559 w: tensor(2., requires_grad=True) b:  tensor([0.1096], requires_grad=True)\n",
      "epoch: 18700 y: tensor([[0.1101],\n",
      "        [0.1101],\n",
      "        [0.1101],\n",
      "        [0.1101]], grad_fn=<AddBackward0>) 9.851363182067871 w: tensor(2., requires_grad=True) b:  tensor([0.1101], requires_grad=True)\n",
      "epoch: 18800 y: tensor([[0.1107],\n",
      "        [0.1107],\n",
      "        [0.1107],\n",
      "        [0.1107]], grad_fn=<AddBackward0>) 9.848021507263184 w: tensor(2., requires_grad=True) b:  tensor([0.1107], requires_grad=True)\n",
      "epoch: 18900 y: tensor([[0.1113],\n",
      "        [0.1113],\n",
      "        [0.1113],\n",
      "        [0.1113]], grad_fn=<AddBackward0>) 9.844682693481445 w: tensor(2., requires_grad=True) b:  tensor([0.1113], requires_grad=True)\n",
      "epoch: 19000 y: tensor([[0.1119],\n",
      "        [0.1119],\n",
      "        [0.1119],\n",
      "        [0.1119]], grad_fn=<AddBackward0>) 9.841347694396973 w: tensor(2., requires_grad=True) b:  tensor([0.1119], requires_grad=True)\n",
      "epoch: 19100 y: tensor([[0.1124],\n",
      "        [0.1124],\n",
      "        [0.1124],\n",
      "        [0.1124]], grad_fn=<AddBackward0>) 9.8380126953125 w: tensor(2., requires_grad=True) b:  tensor([0.1124], requires_grad=True)\n",
      "epoch: 19200 y: tensor([[0.1130],\n",
      "        [0.1130],\n",
      "        [0.1130],\n",
      "        [0.1130]], grad_fn=<AddBackward0>) 9.834678649902344 w: tensor(2., requires_grad=True) b:  tensor([0.1130], requires_grad=True)\n",
      "epoch: 19300 y: tensor([[0.1136],\n",
      "        [0.1136],\n",
      "        [0.1136],\n",
      "        [0.1136]], grad_fn=<AddBackward0>) 9.831344604492188 w: tensor(2., requires_grad=True) b:  tensor([0.1136], requires_grad=True)\n",
      "epoch: 19400 y: tensor([[0.1142],\n",
      "        [0.1142],\n",
      "        [0.1142],\n",
      "        [0.1142]], grad_fn=<AddBackward0>) 9.828011512756348 w: tensor(2., requires_grad=True) b:  tensor([0.1142], requires_grad=True)\n",
      "epoch: 19500 y: tensor([[0.1147],\n",
      "        [0.1147],\n",
      "        [0.1147],\n",
      "        [0.1147]], grad_fn=<AddBackward0>) 9.824679374694824 w: tensor(2., requires_grad=True) b:  tensor([0.1148], requires_grad=True)\n",
      "epoch: 19600 y: tensor([[0.1153],\n",
      "        [0.1153],\n",
      "        [0.1153],\n",
      "        [0.1153]], grad_fn=<AddBackward0>) 9.821352005004883 w: tensor(2., requires_grad=True) b:  tensor([0.1153], requires_grad=True)\n",
      "epoch: 19700 y: tensor([[0.1159],\n",
      "        [0.1159],\n",
      "        [0.1159],\n",
      "        [0.1159]], grad_fn=<AddBackward0>) 9.818023681640625 w: tensor(2., requires_grad=True) b:  tensor([0.1159], requires_grad=True)\n",
      "epoch: 19800 y: tensor([[0.1165],\n",
      "        [0.1165],\n",
      "        [0.1165],\n",
      "        [0.1165]], grad_fn=<AddBackward0>) 9.814699172973633 w: tensor(2., requires_grad=True) b:  tensor([0.1165], requires_grad=True)\n",
      "epoch: 19900 y: tensor([[0.1171],\n",
      "        [0.1171],\n",
      "        [0.1171],\n",
      "        [0.1171]], grad_fn=<AddBackward0>) 9.811372756958008 w: tensor(2., requires_grad=True) b:  tensor([0.1171], requires_grad=True)\n",
      "epoch: 20000 y: tensor([[0.1176],\n",
      "        [0.1176],\n",
      "        [0.1176],\n",
      "        [0.1176]], grad_fn=<AddBackward0>) 9.808048248291016 w: tensor(2., requires_grad=True) b:  tensor([0.1176], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD([w, b], lr=1e-6)\n",
    "epochs = 20001\n",
    "for epoch in range(epochs):\n",
    "    y=x_data.matmul(W) +b\n",
    "    cost = torch.mean((t_data-y)**2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch:' , epoch, 'y:', y, cost.item(), 'w:', w, 'b: ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    h = x.matmul(W) +b\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1176]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(torch.FloatTensor([[10, 11]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1176]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h = torch.FloatTensor([[11, 12]]).matmul(W) + b\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>80</td>\n",
       "      <td>75</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89</td>\n",
       "      <td>91</td>\n",
       "      <td>90</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73</td>\n",
       "      <td>66</td>\n",
       "      <td>70</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>55</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>47</td>\n",
       "      <td>56</td>\n",
       "      <td>60</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>87</td>\n",
       "      <td>79</td>\n",
       "      <td>90</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>79</td>\n",
       "      <td>70</td>\n",
       "      <td>88</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>70</td>\n",
       "      <td>65</td>\n",
       "      <td>74</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>91</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>73</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>78</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>93</td>\n",
       "      <td>89</td>\n",
       "      <td>96</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>68</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>81</td>\n",
       "      <td>90</td>\n",
       "      <td>93</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>86</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>78</td>\n",
       "      <td>83</td>\n",
       "      <td>77</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>82</td>\n",
       "      <td>86</td>\n",
       "      <td>90</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>86</td>\n",
       "      <td>82</td>\n",
       "      <td>89</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>78</td>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>76</td>\n",
       "      <td>83</td>\n",
       "      <td>71</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>96</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1    2    3\n",
       "0   73  80   75  152\n",
       "1   93  88   93  185\n",
       "2   89  91   90  180\n",
       "3   96  98  100  196\n",
       "4   73  66   70  142\n",
       "5   53  46   55  101\n",
       "6   69  74   77  149\n",
       "7   47  56   60  115\n",
       "8   87  79   90  175\n",
       "9   79  70   88  164\n",
       "10  69  70   73  141\n",
       "11  70  65   74  141\n",
       "12  93  95   91  184\n",
       "13  79  80   73  152\n",
       "14  70  73   78  148\n",
       "15  93  89   96  192\n",
       "16  78  75   68  147\n",
       "17  81  90   93  183\n",
       "18  88  92   86  177\n",
       "19  78  83   77  159\n",
       "20  82  86   90  177\n",
       "21  86  82   89  175\n",
       "22  78  83   85  175\n",
       "23  76  83   71  149\n",
       "24  96  93   95  192"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/data-01-test-score.csv', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 73  80  75]\n",
      " [ 93  88  93]\n",
      " [ 89  91  90]\n",
      " [ 96  98 100]\n",
      " [ 73  66  70]\n",
      " [ 53  46  55]\n",
      " [ 69  74  77]\n",
      " [ 47  56  60]\n",
      " [ 87  79  90]\n",
      " [ 79  70  88]\n",
      " [ 69  70  73]\n",
      " [ 70  65  74]\n",
      " [ 93  95  91]\n",
      " [ 79  80  73]\n",
      " [ 70  73  78]\n",
      " [ 93  89  96]\n",
      " [ 78  75  68]\n",
      " [ 81  90  93]\n",
      " [ 88  92  86]\n",
      " [ 78  83  77]\n",
      " [ 82  86  90]\n",
      " [ 86  82  89]\n",
      " [ 78  83  85]\n",
      " [ 76  83  71]\n",
      " [ 96  93  95]]\n",
      "[[152]\n",
      " [185]\n",
      " [180]\n",
      " [196]\n",
      " [142]\n",
      " [101]\n",
      " [149]\n",
      " [115]\n",
      " [175]\n",
      " [164]\n",
      " [141]\n",
      " [141]\n",
      " [184]\n",
      " [152]\n",
      " [148]\n",
      " [192]\n",
      " [147]\n",
      " [183]\n",
      " [177]\n",
      " [159]\n",
      " [177]\n",
      " [175]\n",
      " [175]\n",
      " [149]\n",
      " [192]]\n"
     ]
    }
   ],
   "source": [
    "data = df.values\n",
    "x_data = data[:, :3].reshape(-1, 3) #모든 행의 인덱스 2까지만  // 행은 몇 개인지 모르고, 열은 3개개\n",
    "\n",
    "y_data = data[:, -1].reshape(-1, 1) #마지막 열만 가져옴\n",
    "\n",
    "print(x_data)\n",
    "print(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  80.,  75.],\n",
      "        [ 93.,  88.,  93.],\n",
      "        [ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.],\n",
      "        [ 53.,  46.,  55.],\n",
      "        [ 69.,  74.,  77.],\n",
      "        [ 47.,  56.,  60.],\n",
      "        [ 87.,  79.,  90.],\n",
      "        [ 79.,  70.,  88.],\n",
      "        [ 69.,  70.,  73.],\n",
      "        [ 70.,  65.,  74.],\n",
      "        [ 93.,  95.,  91.],\n",
      "        [ 79.,  80.,  73.],\n",
      "        [ 70.,  73.,  78.],\n",
      "        [ 93.,  89.,  96.],\n",
      "        [ 78.,  75.,  68.],\n",
      "        [ 81.,  90.,  93.],\n",
      "        [ 88.,  92.,  86.],\n",
      "        [ 78.,  83.,  77.],\n",
      "        [ 82.,  86.,  90.],\n",
      "        [ 86.,  82.,  89.],\n",
      "        [ 78.,  83.,  85.],\n",
      "        [ 76.,  83.,  71.],\n",
      "        [ 96.,  93.,  95.]])\n",
      "tensor([[152.],\n",
      "        [185.],\n",
      "        [180.],\n",
      "        [196.],\n",
      "        [142.],\n",
      "        [101.],\n",
      "        [149.],\n",
      "        [115.],\n",
      "        [175.],\n",
      "        [164.],\n",
      "        [141.],\n",
      "        [141.],\n",
      "        [184.],\n",
      "        [152.],\n",
      "        [148.],\n",
      "        [192.],\n",
      "        [147.],\n",
      "        [183.],\n",
      "        [177.],\n",
      "        [159.],\n",
      "        [177.],\n",
      "        [175.],\n",
      "        [175.],\n",
      "        [149.],\n",
      "        [192.]])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.from_numpy(x_data).float() #토치의 텐서 데이터로 만듦. \n",
    "y_train = torch.from_numpy(y_data).float()\n",
    "print(x_train)\n",
    "print(y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((3, 1), requires_grad=True) #토치 데이터의 열이 3개이므로 3행으로 만들어야 함 \n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_list = []\n",
    "optimizer = optim.SGD([W, b], lr = 1e-6)\n",
    "\n",
    "epochs=30001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 cost:  tensor(26811.9609, grad_fn=<MeanBackward0>) W tensor([[0.0262],\n",
      "        [0.0263],\n",
      "        [0.0269]], requires_grad=True) b: tensor([0.0003], requires_grad=True)\n",
      "epoch: 100 cost:  tensor(20.7345, grad_fn=<MeanBackward0>) W tensor([[0.6548],\n",
      "        [0.6583],\n",
      "        [0.6764]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 200 cost:  tensor(11.6739, grad_fn=<MeanBackward0>) W tensor([[0.6656],\n",
      "        [0.6690],\n",
      "        [0.6899]], requires_grad=True) b: tensor([0.0083], requires_grad=True)\n",
      "epoch: 300 cost:  tensor(11.6277, grad_fn=<MeanBackward0>) W tensor([[0.6650],\n",
      "        [0.6683],\n",
      "        [0.6918]], requires_grad=True) b: tensor([0.0083], requires_grad=True)\n",
      "epoch: 400 cost:  tensor(11.5849, grad_fn=<MeanBackward0>) W tensor([[0.6642],\n",
      "        [0.6674],\n",
      "        [0.6935]], requires_grad=True) b: tensor([0.0083], requires_grad=True)\n",
      "epoch: 500 cost:  tensor(11.5425, grad_fn=<MeanBackward0>) W tensor([[0.6634],\n",
      "        [0.6665],\n",
      "        [0.6952]], requires_grad=True) b: tensor([0.0083], requires_grad=True)\n",
      "epoch: 600 cost:  tensor(11.5004, grad_fn=<MeanBackward0>) W tensor([[0.6626],\n",
      "        [0.6656],\n",
      "        [0.6968]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 700 cost:  tensor(11.4586, grad_fn=<MeanBackward0>) W tensor([[0.6618],\n",
      "        [0.6647],\n",
      "        [0.6985]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 800 cost:  tensor(11.4172, grad_fn=<MeanBackward0>) W tensor([[0.6610],\n",
      "        [0.6638],\n",
      "        [0.7001]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 900 cost:  tensor(11.3761, grad_fn=<MeanBackward0>) W tensor([[0.6602],\n",
      "        [0.6629],\n",
      "        [0.7018]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 1000 cost:  tensor(11.3354, grad_fn=<MeanBackward0>) W tensor([[0.6594],\n",
      "        [0.6620],\n",
      "        [0.7034]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 1100 cost:  tensor(11.2950, grad_fn=<MeanBackward0>) W tensor([[0.6586],\n",
      "        [0.6612],\n",
      "        [0.7050]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 1200 cost:  tensor(11.2549, grad_fn=<MeanBackward0>) W tensor([[0.6578],\n",
      "        [0.6603],\n",
      "        [0.7066]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 1300 cost:  tensor(11.2151, grad_fn=<MeanBackward0>) W tensor([[0.6570],\n",
      "        [0.6594],\n",
      "        [0.7083]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 1400 cost:  tensor(11.1757, grad_fn=<MeanBackward0>) W tensor([[0.6563],\n",
      "        [0.6586],\n",
      "        [0.7099]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 1500 cost:  tensor(11.1366, grad_fn=<MeanBackward0>) W tensor([[0.6555],\n",
      "        [0.6577],\n",
      "        [0.7115]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 1600 cost:  tensor(11.0977, grad_fn=<MeanBackward0>) W tensor([[0.6547],\n",
      "        [0.6568],\n",
      "        [0.7131]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 1700 cost:  tensor(11.0592, grad_fn=<MeanBackward0>) W tensor([[0.6539],\n",
      "        [0.6560],\n",
      "        [0.7147]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 1800 cost:  tensor(11.0211, grad_fn=<MeanBackward0>) W tensor([[0.6531],\n",
      "        [0.6551],\n",
      "        [0.7162]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 1900 cost:  tensor(10.9832, grad_fn=<MeanBackward0>) W tensor([[0.6524],\n",
      "        [0.6543],\n",
      "        [0.7178]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 2000 cost:  tensor(10.9456, grad_fn=<MeanBackward0>) W tensor([[0.6516],\n",
      "        [0.6535],\n",
      "        [0.7194]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 2100 cost:  tensor(10.9084, grad_fn=<MeanBackward0>) W tensor([[0.6508],\n",
      "        [0.6526],\n",
      "        [0.7209]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 2200 cost:  tensor(10.8714, grad_fn=<MeanBackward0>) W tensor([[0.6501],\n",
      "        [0.6518],\n",
      "        [0.7225]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 2300 cost:  tensor(10.8348, grad_fn=<MeanBackward0>) W tensor([[0.6493],\n",
      "        [0.6510],\n",
      "        [0.7240]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 2400 cost:  tensor(10.7984, grad_fn=<MeanBackward0>) W tensor([[0.6485],\n",
      "        [0.6502],\n",
      "        [0.7256]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 2500 cost:  tensor(10.7623, grad_fn=<MeanBackward0>) W tensor([[0.6478],\n",
      "        [0.6493],\n",
      "        [0.7271]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 2600 cost:  tensor(10.7266, grad_fn=<MeanBackward0>) W tensor([[0.6470],\n",
      "        [0.6485],\n",
      "        [0.7286]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 2700 cost:  tensor(10.6911, grad_fn=<MeanBackward0>) W tensor([[0.6463],\n",
      "        [0.6477],\n",
      "        [0.7302]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 2800 cost:  tensor(10.6559, grad_fn=<MeanBackward0>) W tensor([[0.6455],\n",
      "        [0.6469],\n",
      "        [0.7317]], requires_grad=True) b: tensor([0.0082], requires_grad=True)\n",
      "epoch: 2900 cost:  tensor(10.6210, grad_fn=<MeanBackward0>) W tensor([[0.6448],\n",
      "        [0.6461],\n",
      "        [0.7332]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 3000 cost:  tensor(10.5864, grad_fn=<MeanBackward0>) W tensor([[0.6440],\n",
      "        [0.6453],\n",
      "        [0.7347]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 3100 cost:  tensor(10.5520, grad_fn=<MeanBackward0>) W tensor([[0.6432],\n",
      "        [0.6445],\n",
      "        [0.7362]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 3200 cost:  tensor(10.5180, grad_fn=<MeanBackward0>) W tensor([[0.6425],\n",
      "        [0.6438],\n",
      "        [0.7377]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 3300 cost:  tensor(10.4842, grad_fn=<MeanBackward0>) W tensor([[0.6418],\n",
      "        [0.6430],\n",
      "        [0.7392]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 3400 cost:  tensor(10.4506, grad_fn=<MeanBackward0>) W tensor([[0.6410],\n",
      "        [0.6422],\n",
      "        [0.7407]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 3500 cost:  tensor(10.4174, grad_fn=<MeanBackward0>) W tensor([[0.6403],\n",
      "        [0.6414],\n",
      "        [0.7421]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 3600 cost:  tensor(10.3844, grad_fn=<MeanBackward0>) W tensor([[0.6395],\n",
      "        [0.6407],\n",
      "        [0.7436]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 3700 cost:  tensor(10.3517, grad_fn=<MeanBackward0>) W tensor([[0.6388],\n",
      "        [0.6399],\n",
      "        [0.7451]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 3800 cost:  tensor(10.3193, grad_fn=<MeanBackward0>) W tensor([[0.6381],\n",
      "        [0.6391],\n",
      "        [0.7465]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 3900 cost:  tensor(10.2871, grad_fn=<MeanBackward0>) W tensor([[0.6373],\n",
      "        [0.6384],\n",
      "        [0.7480]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 4000 cost:  tensor(10.2552, grad_fn=<MeanBackward0>) W tensor([[0.6366],\n",
      "        [0.6376],\n",
      "        [0.7494]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 4100 cost:  tensor(10.2235, grad_fn=<MeanBackward0>) W tensor([[0.6359],\n",
      "        [0.6369],\n",
      "        [0.7509]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 4200 cost:  tensor(10.1921, grad_fn=<MeanBackward0>) W tensor([[0.6351],\n",
      "        [0.6361],\n",
      "        [0.7523]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 4300 cost:  tensor(10.1609, grad_fn=<MeanBackward0>) W tensor([[0.6344],\n",
      "        [0.6354],\n",
      "        [0.7537]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 4400 cost:  tensor(10.1300, grad_fn=<MeanBackward0>) W tensor([[0.6337],\n",
      "        [0.6347],\n",
      "        [0.7552]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 4500 cost:  tensor(10.0994, grad_fn=<MeanBackward0>) W tensor([[0.6330],\n",
      "        [0.6339],\n",
      "        [0.7566]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 4600 cost:  tensor(10.0689, grad_fn=<MeanBackward0>) W tensor([[0.6323],\n",
      "        [0.6332],\n",
      "        [0.7580]], requires_grad=True) b: tensor([0.0081], requires_grad=True)\n",
      "epoch: 4700 cost:  tensor(10.0387, grad_fn=<MeanBackward0>) W tensor([[0.6315],\n",
      "        [0.6325],\n",
      "        [0.7594]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 4800 cost:  tensor(10.0088, grad_fn=<MeanBackward0>) W tensor([[0.6308],\n",
      "        [0.6317],\n",
      "        [0.7608]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 4900 cost:  tensor(9.9791, grad_fn=<MeanBackward0>) W tensor([[0.6301],\n",
      "        [0.6310],\n",
      "        [0.7622]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 5000 cost:  tensor(9.9496, grad_fn=<MeanBackward0>) W tensor([[0.6294],\n",
      "        [0.6303],\n",
      "        [0.7636]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 5100 cost:  tensor(9.9204, grad_fn=<MeanBackward0>) W tensor([[0.6287],\n",
      "        [0.6296],\n",
      "        [0.7650]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 5200 cost:  tensor(9.8914, grad_fn=<MeanBackward0>) W tensor([[0.6280],\n",
      "        [0.6289],\n",
      "        [0.7663]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 5300 cost:  tensor(9.8627, grad_fn=<MeanBackward0>) W tensor([[0.6273],\n",
      "        [0.6282],\n",
      "        [0.7677]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 5400 cost:  tensor(9.8342, grad_fn=<MeanBackward0>) W tensor([[0.6266],\n",
      "        [0.6275],\n",
      "        [0.7691]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 5500 cost:  tensor(9.8058, grad_fn=<MeanBackward0>) W tensor([[0.6259],\n",
      "        [0.6268],\n",
      "        [0.7704]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 5600 cost:  tensor(9.7778, grad_fn=<MeanBackward0>) W tensor([[0.6252],\n",
      "        [0.6261],\n",
      "        [0.7718]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 5700 cost:  tensor(9.7499, grad_fn=<MeanBackward0>) W tensor([[0.6245],\n",
      "        [0.6254],\n",
      "        [0.7731]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 5800 cost:  tensor(9.7223, grad_fn=<MeanBackward0>) W tensor([[0.6238],\n",
      "        [0.6247],\n",
      "        [0.7745]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 5900 cost:  tensor(9.6949, grad_fn=<MeanBackward0>) W tensor([[0.6231],\n",
      "        [0.6241],\n",
      "        [0.7758]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 6000 cost:  tensor(9.6677, grad_fn=<MeanBackward0>) W tensor([[0.6224],\n",
      "        [0.6234],\n",
      "        [0.7772]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 6100 cost:  tensor(9.6407, grad_fn=<MeanBackward0>) W tensor([[0.6217],\n",
      "        [0.6227],\n",
      "        [0.7785]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 6200 cost:  tensor(9.6140, grad_fn=<MeanBackward0>) W tensor([[0.6210],\n",
      "        [0.6220],\n",
      "        [0.7798]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 6300 cost:  tensor(9.5874, grad_fn=<MeanBackward0>) W tensor([[0.6203],\n",
      "        [0.6214],\n",
      "        [0.7811]], requires_grad=True) b: tensor([0.0080], requires_grad=True)\n",
      "epoch: 6400 cost:  tensor(9.5611, grad_fn=<MeanBackward0>) W tensor([[0.6196],\n",
      "        [0.6207],\n",
      "        [0.7824]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 6500 cost:  tensor(9.5350, grad_fn=<MeanBackward0>) W tensor([[0.6189],\n",
      "        [0.6201],\n",
      "        [0.7838]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 6600 cost:  tensor(9.5091, grad_fn=<MeanBackward0>) W tensor([[0.6183],\n",
      "        [0.6194],\n",
      "        [0.7851]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 6700 cost:  tensor(9.4834, grad_fn=<MeanBackward0>) W tensor([[0.6176],\n",
      "        [0.6188],\n",
      "        [0.7864]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 6800 cost:  tensor(9.4579, grad_fn=<MeanBackward0>) W tensor([[0.6169],\n",
      "        [0.6181],\n",
      "        [0.7876]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 6900 cost:  tensor(9.4327, grad_fn=<MeanBackward0>) W tensor([[0.6162],\n",
      "        [0.6175],\n",
      "        [0.7889]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 7000 cost:  tensor(9.4076, grad_fn=<MeanBackward0>) W tensor([[0.6156],\n",
      "        [0.6168],\n",
      "        [0.7902]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 7100 cost:  tensor(9.3827, grad_fn=<MeanBackward0>) W tensor([[0.6149],\n",
      "        [0.6162],\n",
      "        [0.7915]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 7200 cost:  tensor(9.3580, grad_fn=<MeanBackward0>) W tensor([[0.6142],\n",
      "        [0.6156],\n",
      "        [0.7928]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 7300 cost:  tensor(9.3335, grad_fn=<MeanBackward0>) W tensor([[0.6135],\n",
      "        [0.6149],\n",
      "        [0.7940]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 7400 cost:  tensor(9.3093, grad_fn=<MeanBackward0>) W tensor([[0.6129],\n",
      "        [0.6143],\n",
      "        [0.7953]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 7500 cost:  tensor(9.2852, grad_fn=<MeanBackward0>) W tensor([[0.6122],\n",
      "        [0.6137],\n",
      "        [0.7965]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 7600 cost:  tensor(9.2613, grad_fn=<MeanBackward0>) W tensor([[0.6115],\n",
      "        [0.6131],\n",
      "        [0.7978]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 7700 cost:  tensor(9.2376, grad_fn=<MeanBackward0>) W tensor([[0.6109],\n",
      "        [0.6124],\n",
      "        [0.7990]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 7800 cost:  tensor(9.2140, grad_fn=<MeanBackward0>) W tensor([[0.6102],\n",
      "        [0.6118],\n",
      "        [0.8003]], requires_grad=True) b: tensor([0.0079], requires_grad=True)\n",
      "epoch: 7900 cost:  tensor(9.1907, grad_fn=<MeanBackward0>) W tensor([[0.6096],\n",
      "        [0.6112],\n",
      "        [0.8015]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 8000 cost:  tensor(9.1676, grad_fn=<MeanBackward0>) W tensor([[0.6089],\n",
      "        [0.6106],\n",
      "        [0.8027]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 8100 cost:  tensor(9.1446, grad_fn=<MeanBackward0>) W tensor([[0.6083],\n",
      "        [0.6100],\n",
      "        [0.8040]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 8200 cost:  tensor(9.1218, grad_fn=<MeanBackward0>) W tensor([[0.6076],\n",
      "        [0.6094],\n",
      "        [0.8052]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 8300 cost:  tensor(9.0992, grad_fn=<MeanBackward0>) W tensor([[0.6069],\n",
      "        [0.6088],\n",
      "        [0.8064]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 8400 cost:  tensor(9.0768, grad_fn=<MeanBackward0>) W tensor([[0.6063],\n",
      "        [0.6082],\n",
      "        [0.8076]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 8500 cost:  tensor(9.0545, grad_fn=<MeanBackward0>) W tensor([[0.6057],\n",
      "        [0.6076],\n",
      "        [0.8088]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 8600 cost:  tensor(9.0325, grad_fn=<MeanBackward0>) W tensor([[0.6050],\n",
      "        [0.6070],\n",
      "        [0.8100]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 8700 cost:  tensor(9.0106, grad_fn=<MeanBackward0>) W tensor([[0.6044],\n",
      "        [0.6065],\n",
      "        [0.8112]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 8800 cost:  tensor(8.9889, grad_fn=<MeanBackward0>) W tensor([[0.6037],\n",
      "        [0.6059],\n",
      "        [0.8124]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 8900 cost:  tensor(8.9673, grad_fn=<MeanBackward0>) W tensor([[0.6031],\n",
      "        [0.6053],\n",
      "        [0.8136]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 9000 cost:  tensor(8.9459, grad_fn=<MeanBackward0>) W tensor([[0.6024],\n",
      "        [0.6047],\n",
      "        [0.8148]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 9100 cost:  tensor(8.9247, grad_fn=<MeanBackward0>) W tensor([[0.6018],\n",
      "        [0.6042],\n",
      "        [0.8160]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 9200 cost:  tensor(8.9037, grad_fn=<MeanBackward0>) W tensor([[0.6012],\n",
      "        [0.6036],\n",
      "        [0.8171]], requires_grad=True) b: tensor([0.0078], requires_grad=True)\n",
      "epoch: 9300 cost:  tensor(8.8828, grad_fn=<MeanBackward0>) W tensor([[0.6005],\n",
      "        [0.6030],\n",
      "        [0.8183]], requires_grad=True) b: tensor([0.0077], requires_grad=True)\n",
      "epoch: 9400 cost:  tensor(8.8621, grad_fn=<MeanBackward0>) W tensor([[0.5999],\n",
      "        [0.6025],\n",
      "        [0.8195]], requires_grad=True) b: tensor([0.0077], requires_grad=True)\n",
      "epoch: 9500 cost:  tensor(8.8415, grad_fn=<MeanBackward0>) W tensor([[0.5993],\n",
      "        [0.6019],\n",
      "        [0.8206]], requires_grad=True) b: tensor([0.0077], requires_grad=True)\n",
      "epoch: 9600 cost:  tensor(8.8211, grad_fn=<MeanBackward0>) W tensor([[0.5986],\n",
      "        [0.6014],\n",
      "        [0.8218]], requires_grad=True) b: tensor([0.0077], requires_grad=True)\n",
      "epoch: 9700 cost:  tensor(8.8009, grad_fn=<MeanBackward0>) W tensor([[0.5980],\n",
      "        [0.6008],\n",
      "        [0.8229]], requires_grad=True) b: tensor([0.0077], requires_grad=True)\n",
      "epoch: 9800 cost:  tensor(8.7809, grad_fn=<MeanBackward0>) W tensor([[0.5974],\n",
      "        [0.6003],\n",
      "        [0.8241]], requires_grad=True) b: tensor([0.0077], requires_grad=True)\n",
      "epoch: 9900 cost:  tensor(8.7609, grad_fn=<MeanBackward0>) W tensor([[0.5968],\n",
      "        [0.5997],\n",
      "        [0.8252]], requires_grad=True) b: tensor([0.0077], requires_grad=True)\n",
      "epoch: 10000 cost:  tensor(8.7412, grad_fn=<MeanBackward0>) W tensor([[0.5961],\n",
      "        [0.5992],\n",
      "        [0.8263]], requires_grad=True) b: tensor([0.0077], requires_grad=True)\n",
      "epoch: 10100 cost:  tensor(8.7216, grad_fn=<MeanBackward0>) W tensor([[0.5955],\n",
      "        [0.5986],\n",
      "        [0.8275]], requires_grad=True) b: tensor([0.0077], requires_grad=True)\n",
      "epoch: 10200 cost:  tensor(8.7021, grad_fn=<MeanBackward0>) W tensor([[0.5949],\n",
      "        [0.5981],\n",
      "        [0.8286]], requires_grad=True) b: tensor([0.0077], requires_grad=True)\n",
      "epoch: 10300 cost:  tensor(8.6828, grad_fn=<MeanBackward0>) W tensor([[0.5943],\n",
      "        [0.5975],\n",
      "        [0.8297]], requires_grad=True) b: tensor([0.0077], requires_grad=True)\n",
      "epoch: 10400 cost:  tensor(8.6637, grad_fn=<MeanBackward0>) W tensor([[0.5937],\n",
      "        [0.5970],\n",
      "        [0.8309]], requires_grad=True) b: tensor([0.0077], requires_grad=True)\n",
      "epoch: 10500 cost:  tensor(8.6447, grad_fn=<MeanBackward0>) W tensor([[0.5930],\n",
      "        [0.5965],\n",
      "        [0.8320]], requires_grad=True) b: tensor([0.0077], requires_grad=True)\n",
      "epoch: 10600 cost:  tensor(8.6259, grad_fn=<MeanBackward0>) W tensor([[0.5924],\n",
      "        [0.5960],\n",
      "        [0.8331]], requires_grad=True) b: tensor([0.0076], requires_grad=True)\n",
      "epoch: 10700 cost:  tensor(8.6072, grad_fn=<MeanBackward0>) W tensor([[0.5918],\n",
      "        [0.5954],\n",
      "        [0.8342]], requires_grad=True) b: tensor([0.0076], requires_grad=True)\n",
      "epoch: 10800 cost:  tensor(8.5886, grad_fn=<MeanBackward0>) W tensor([[0.5912],\n",
      "        [0.5949],\n",
      "        [0.8353]], requires_grad=True) b: tensor([0.0076], requires_grad=True)\n",
      "epoch: 10900 cost:  tensor(8.5702, grad_fn=<MeanBackward0>) W tensor([[0.5906],\n",
      "        [0.5944],\n",
      "        [0.8364]], requires_grad=True) b: tensor([0.0076], requires_grad=True)\n",
      "epoch: 11000 cost:  tensor(8.5520, grad_fn=<MeanBackward0>) W tensor([[0.5900],\n",
      "        [0.5939],\n",
      "        [0.8375]], requires_grad=True) b: tensor([0.0076], requires_grad=True)\n",
      "epoch: 11100 cost:  tensor(8.5339, grad_fn=<MeanBackward0>) W tensor([[0.5894],\n",
      "        [0.5934],\n",
      "        [0.8386]], requires_grad=True) b: tensor([0.0076], requires_grad=True)\n",
      "epoch: 11200 cost:  tensor(8.5159, grad_fn=<MeanBackward0>) W tensor([[0.5888],\n",
      "        [0.5929],\n",
      "        [0.8396]], requires_grad=True) b: tensor([0.0076], requires_grad=True)\n",
      "epoch: 11300 cost:  tensor(8.4981, grad_fn=<MeanBackward0>) W tensor([[0.5882],\n",
      "        [0.5924],\n",
      "        [0.8407]], requires_grad=True) b: tensor([0.0076], requires_grad=True)\n",
      "epoch: 11400 cost:  tensor(8.4804, grad_fn=<MeanBackward0>) W tensor([[0.5876],\n",
      "        [0.5919],\n",
      "        [0.8418]], requires_grad=True) b: tensor([0.0076], requires_grad=True)\n",
      "epoch: 11500 cost:  tensor(8.4629, grad_fn=<MeanBackward0>) W tensor([[0.5870],\n",
      "        [0.5914],\n",
      "        [0.8429]], requires_grad=True) b: tensor([0.0076], requires_grad=True)\n",
      "epoch: 11600 cost:  tensor(8.4455, grad_fn=<MeanBackward0>) W tensor([[0.5864],\n",
      "        [0.5909],\n",
      "        [0.8439]], requires_grad=True) b: tensor([0.0076], requires_grad=True)\n",
      "epoch: 11700 cost:  tensor(8.4282, grad_fn=<MeanBackward0>) W tensor([[0.5858],\n",
      "        [0.5904],\n",
      "        [0.8450]], requires_grad=True) b: tensor([0.0076], requires_grad=True)\n",
      "epoch: 11800 cost:  tensor(8.4111, grad_fn=<MeanBackward0>) W tensor([[0.5852],\n",
      "        [0.5899],\n",
      "        [0.8460]], requires_grad=True) b: tensor([0.0075], requires_grad=True)\n",
      "epoch: 11900 cost:  tensor(8.3941, grad_fn=<MeanBackward0>) W tensor([[0.5846],\n",
      "        [0.5894],\n",
      "        [0.8471]], requires_grad=True) b: tensor([0.0075], requires_grad=True)\n",
      "epoch: 12000 cost:  tensor(8.3772, grad_fn=<MeanBackward0>) W tensor([[0.5840],\n",
      "        [0.5889],\n",
      "        [0.8481]], requires_grad=True) b: tensor([0.0075], requires_grad=True)\n",
      "epoch: 12100 cost:  tensor(8.3605, grad_fn=<MeanBackward0>) W tensor([[0.5834],\n",
      "        [0.5884],\n",
      "        [0.8492]], requires_grad=True) b: tensor([0.0075], requires_grad=True)\n",
      "epoch: 12200 cost:  tensor(8.3439, grad_fn=<MeanBackward0>) W tensor([[0.5828],\n",
      "        [0.5879],\n",
      "        [0.8502]], requires_grad=True) b: tensor([0.0075], requires_grad=True)\n",
      "epoch: 12300 cost:  tensor(8.3274, grad_fn=<MeanBackward0>) W tensor([[0.5823],\n",
      "        [0.5875],\n",
      "        [0.8513]], requires_grad=True) b: tensor([0.0075], requires_grad=True)\n",
      "epoch: 12400 cost:  tensor(8.3111, grad_fn=<MeanBackward0>) W tensor([[0.5817],\n",
      "        [0.5870],\n",
      "        [0.8523]], requires_grad=True) b: tensor([0.0075], requires_grad=True)\n",
      "epoch: 12500 cost:  tensor(8.2949, grad_fn=<MeanBackward0>) W tensor([[0.5811],\n",
      "        [0.5865],\n",
      "        [0.8533]], requires_grad=True) b: tensor([0.0075], requires_grad=True)\n",
      "epoch: 12600 cost:  tensor(8.2788, grad_fn=<MeanBackward0>) W tensor([[0.5805],\n",
      "        [0.5860],\n",
      "        [0.8544]], requires_grad=True) b: tensor([0.0075], requires_grad=True)\n",
      "epoch: 12700 cost:  tensor(8.2628, grad_fn=<MeanBackward0>) W tensor([[0.5799],\n",
      "        [0.5856],\n",
      "        [0.8554]], requires_grad=True) b: tensor([0.0075], requires_grad=True)\n",
      "epoch: 12800 cost:  tensor(8.2470, grad_fn=<MeanBackward0>) W tensor([[0.5794],\n",
      "        [0.5851],\n",
      "        [0.8564]], requires_grad=True) b: tensor([0.0075], requires_grad=True)\n",
      "epoch: 12900 cost:  tensor(8.2313, grad_fn=<MeanBackward0>) W tensor([[0.5788],\n",
      "        [0.5847],\n",
      "        [0.8574]], requires_grad=True) b: tensor([0.0075], requires_grad=True)\n",
      "epoch: 13000 cost:  tensor(8.2157, grad_fn=<MeanBackward0>) W tensor([[0.5782],\n",
      "        [0.5842],\n",
      "        [0.8584]], requires_grad=True) b: tensor([0.0074], requires_grad=True)\n",
      "epoch: 13100 cost:  tensor(8.2003, grad_fn=<MeanBackward0>) W tensor([[0.5776],\n",
      "        [0.5837],\n",
      "        [0.8594]], requires_grad=True) b: tensor([0.0074], requires_grad=True)\n",
      "epoch: 13200 cost:  tensor(8.1849, grad_fn=<MeanBackward0>) W tensor([[0.5771],\n",
      "        [0.5833],\n",
      "        [0.8604]], requires_grad=True) b: tensor([0.0074], requires_grad=True)\n",
      "epoch: 13300 cost:  tensor(8.1697, grad_fn=<MeanBackward0>) W tensor([[0.5765],\n",
      "        [0.5828],\n",
      "        [0.8614]], requires_grad=True) b: tensor([0.0074], requires_grad=True)\n",
      "epoch: 13400 cost:  tensor(8.1546, grad_fn=<MeanBackward0>) W tensor([[0.5759],\n",
      "        [0.5824],\n",
      "        [0.8624]], requires_grad=True) b: tensor([0.0074], requires_grad=True)\n",
      "epoch: 13500 cost:  tensor(8.1396, grad_fn=<MeanBackward0>) W tensor([[0.5753],\n",
      "        [0.5819],\n",
      "        [0.8634]], requires_grad=True) b: tensor([0.0074], requires_grad=True)\n",
      "epoch: 13600 cost:  tensor(8.1247, grad_fn=<MeanBackward0>) W tensor([[0.5748],\n",
      "        [0.5815],\n",
      "        [0.8644]], requires_grad=True) b: tensor([0.0074], requires_grad=True)\n",
      "epoch: 13700 cost:  tensor(8.1100, grad_fn=<MeanBackward0>) W tensor([[0.5742],\n",
      "        [0.5811],\n",
      "        [0.8654]], requires_grad=True) b: tensor([0.0074], requires_grad=True)\n",
      "epoch: 13800 cost:  tensor(8.0953, grad_fn=<MeanBackward0>) W tensor([[0.5737],\n",
      "        [0.5806],\n",
      "        [0.8663]], requires_grad=True) b: tensor([0.0074], requires_grad=True)\n",
      "epoch: 13900 cost:  tensor(8.0808, grad_fn=<MeanBackward0>) W tensor([[0.5731],\n",
      "        [0.5802],\n",
      "        [0.8673]], requires_grad=True) b: tensor([0.0074], requires_grad=True)\n",
      "epoch: 14000 cost:  tensor(8.0664, grad_fn=<MeanBackward0>) W tensor([[0.5725],\n",
      "        [0.5797],\n",
      "        [0.8683]], requires_grad=True) b: tensor([0.0074], requires_grad=True)\n",
      "epoch: 14100 cost:  tensor(8.0521, grad_fn=<MeanBackward0>) W tensor([[0.5720],\n",
      "        [0.5793],\n",
      "        [0.8692]], requires_grad=True) b: tensor([0.0073], requires_grad=True)\n",
      "epoch: 14200 cost:  tensor(8.0379, grad_fn=<MeanBackward0>) W tensor([[0.5714],\n",
      "        [0.5789],\n",
      "        [0.8702]], requires_grad=True) b: tensor([0.0073], requires_grad=True)\n",
      "epoch: 14300 cost:  tensor(8.0238, grad_fn=<MeanBackward0>) W tensor([[0.5709],\n",
      "        [0.5785],\n",
      "        [0.8712]], requires_grad=True) b: tensor([0.0073], requires_grad=True)\n",
      "epoch: 14400 cost:  tensor(8.0099, grad_fn=<MeanBackward0>) W tensor([[0.5703],\n",
      "        [0.5780],\n",
      "        [0.8721]], requires_grad=True) b: tensor([0.0073], requires_grad=True)\n",
      "epoch: 14500 cost:  tensor(7.9960, grad_fn=<MeanBackward0>) W tensor([[0.5698],\n",
      "        [0.5776],\n",
      "        [0.8731]], requires_grad=True) b: tensor([0.0073], requires_grad=True)\n",
      "epoch: 14600 cost:  tensor(7.9823, grad_fn=<MeanBackward0>) W tensor([[0.5692],\n",
      "        [0.5772],\n",
      "        [0.8740]], requires_grad=True) b: tensor([0.0073], requires_grad=True)\n",
      "epoch: 14700 cost:  tensor(7.9686, grad_fn=<MeanBackward0>) W tensor([[0.5687],\n",
      "        [0.5768],\n",
      "        [0.8749]], requires_grad=True) b: tensor([0.0073], requires_grad=True)\n",
      "epoch: 14800 cost:  tensor(7.9550, grad_fn=<MeanBackward0>) W tensor([[0.5681],\n",
      "        [0.5764],\n",
      "        [0.8759]], requires_grad=True) b: tensor([0.0073], requires_grad=True)\n",
      "epoch: 14900 cost:  tensor(7.9416, grad_fn=<MeanBackward0>) W tensor([[0.5676],\n",
      "        [0.5759],\n",
      "        [0.8768]], requires_grad=True) b: tensor([0.0073], requires_grad=True)\n",
      "epoch: 15000 cost:  tensor(7.9283, grad_fn=<MeanBackward0>) W tensor([[0.5670],\n",
      "        [0.5755],\n",
      "        [0.8777]], requires_grad=True) b: tensor([0.0073], requires_grad=True)\n",
      "epoch: 15100 cost:  tensor(7.9151, grad_fn=<MeanBackward0>) W tensor([[0.5665],\n",
      "        [0.5751],\n",
      "        [0.8787]], requires_grad=True) b: tensor([0.0073], requires_grad=True)\n",
      "epoch: 15200 cost:  tensor(7.9019, grad_fn=<MeanBackward0>) W tensor([[0.5659],\n",
      "        [0.5747],\n",
      "        [0.8796]], requires_grad=True) b: tensor([0.0072], requires_grad=True)\n",
      "epoch: 15300 cost:  tensor(7.8889, grad_fn=<MeanBackward0>) W tensor([[0.5654],\n",
      "        [0.5743],\n",
      "        [0.8805]], requires_grad=True) b: tensor([0.0072], requires_grad=True)\n",
      "epoch: 15400 cost:  tensor(7.8759, grad_fn=<MeanBackward0>) W tensor([[0.5648],\n",
      "        [0.5739],\n",
      "        [0.8814]], requires_grad=True) b: tensor([0.0072], requires_grad=True)\n",
      "epoch: 15500 cost:  tensor(7.8631, grad_fn=<MeanBackward0>) W tensor([[0.5643],\n",
      "        [0.5735],\n",
      "        [0.8823]], requires_grad=True) b: tensor([0.0072], requires_grad=True)\n",
      "epoch: 15600 cost:  tensor(7.8504, grad_fn=<MeanBackward0>) W tensor([[0.5638],\n",
      "        [0.5731],\n",
      "        [0.8833]], requires_grad=True) b: tensor([0.0072], requires_grad=True)\n",
      "epoch: 15700 cost:  tensor(7.8378, grad_fn=<MeanBackward0>) W tensor([[0.5632],\n",
      "        [0.5727],\n",
      "        [0.8842]], requires_grad=True) b: tensor([0.0072], requires_grad=True)\n",
      "epoch: 15800 cost:  tensor(7.8252, grad_fn=<MeanBackward0>) W tensor([[0.5627],\n",
      "        [0.5723],\n",
      "        [0.8851]], requires_grad=True) b: tensor([0.0072], requires_grad=True)\n",
      "epoch: 15900 cost:  tensor(7.8128, grad_fn=<MeanBackward0>) W tensor([[0.5622],\n",
      "        [0.5719],\n",
      "        [0.8860]], requires_grad=True) b: tensor([0.0072], requires_grad=True)\n",
      "epoch: 16000 cost:  tensor(7.8004, grad_fn=<MeanBackward0>) W tensor([[0.5616],\n",
      "        [0.5716],\n",
      "        [0.8869]], requires_grad=True) b: tensor([0.0072], requires_grad=True)\n",
      "epoch: 16100 cost:  tensor(7.7882, grad_fn=<MeanBackward0>) W tensor([[0.5611],\n",
      "        [0.5712],\n",
      "        [0.8877]], requires_grad=True) b: tensor([0.0072], requires_grad=True)\n",
      "epoch: 16200 cost:  tensor(7.7760, grad_fn=<MeanBackward0>) W tensor([[0.5606],\n",
      "        [0.5708],\n",
      "        [0.8886]], requires_grad=True) b: tensor([0.0072], requires_grad=True)\n",
      "epoch: 16300 cost:  tensor(7.7640, grad_fn=<MeanBackward0>) W tensor([[0.5600],\n",
      "        [0.5704],\n",
      "        [0.8895]], requires_grad=True) b: tensor([0.0071], requires_grad=True)\n",
      "epoch: 16400 cost:  tensor(7.7520, grad_fn=<MeanBackward0>) W tensor([[0.5595],\n",
      "        [0.5700],\n",
      "        [0.8904]], requires_grad=True) b: tensor([0.0071], requires_grad=True)\n",
      "epoch: 16500 cost:  tensor(7.7401, grad_fn=<MeanBackward0>) W tensor([[0.5590],\n",
      "        [0.5697],\n",
      "        [0.8913]], requires_grad=True) b: tensor([0.0071], requires_grad=True)\n",
      "epoch: 16600 cost:  tensor(7.7283, grad_fn=<MeanBackward0>) W tensor([[0.5585],\n",
      "        [0.5693],\n",
      "        [0.8922]], requires_grad=True) b: tensor([0.0071], requires_grad=True)\n",
      "epoch: 16700 cost:  tensor(7.7166, grad_fn=<MeanBackward0>) W tensor([[0.5580],\n",
      "        [0.5689],\n",
      "        [0.8930]], requires_grad=True) b: tensor([0.0071], requires_grad=True)\n",
      "epoch: 16800 cost:  tensor(7.7050, grad_fn=<MeanBackward0>) W tensor([[0.5574],\n",
      "        [0.5685],\n",
      "        [0.8939]], requires_grad=True) b: tensor([0.0071], requires_grad=True)\n",
      "epoch: 16900 cost:  tensor(7.6935, grad_fn=<MeanBackward0>) W tensor([[0.5569],\n",
      "        [0.5682],\n",
      "        [0.8948]], requires_grad=True) b: tensor([0.0071], requires_grad=True)\n",
      "epoch: 17000 cost:  tensor(7.6821, grad_fn=<MeanBackward0>) W tensor([[0.5564],\n",
      "        [0.5678],\n",
      "        [0.8956]], requires_grad=True) b: tensor([0.0071], requires_grad=True)\n",
      "epoch: 17100 cost:  tensor(7.6708, grad_fn=<MeanBackward0>) W tensor([[0.5559],\n",
      "        [0.5674],\n",
      "        [0.8965]], requires_grad=True) b: tensor([0.0071], requires_grad=True)\n",
      "epoch: 17200 cost:  tensor(7.6595, grad_fn=<MeanBackward0>) W tensor([[0.5554],\n",
      "        [0.5671],\n",
      "        [0.8973]], requires_grad=True) b: tensor([0.0071], requires_grad=True)\n",
      "epoch: 17300 cost:  tensor(7.6484, grad_fn=<MeanBackward0>) W tensor([[0.5549],\n",
      "        [0.5667],\n",
      "        [0.8982]], requires_grad=True) b: tensor([0.0070], requires_grad=True)\n",
      "epoch: 17400 cost:  tensor(7.6373, grad_fn=<MeanBackward0>) W tensor([[0.5543],\n",
      "        [0.5664],\n",
      "        [0.8990]], requires_grad=True) b: tensor([0.0070], requires_grad=True)\n",
      "epoch: 17500 cost:  tensor(7.6263, grad_fn=<MeanBackward0>) W tensor([[0.5538],\n",
      "        [0.5660],\n",
      "        [0.8999]], requires_grad=True) b: tensor([0.0070], requires_grad=True)\n",
      "epoch: 17600 cost:  tensor(7.6154, grad_fn=<MeanBackward0>) W tensor([[0.5533],\n",
      "        [0.5657],\n",
      "        [0.9007]], requires_grad=True) b: tensor([0.0070], requires_grad=True)\n",
      "epoch: 17700 cost:  tensor(7.6046, grad_fn=<MeanBackward0>) W tensor([[0.5528],\n",
      "        [0.5653],\n",
      "        [0.9015]], requires_grad=True) b: tensor([0.0070], requires_grad=True)\n",
      "epoch: 17800 cost:  tensor(7.5938, grad_fn=<MeanBackward0>) W tensor([[0.5523],\n",
      "        [0.5650],\n",
      "        [0.9024]], requires_grad=True) b: tensor([0.0070], requires_grad=True)\n",
      "epoch: 17900 cost:  tensor(7.5832, grad_fn=<MeanBackward0>) W tensor([[0.5518],\n",
      "        [0.5646],\n",
      "        [0.9032]], requires_grad=True) b: tensor([0.0070], requires_grad=True)\n",
      "epoch: 18000 cost:  tensor(7.5726, grad_fn=<MeanBackward0>) W tensor([[0.5513],\n",
      "        [0.5643],\n",
      "        [0.9040]], requires_grad=True) b: tensor([0.0070], requires_grad=True)\n",
      "epoch: 18100 cost:  tensor(7.5621, grad_fn=<MeanBackward0>) W tensor([[0.5508],\n",
      "        [0.5639],\n",
      "        [0.9049]], requires_grad=True) b: tensor([0.0070], requires_grad=True)\n",
      "epoch: 18200 cost:  tensor(7.5517, grad_fn=<MeanBackward0>) W tensor([[0.5503],\n",
      "        [0.5636],\n",
      "        [0.9057]], requires_grad=True) b: tensor([0.0070], requires_grad=True)\n",
      "epoch: 18300 cost:  tensor(7.5414, grad_fn=<MeanBackward0>) W tensor([[0.5498],\n",
      "        [0.5632],\n",
      "        [0.9065]], requires_grad=True) b: tensor([0.0069], requires_grad=True)\n",
      "epoch: 18400 cost:  tensor(7.5311, grad_fn=<MeanBackward0>) W tensor([[0.5493],\n",
      "        [0.5629],\n",
      "        [0.9073]], requires_grad=True) b: tensor([0.0069], requires_grad=True)\n",
      "epoch: 18500 cost:  tensor(7.5209, grad_fn=<MeanBackward0>) W tensor([[0.5488],\n",
      "        [0.5626],\n",
      "        [0.9081]], requires_grad=True) b: tensor([0.0069], requires_grad=True)\n",
      "epoch: 18600 cost:  tensor(7.5108, grad_fn=<MeanBackward0>) W tensor([[0.5483],\n",
      "        [0.5622],\n",
      "        [0.9089]], requires_grad=True) b: tensor([0.0069], requires_grad=True)\n",
      "epoch: 18700 cost:  tensor(7.5008, grad_fn=<MeanBackward0>) W tensor([[0.5478],\n",
      "        [0.5619],\n",
      "        [0.9097]], requires_grad=True) b: tensor([0.0069], requires_grad=True)\n",
      "epoch: 18800 cost:  tensor(7.4909, grad_fn=<MeanBackward0>) W tensor([[0.5473],\n",
      "        [0.5616],\n",
      "        [0.9105]], requires_grad=True) b: tensor([0.0069], requires_grad=True)\n",
      "epoch: 18900 cost:  tensor(7.4810, grad_fn=<MeanBackward0>) W tensor([[0.5468],\n",
      "        [0.5613],\n",
      "        [0.9113]], requires_grad=True) b: tensor([0.0069], requires_grad=True)\n",
      "epoch: 19000 cost:  tensor(7.4712, grad_fn=<MeanBackward0>) W tensor([[0.5463],\n",
      "        [0.5609],\n",
      "        [0.9121]], requires_grad=True) b: tensor([0.0069], requires_grad=True)\n",
      "epoch: 19100 cost:  tensor(7.4615, grad_fn=<MeanBackward0>) W tensor([[0.5458],\n",
      "        [0.5606],\n",
      "        [0.9129]], requires_grad=True) b: tensor([0.0069], requires_grad=True)\n",
      "epoch: 19200 cost:  tensor(7.4519, grad_fn=<MeanBackward0>) W tensor([[0.5453],\n",
      "        [0.5603],\n",
      "        [0.9137]], requires_grad=True) b: tensor([0.0069], requires_grad=True)\n",
      "epoch: 19300 cost:  tensor(7.4423, grad_fn=<MeanBackward0>) W tensor([[0.5449],\n",
      "        [0.5600],\n",
      "        [0.9145]], requires_grad=True) b: tensor([0.0068], requires_grad=True)\n",
      "epoch: 19400 cost:  tensor(7.4328, grad_fn=<MeanBackward0>) W tensor([[0.5444],\n",
      "        [0.5597],\n",
      "        [0.9153]], requires_grad=True) b: tensor([0.0068], requires_grad=True)\n",
      "epoch: 19500 cost:  tensor(7.4234, grad_fn=<MeanBackward0>) W tensor([[0.5439],\n",
      "        [0.5593],\n",
      "        [0.9161]], requires_grad=True) b: tensor([0.0068], requires_grad=True)\n",
      "epoch: 19600 cost:  tensor(7.4140, grad_fn=<MeanBackward0>) W tensor([[0.5434],\n",
      "        [0.5590],\n",
      "        [0.9168]], requires_grad=True) b: tensor([0.0068], requires_grad=True)\n",
      "epoch: 19700 cost:  tensor(7.4047, grad_fn=<MeanBackward0>) W tensor([[0.5429],\n",
      "        [0.5587],\n",
      "        [0.9176]], requires_grad=True) b: tensor([0.0068], requires_grad=True)\n",
      "epoch: 19800 cost:  tensor(7.3955, grad_fn=<MeanBackward0>) W tensor([[0.5424],\n",
      "        [0.5584],\n",
      "        [0.9184]], requires_grad=True) b: tensor([0.0068], requires_grad=True)\n",
      "epoch: 19900 cost:  tensor(7.3864, grad_fn=<MeanBackward0>) W tensor([[0.5420],\n",
      "        [0.5581],\n",
      "        [0.9191]], requires_grad=True) b: tensor([0.0068], requires_grad=True)\n",
      "epoch: 20000 cost:  tensor(7.3773, grad_fn=<MeanBackward0>) W tensor([[0.5415],\n",
      "        [0.5578],\n",
      "        [0.9199]], requires_grad=True) b: tensor([0.0068], requires_grad=True)\n",
      "epoch: 20100 cost:  tensor(7.3683, grad_fn=<MeanBackward0>) W tensor([[0.5410],\n",
      "        [0.5575],\n",
      "        [0.9207]], requires_grad=True) b: tensor([0.0068], requires_grad=True)\n",
      "epoch: 20200 cost:  tensor(7.3594, grad_fn=<MeanBackward0>) W tensor([[0.5405],\n",
      "        [0.5572],\n",
      "        [0.9214]], requires_grad=True) b: tensor([0.0068], requires_grad=True)\n",
      "epoch: 20300 cost:  tensor(7.3505, grad_fn=<MeanBackward0>) W tensor([[0.5401],\n",
      "        [0.5569],\n",
      "        [0.9222]], requires_grad=True) b: tensor([0.0067], requires_grad=True)\n",
      "epoch: 20400 cost:  tensor(7.3417, grad_fn=<MeanBackward0>) W tensor([[0.5396],\n",
      "        [0.5566],\n",
      "        [0.9229]], requires_grad=True) b: tensor([0.0067], requires_grad=True)\n",
      "epoch: 20500 cost:  tensor(7.3330, grad_fn=<MeanBackward0>) W tensor([[0.5391],\n",
      "        [0.5563],\n",
      "        [0.9237]], requires_grad=True) b: tensor([0.0067], requires_grad=True)\n",
      "epoch: 20600 cost:  tensor(7.3243, grad_fn=<MeanBackward0>) W tensor([[0.5386],\n",
      "        [0.5560],\n",
      "        [0.9244]], requires_grad=True) b: tensor([0.0067], requires_grad=True)\n",
      "epoch: 20700 cost:  tensor(7.3157, grad_fn=<MeanBackward0>) W tensor([[0.5382],\n",
      "        [0.5557],\n",
      "        [0.9252]], requires_grad=True) b: tensor([0.0067], requires_grad=True)\n",
      "epoch: 20800 cost:  tensor(7.3071, grad_fn=<MeanBackward0>) W tensor([[0.5377],\n",
      "        [0.5554],\n",
      "        [0.9259]], requires_grad=True) b: tensor([0.0067], requires_grad=True)\n",
      "epoch: 20900 cost:  tensor(7.2987, grad_fn=<MeanBackward0>) W tensor([[0.5372],\n",
      "        [0.5551],\n",
      "        [0.9266]], requires_grad=True) b: tensor([0.0067], requires_grad=True)\n",
      "epoch: 21000 cost:  tensor(7.2903, grad_fn=<MeanBackward0>) W tensor([[0.5368],\n",
      "        [0.5548],\n",
      "        [0.9274]], requires_grad=True) b: tensor([0.0067], requires_grad=True)\n",
      "epoch: 21100 cost:  tensor(7.2819, grad_fn=<MeanBackward0>) W tensor([[0.5363],\n",
      "        [0.5545],\n",
      "        [0.9281]], requires_grad=True) b: tensor([0.0067], requires_grad=True)\n",
      "epoch: 21200 cost:  tensor(7.2736, grad_fn=<MeanBackward0>) W tensor([[0.5358],\n",
      "        [0.5542],\n",
      "        [0.9288]], requires_grad=True) b: tensor([0.0066], requires_grad=True)\n",
      "epoch: 21300 cost:  tensor(7.2654, grad_fn=<MeanBackward0>) W tensor([[0.5354],\n",
      "        [0.5540],\n",
      "        [0.9296]], requires_grad=True) b: tensor([0.0066], requires_grad=True)\n",
      "epoch: 21400 cost:  tensor(7.2573, grad_fn=<MeanBackward0>) W tensor([[0.5349],\n",
      "        [0.5537],\n",
      "        [0.9303]], requires_grad=True) b: tensor([0.0066], requires_grad=True)\n",
      "epoch: 21500 cost:  tensor(7.2492, grad_fn=<MeanBackward0>) W tensor([[0.5345],\n",
      "        [0.5534],\n",
      "        [0.9310]], requires_grad=True) b: tensor([0.0066], requires_grad=True)\n",
      "epoch: 21600 cost:  tensor(7.2411, grad_fn=<MeanBackward0>) W tensor([[0.5340],\n",
      "        [0.5531],\n",
      "        [0.9317]], requires_grad=True) b: tensor([0.0066], requires_grad=True)\n",
      "epoch: 21700 cost:  tensor(7.2331, grad_fn=<MeanBackward0>) W tensor([[0.5335],\n",
      "        [0.5528],\n",
      "        [0.9324]], requires_grad=True) b: tensor([0.0066], requires_grad=True)\n",
      "epoch: 21800 cost:  tensor(7.2252, grad_fn=<MeanBackward0>) W tensor([[0.5331],\n",
      "        [0.5526],\n",
      "        [0.9332]], requires_grad=True) b: tensor([0.0066], requires_grad=True)\n",
      "epoch: 21900 cost:  tensor(7.2174, grad_fn=<MeanBackward0>) W tensor([[0.5326],\n",
      "        [0.5523],\n",
      "        [0.9339]], requires_grad=True) b: tensor([0.0066], requires_grad=True)\n",
      "epoch: 22000 cost:  tensor(7.2096, grad_fn=<MeanBackward0>) W tensor([[0.5322],\n",
      "        [0.5520],\n",
      "        [0.9346]], requires_grad=True) b: tensor([0.0066], requires_grad=True)\n",
      "epoch: 22100 cost:  tensor(7.2019, grad_fn=<MeanBackward0>) W tensor([[0.5317],\n",
      "        [0.5518],\n",
      "        [0.9353]], requires_grad=True) b: tensor([0.0065], requires_grad=True)\n",
      "epoch: 22200 cost:  tensor(7.1941, grad_fn=<MeanBackward0>) W tensor([[0.5313],\n",
      "        [0.5515],\n",
      "        [0.9360]], requires_grad=True) b: tensor([0.0065], requires_grad=True)\n",
      "epoch: 22300 cost:  tensor(7.1865, grad_fn=<MeanBackward0>) W tensor([[0.5308],\n",
      "        [0.5512],\n",
      "        [0.9367]], requires_grad=True) b: tensor([0.0065], requires_grad=True)\n",
      "epoch: 22400 cost:  tensor(7.1790, grad_fn=<MeanBackward0>) W tensor([[0.5304],\n",
      "        [0.5510],\n",
      "        [0.9374]], requires_grad=True) b: tensor([0.0065], requires_grad=True)\n",
      "epoch: 22500 cost:  tensor(7.1715, grad_fn=<MeanBackward0>) W tensor([[0.5299],\n",
      "        [0.5507],\n",
      "        [0.9381]], requires_grad=True) b: tensor([0.0065], requires_grad=True)\n",
      "epoch: 22600 cost:  tensor(7.1640, grad_fn=<MeanBackward0>) W tensor([[0.5295],\n",
      "        [0.5504],\n",
      "        [0.9388]], requires_grad=True) b: tensor([0.0065], requires_grad=True)\n",
      "epoch: 22700 cost:  tensor(7.1566, grad_fn=<MeanBackward0>) W tensor([[0.5290],\n",
      "        [0.5502],\n",
      "        [0.9394]], requires_grad=True) b: tensor([0.0065], requires_grad=True)\n",
      "epoch: 22800 cost:  tensor(7.1493, grad_fn=<MeanBackward0>) W tensor([[0.5286],\n",
      "        [0.5499],\n",
      "        [0.9401]], requires_grad=True) b: tensor([0.0065], requires_grad=True)\n",
      "epoch: 22900 cost:  tensor(7.1420, grad_fn=<MeanBackward0>) W tensor([[0.5282],\n",
      "        [0.5497],\n",
      "        [0.9408]], requires_grad=True) b: tensor([0.0065], requires_grad=True)\n",
      "epoch: 23000 cost:  tensor(7.1347, grad_fn=<MeanBackward0>) W tensor([[0.5277],\n",
      "        [0.5494],\n",
      "        [0.9415]], requires_grad=True) b: tensor([0.0064], requires_grad=True)\n",
      "epoch: 23100 cost:  tensor(7.1276, grad_fn=<MeanBackward0>) W tensor([[0.5273],\n",
      "        [0.5491],\n",
      "        [0.9422]], requires_grad=True) b: tensor([0.0064], requires_grad=True)\n",
      "epoch: 23200 cost:  tensor(7.1204, grad_fn=<MeanBackward0>) W tensor([[0.5268],\n",
      "        [0.5489],\n",
      "        [0.9428]], requires_grad=True) b: tensor([0.0064], requires_grad=True)\n",
      "epoch: 23300 cost:  tensor(7.1133, grad_fn=<MeanBackward0>) W tensor([[0.5264],\n",
      "        [0.5486],\n",
      "        [0.9435]], requires_grad=True) b: tensor([0.0064], requires_grad=True)\n",
      "epoch: 23400 cost:  tensor(7.1063, grad_fn=<MeanBackward0>) W tensor([[0.5260],\n",
      "        [0.5484],\n",
      "        [0.9442]], requires_grad=True) b: tensor([0.0064], requires_grad=True)\n",
      "epoch: 23500 cost:  tensor(7.0994, grad_fn=<MeanBackward0>) W tensor([[0.5255],\n",
      "        [0.5481],\n",
      "        [0.9449]], requires_grad=True) b: tensor([0.0064], requires_grad=True)\n",
      "epoch: 23600 cost:  tensor(7.0925, grad_fn=<MeanBackward0>) W tensor([[0.5251],\n",
      "        [0.5479],\n",
      "        [0.9455]], requires_grad=True) b: tensor([0.0064], requires_grad=True)\n",
      "epoch: 23700 cost:  tensor(7.0856, grad_fn=<MeanBackward0>) W tensor([[0.5246],\n",
      "        [0.5477],\n",
      "        [0.9462]], requires_grad=True) b: tensor([0.0064], requires_grad=True)\n",
      "epoch: 23800 cost:  tensor(7.0788, grad_fn=<MeanBackward0>) W tensor([[0.5242],\n",
      "        [0.5474],\n",
      "        [0.9468]], requires_grad=True) b: tensor([0.0064], requires_grad=True)\n",
      "epoch: 23900 cost:  tensor(7.0720, grad_fn=<MeanBackward0>) W tensor([[0.5238],\n",
      "        [0.5472],\n",
      "        [0.9475]], requires_grad=True) b: tensor([0.0063], requires_grad=True)\n",
      "epoch: 24000 cost:  tensor(7.0653, grad_fn=<MeanBackward0>) W tensor([[0.5234],\n",
      "        [0.5469],\n",
      "        [0.9481]], requires_grad=True) b: tensor([0.0063], requires_grad=True)\n",
      "epoch: 24100 cost:  tensor(7.0587, grad_fn=<MeanBackward0>) W tensor([[0.5229],\n",
      "        [0.5467],\n",
      "        [0.9488]], requires_grad=True) b: tensor([0.0063], requires_grad=True)\n",
      "epoch: 24200 cost:  tensor(7.0520, grad_fn=<MeanBackward0>) W tensor([[0.5225],\n",
      "        [0.5464],\n",
      "        [0.9494]], requires_grad=True) b: tensor([0.0063], requires_grad=True)\n",
      "epoch: 24300 cost:  tensor(7.0454, grad_fn=<MeanBackward0>) W tensor([[0.5221],\n",
      "        [0.5462],\n",
      "        [0.9501]], requires_grad=True) b: tensor([0.0063], requires_grad=True)\n",
      "epoch: 24400 cost:  tensor(7.0389, grad_fn=<MeanBackward0>) W tensor([[0.5216],\n",
      "        [0.5460],\n",
      "        [0.9507]], requires_grad=True) b: tensor([0.0063], requires_grad=True)\n",
      "epoch: 24500 cost:  tensor(7.0325, grad_fn=<MeanBackward0>) W tensor([[0.5212],\n",
      "        [0.5457],\n",
      "        [0.9514]], requires_grad=True) b: tensor([0.0063], requires_grad=True)\n",
      "epoch: 24600 cost:  tensor(7.0261, grad_fn=<MeanBackward0>) W tensor([[0.5208],\n",
      "        [0.5455],\n",
      "        [0.9520]], requires_grad=True) b: tensor([0.0063], requires_grad=True)\n",
      "epoch: 24700 cost:  tensor(7.0197, grad_fn=<MeanBackward0>) W tensor([[0.5204],\n",
      "        [0.5453],\n",
      "        [0.9527]], requires_grad=True) b: tensor([0.0063], requires_grad=True)\n",
      "epoch: 24800 cost:  tensor(7.0133, grad_fn=<MeanBackward0>) W tensor([[0.5199],\n",
      "        [0.5451],\n",
      "        [0.9533]], requires_grad=True) b: tensor([0.0062], requires_grad=True)\n",
      "epoch: 24900 cost:  tensor(7.0071, grad_fn=<MeanBackward0>) W tensor([[0.5195],\n",
      "        [0.5448],\n",
      "        [0.9539]], requires_grad=True) b: tensor([0.0062], requires_grad=True)\n",
      "epoch: 25000 cost:  tensor(7.0009, grad_fn=<MeanBackward0>) W tensor([[0.5191],\n",
      "        [0.5446],\n",
      "        [0.9545]], requires_grad=True) b: tensor([0.0062], requires_grad=True)\n",
      "epoch: 25100 cost:  tensor(6.9947, grad_fn=<MeanBackward0>) W tensor([[0.5187],\n",
      "        [0.5444],\n",
      "        [0.9552]], requires_grad=True) b: tensor([0.0062], requires_grad=True)\n",
      "epoch: 25200 cost:  tensor(6.9885, grad_fn=<MeanBackward0>) W tensor([[0.5183],\n",
      "        [0.5441],\n",
      "        [0.9558]], requires_grad=True) b: tensor([0.0062], requires_grad=True)\n",
      "epoch: 25300 cost:  tensor(6.9825, grad_fn=<MeanBackward0>) W tensor([[0.5179],\n",
      "        [0.5439],\n",
      "        [0.9564]], requires_grad=True) b: tensor([0.0062], requires_grad=True)\n",
      "epoch: 25400 cost:  tensor(6.9764, grad_fn=<MeanBackward0>) W tensor([[0.5174],\n",
      "        [0.5437],\n",
      "        [0.9570]], requires_grad=True) b: tensor([0.0062], requires_grad=True)\n",
      "epoch: 25500 cost:  tensor(6.9704, grad_fn=<MeanBackward0>) W tensor([[0.5170],\n",
      "        [0.5435],\n",
      "        [0.9577]], requires_grad=True) b: tensor([0.0062], requires_grad=True)\n",
      "epoch: 25600 cost:  tensor(6.9644, grad_fn=<MeanBackward0>) W tensor([[0.5166],\n",
      "        [0.5433],\n",
      "        [0.9583]], requires_grad=True) b: tensor([0.0061], requires_grad=True)\n",
      "epoch: 25700 cost:  tensor(6.9585, grad_fn=<MeanBackward0>) W tensor([[0.5162],\n",
      "        [0.5431],\n",
      "        [0.9589]], requires_grad=True) b: tensor([0.0061], requires_grad=True)\n",
      "epoch: 25800 cost:  tensor(6.9527, grad_fn=<MeanBackward0>) W tensor([[0.5158],\n",
      "        [0.5428],\n",
      "        [0.9595]], requires_grad=True) b: tensor([0.0061], requires_grad=True)\n",
      "epoch: 25900 cost:  tensor(6.9468, grad_fn=<MeanBackward0>) W tensor([[0.5154],\n",
      "        [0.5426],\n",
      "        [0.9601]], requires_grad=True) b: tensor([0.0061], requires_grad=True)\n",
      "epoch: 26000 cost:  tensor(6.9410, grad_fn=<MeanBackward0>) W tensor([[0.5150],\n",
      "        [0.5424],\n",
      "        [0.9607]], requires_grad=True) b: tensor([0.0061], requires_grad=True)\n",
      "epoch: 26100 cost:  tensor(6.9353, grad_fn=<MeanBackward0>) W tensor([[0.5146],\n",
      "        [0.5422],\n",
      "        [0.9613]], requires_grad=True) b: tensor([0.0061], requires_grad=True)\n",
      "epoch: 26200 cost:  tensor(6.9296, grad_fn=<MeanBackward0>) W tensor([[0.5142],\n",
      "        [0.5420],\n",
      "        [0.9619]], requires_grad=True) b: tensor([0.0061], requires_grad=True)\n",
      "epoch: 26300 cost:  tensor(6.9240, grad_fn=<MeanBackward0>) W tensor([[0.5138],\n",
      "        [0.5418],\n",
      "        [0.9625]], requires_grad=True) b: tensor([0.0061], requires_grad=True)\n",
      "epoch: 26400 cost:  tensor(6.9184, grad_fn=<MeanBackward0>) W tensor([[0.5134],\n",
      "        [0.5416],\n",
      "        [0.9631]], requires_grad=True) b: tensor([0.0061], requires_grad=True)\n",
      "epoch: 26500 cost:  tensor(6.9128, grad_fn=<MeanBackward0>) W tensor([[0.5130],\n",
      "        [0.5414],\n",
      "        [0.9637]], requires_grad=True) b: tensor([0.0060], requires_grad=True)\n",
      "epoch: 26600 cost:  tensor(6.9073, grad_fn=<MeanBackward0>) W tensor([[0.5125],\n",
      "        [0.5412],\n",
      "        [0.9643]], requires_grad=True) b: tensor([0.0060], requires_grad=True)\n",
      "epoch: 26700 cost:  tensor(6.9018, grad_fn=<MeanBackward0>) W tensor([[0.5121],\n",
      "        [0.5410],\n",
      "        [0.9649]], requires_grad=True) b: tensor([0.0060], requires_grad=True)\n",
      "epoch: 26800 cost:  tensor(6.8963, grad_fn=<MeanBackward0>) W tensor([[0.5117],\n",
      "        [0.5408],\n",
      "        [0.9655]], requires_grad=True) b: tensor([0.0060], requires_grad=True)\n",
      "epoch: 26900 cost:  tensor(6.8909, grad_fn=<MeanBackward0>) W tensor([[0.5113],\n",
      "        [0.5406],\n",
      "        [0.9660]], requires_grad=True) b: tensor([0.0060], requires_grad=True)\n",
      "epoch: 27000 cost:  tensor(6.8855, grad_fn=<MeanBackward0>) W tensor([[0.5109],\n",
      "        [0.5404],\n",
      "        [0.9666]], requires_grad=True) b: tensor([0.0060], requires_grad=True)\n",
      "epoch: 27100 cost:  tensor(6.8802, grad_fn=<MeanBackward0>) W tensor([[0.5106],\n",
      "        [0.5402],\n",
      "        [0.9672]], requires_grad=True) b: tensor([0.0060], requires_grad=True)\n",
      "epoch: 27200 cost:  tensor(6.8749, grad_fn=<MeanBackward0>) W tensor([[0.5102],\n",
      "        [0.5400],\n",
      "        [0.9678]], requires_grad=True) b: tensor([0.0060], requires_grad=True)\n",
      "epoch: 27300 cost:  tensor(6.8697, grad_fn=<MeanBackward0>) W tensor([[0.5098],\n",
      "        [0.5398],\n",
      "        [0.9683]], requires_grad=True) b: tensor([0.0059], requires_grad=True)\n",
      "epoch: 27400 cost:  tensor(6.8645, grad_fn=<MeanBackward0>) W tensor([[0.5094],\n",
      "        [0.5396],\n",
      "        [0.9689]], requires_grad=True) b: tensor([0.0059], requires_grad=True)\n",
      "epoch: 27500 cost:  tensor(6.8593, grad_fn=<MeanBackward0>) W tensor([[0.5090],\n",
      "        [0.5394],\n",
      "        [0.9695]], requires_grad=True) b: tensor([0.0059], requires_grad=True)\n",
      "epoch: 27600 cost:  tensor(6.8542, grad_fn=<MeanBackward0>) W tensor([[0.5086],\n",
      "        [0.5392],\n",
      "        [0.9701]], requires_grad=True) b: tensor([0.0059], requires_grad=True)\n",
      "epoch: 27700 cost:  tensor(6.8491, grad_fn=<MeanBackward0>) W tensor([[0.5082],\n",
      "        [0.5390],\n",
      "        [0.9706]], requires_grad=True) b: tensor([0.0059], requires_grad=True)\n",
      "epoch: 27800 cost:  tensor(6.8440, grad_fn=<MeanBackward0>) W tensor([[0.5078],\n",
      "        [0.5388],\n",
      "        [0.9712]], requires_grad=True) b: tensor([0.0059], requires_grad=True)\n",
      "epoch: 27900 cost:  tensor(6.8390, grad_fn=<MeanBackward0>) W tensor([[0.5074],\n",
      "        [0.5386],\n",
      "        [0.9718]], requires_grad=True) b: tensor([0.0059], requires_grad=True)\n",
      "epoch: 28000 cost:  tensor(6.8340, grad_fn=<MeanBackward0>) W tensor([[0.5070],\n",
      "        [0.5384],\n",
      "        [0.9723]], requires_grad=True) b: tensor([0.0059], requires_grad=True)\n",
      "epoch: 28100 cost:  tensor(6.8290, grad_fn=<MeanBackward0>) W tensor([[0.5066],\n",
      "        [0.5383],\n",
      "        [0.9729]], requires_grad=True) b: tensor([0.0059], requires_grad=True)\n",
      "epoch: 28200 cost:  tensor(6.8241, grad_fn=<MeanBackward0>) W tensor([[0.5063],\n",
      "        [0.5381],\n",
      "        [0.9734]], requires_grad=True) b: tensor([0.0058], requires_grad=True)\n",
      "epoch: 28300 cost:  tensor(6.8192, grad_fn=<MeanBackward0>) W tensor([[0.5059],\n",
      "        [0.5379],\n",
      "        [0.9740]], requires_grad=True) b: tensor([0.0058], requires_grad=True)\n",
      "epoch: 28400 cost:  tensor(6.8144, grad_fn=<MeanBackward0>) W tensor([[0.5055],\n",
      "        [0.5377],\n",
      "        [0.9745]], requires_grad=True) b: tensor([0.0058], requires_grad=True)\n",
      "epoch: 28500 cost:  tensor(6.8096, grad_fn=<MeanBackward0>) W tensor([[0.5051],\n",
      "        [0.5375],\n",
      "        [0.9751]], requires_grad=True) b: tensor([0.0058], requires_grad=True)\n",
      "epoch: 28600 cost:  tensor(6.8048, grad_fn=<MeanBackward0>) W tensor([[0.5047],\n",
      "        [0.5373],\n",
      "        [0.9756]], requires_grad=True) b: tensor([0.0058], requires_grad=True)\n",
      "epoch: 28700 cost:  tensor(6.8001, grad_fn=<MeanBackward0>) W tensor([[0.5043],\n",
      "        [0.5372],\n",
      "        [0.9762]], requires_grad=True) b: tensor([0.0058], requires_grad=True)\n",
      "epoch: 28800 cost:  tensor(6.7954, grad_fn=<MeanBackward0>) W tensor([[0.5040],\n",
      "        [0.5370],\n",
      "        [0.9767]], requires_grad=True) b: tensor([0.0058], requires_grad=True)\n",
      "epoch: 28900 cost:  tensor(6.7907, grad_fn=<MeanBackward0>) W tensor([[0.5036],\n",
      "        [0.5368],\n",
      "        [0.9773]], requires_grad=True) b: tensor([0.0058], requires_grad=True)\n",
      "epoch: 29000 cost:  tensor(6.7861, grad_fn=<MeanBackward0>) W tensor([[0.5032],\n",
      "        [0.5366],\n",
      "        [0.9778]], requires_grad=True) b: tensor([0.0057], requires_grad=True)\n",
      "epoch: 29100 cost:  tensor(6.7815, grad_fn=<MeanBackward0>) W tensor([[0.5028],\n",
      "        [0.5365],\n",
      "        [0.9783]], requires_grad=True) b: tensor([0.0057], requires_grad=True)\n",
      "epoch: 29200 cost:  tensor(6.7769, grad_fn=<MeanBackward0>) W tensor([[0.5025],\n",
      "        [0.5363],\n",
      "        [0.9789]], requires_grad=True) b: tensor([0.0057], requires_grad=True)\n",
      "epoch: 29300 cost:  tensor(6.7724, grad_fn=<MeanBackward0>) W tensor([[0.5021],\n",
      "        [0.5361],\n",
      "        [0.9794]], requires_grad=True) b: tensor([0.0057], requires_grad=True)\n",
      "epoch: 29400 cost:  tensor(6.7679, grad_fn=<MeanBackward0>) W tensor([[0.5017],\n",
      "        [0.5359],\n",
      "        [0.9799]], requires_grad=True) b: tensor([0.0057], requires_grad=True)\n",
      "epoch: 29500 cost:  tensor(6.7634, grad_fn=<MeanBackward0>) W tensor([[0.5013],\n",
      "        [0.5358],\n",
      "        [0.9805]], requires_grad=True) b: tensor([0.0057], requires_grad=True)\n",
      "epoch: 29600 cost:  tensor(6.7589, grad_fn=<MeanBackward0>) W tensor([[0.5010],\n",
      "        [0.5356],\n",
      "        [0.9810]], requires_grad=True) b: tensor([0.0057], requires_grad=True)\n",
      "epoch: 29700 cost:  tensor(6.7545, grad_fn=<MeanBackward0>) W tensor([[0.5006],\n",
      "        [0.5354],\n",
      "        [0.9815]], requires_grad=True) b: tensor([0.0057], requires_grad=True)\n",
      "epoch: 29800 cost:  tensor(6.7502, grad_fn=<MeanBackward0>) W tensor([[0.5002],\n",
      "        [0.5353],\n",
      "        [0.9820]], requires_grad=True) b: tensor([0.0056], requires_grad=True)\n",
      "epoch: 29900 cost:  tensor(6.7458, grad_fn=<MeanBackward0>) W tensor([[0.4999],\n",
      "        [0.5351],\n",
      "        [0.9825]], requires_grad=True) b: tensor([0.0056], requires_grad=True)\n",
      "epoch: 30000 cost:  tensor(6.7415, grad_fn=<MeanBackward0>) W tensor([[0.4995],\n",
      "        [0.5349],\n",
      "        [0.9831]], requires_grad=True) b: tensor([0.0056], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    y = x_train.matmul(W) +b\n",
    "    cost = torch.mean((y-y_train)**2)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    cost_list.append([epoch, cost]) #각 epoch마다 cost를 넣어 줌줌\n",
    "    \n",
    "    if epoch %100 == 0:\n",
    "        print('epoch:', epoch, 'cost: ', cost, 'W', W, 'b:', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x) :\n",
    "    return x.matmul(W) +b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[137.6625],\n",
       "        [189.2492],\n",
       "        [194.0849]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = torch.FloatTensor([[65, 68, 70], [100, 95, 90], [90, 95,100]])\n",
    "predict(new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
